[
    {
        "title": "eViterbo: Linking Humanities Research and Open Data",
        "url": "https://openreview.net/forum?id=C2Nz5zVH0M",
        "abstract": "eViterbo is a platform that combines a MediaWiki-based encyclopedia with an open, structured linked data database, built on a Wikibase suite. Designed as a collaborative research tool, it operates under a CC BY-SA 4.0 license, ensuring openness and accessibility. Its structured data is intentionally designed to be shared with Wikidata, amplifying its potential for interoperability and global knowledge integration.\n\nDeveloped within a research project based at CHAM – Center for the Humanities, FCSH, Universidade NOVA de Lisboa, eViterbo the synergies between academia and the Wikimedia projects\n\nThis presentation explores the conceptual and technical decisions behind the creation of eViterbo, including the development of its infrastructure, data model, and collaborative workflows. It also examines the challenges faced in building and maintaining a platform of this kind in a humanities-oriented environment, including strategies for its adaptation, evolution, and maintenance in a social sciences and humanities academic environment."
    },
    {
        "title": "Opening up and linking type catalogues in Wikidata – increasing the visibility of natural history collections",
        "url": "https://openreview.net/forum?id=KiF1D4fIkH",
        "abstract": "Natural history museums and collections around the world house several billion preserved specimens used by the scientific community to answer questions about the biodiversity and geodiversity on Earth. Collections are increasingly digitised, opened up and made accessible to the wider scientific community and beyond. However, initially only basic information is shared, further related metadata or information on historical contexts is often not connected.\n\nType specimens are the most important objects in these collections because they are associated with the names of new taxa, serving as a permanent reference. As name-bearing specimens, the type material is regularly examined by scientists as decisive objects for resolving taxonomic issues and clarifying species delimitation. Having easy access to type material is, therefore, an important prerequisite for facilitating research. Type catalogues list present and sometimes lost or missing type material of specific – historical and contemporary – collections, for certain taxonomic groups or research expeditions. Collection catalogues have been published for several centuries and comprise information such as the housing institutions of types and duplicate material, type localities, annotations, and sometimes illustrations.\n\nThe open, multilingual and multidisciplinary knowledgebase Wikidata supports discoverability, transparency and accessibility of research data. It is community-curated and serves as a hub for external identifiers and provides structured, human and machine-readable data. Wikidata already comprises data for a huge number of publications including type catalogues, many are accessible via the Biodiversity Heritage Library (BHL) or other digital libraries. However, they are currently not easily searchable as a type of scholarly work.\n\nData from Wikidata can be reused by other platforms and tools. Well-curated high-quality datasets related to natural history collections require the use of community-agreed standards and persistent identifiers as well as international collaboration. This includes exchange with different organisations promoting standardisation and open access to biodiversity data such as the Global Biodiversity Information Facility (GBIF), the Consortium of European Taxonomic Facilities (CETAF) and Biodiversity Information Standards (TDWG). For example, a TDWG Task Group is developing a terminology on how to model research expeditions in Wikidata, and the BHL-Wiki Working Group is involved in data modelling. Such collaborations – also with the wider Wiki community – help to improve the modelling of type catalogues and other entities in Wikidata and to develop best practice recommendations.\n\nIn this study, new Wikidata items are created for articles of type catalogues published in different languages and academic journals, and existing items are enriched by adding different external identifiers (e.g. DOIs, BHL page IDs). In addition, further entities such as the type specimen holding institution(s) or collection agents connected to the material or collection are linked. The project started with type catalogues from the Museum für Naturkunde Berlin, and was then expanded to compile additional catalogues from around the world. The growing open dataset in Wikidata can be (re)used for research in different fields such as taxonomy and systematics, history of collections, digital humanities or provenance research. It highlights the potential of Wikidata for research and knowledge contextualisation."
    },
    {
        "title": "An ontology for Italian theatrical cultural heritage on wikibase.cloud",
        "url": "https://openreview.net/forum?id=rqmdxlzHpD",
        "abstract": "The paper focuses on the innovative “Hyperstage” knowledge base ontology model for the semantic reconstruction of Italian theatrical productions. Innovation combines (1) a bottom-up ontology on Wikibase.cloud for the organization and valorization of metadata with (2) an integrated PKB taxonomy based on the three different categories of a) conception, b) staging, and c) post-production documentation. This framework allows automatic aggregation of each digital object associated with the theatrical production into one of these phases. The project aims to transcend the traditional limits of theatrical data models through a system of ontological hierarchical relationships to trace the evolutionary path of works and identify influences and connections between different productions."
    },
    {
        "title": "Collecting and Detecting Ancient Greek Historians through Wikibase and Wikidata",
        "url": "https://openreview.net/forum?id=XyOzBSxykA",
        "abstract": "Polybius and the lost ancient Greek historians\n\nIn this paper, I will use Wikibase and Wikidata as part of my PhD project on Ancient Greek History and Literature at Leipzig University. Most works of ancient Greek historians are lost and survive only through quotations by later sources. Classical scholars usually call these quotations “fragments.” My PhD project focuses on The Histories of Polybius (206–124 BCE ca.), as this work contains various references to earlier historiographers. It specifically explores the language that Polybius uses when citing other historians and aims to provide insight into their reuse and reception within The Histories. The ultimate goal is to clarify whether Polybius engaged with a specific canon of ancient historians during the composition of his work. This research is part of the MECANO project (https://mecano-dn.eu/), which investigates the dynamics of canonization of Greco-Roman texts by combining traditional approaches with new digital methods.\n\nFirst step: Collecting structured data\n\nTo achieve these goals, I am developing a Wikibase that systematically collects structured data on the quotations of the lost historiographers cited by Polybius. The database will include the original Greek text of the quotations, metadata about the quoted authors (e.g., name, provenance, period) and their works (e.g., title, number of books, content), as well as references to the relevant sections of The Histories (book, chapter, paragraph) and to the classification of the quotations in Jacoby’s Die Fragmente der Griechischen Historiker, the authoritative collection of fragments of the lost historiographers. Since the PhD project focuses on the citing language, every Wikibase instance will also highlight relevant linguistic elements (e.g., verbs of saying and writing, forms of the author’s name, variations in the title of works).\n\nSecond step: Detecting structured data\n\nThen, I will employ the Wikidata Query Service to detect the structured data. Specifically, I aim to create default queries that may prove interesting both for my PhD project and potential Wikibase users. Indeed, queries such as “quotations where Polybius uses the verb ἱστορέω” or “quotations where Polybius specifies the title of works” can help analysing the language and, thus, the citing practice (or ratio laudandi, as classical scholars usually call it) of Polybius.\n\nObjectives\n\nThe main objective is to create new datasets according to the principle of Linked Open Data and to make them available and reusable for research communities across different disciplines. Nowadays a key issue, especially in Digital Classics, is the lack of coherently structured data and metadata about ancient authors and texts. The Wikibase I am developing could therefore contribute not only to the Wikimedia community by integrating new open data, but also to Classical scholarship. Ultimately, I aim to show that new technologies are proving helpful in advancing even traditional approaches to the Humanities."
    },
    {
        "title": "Visualizing Wikidata: the RAWGraphs 2.0 approach",
        "url": "https://openreview.net/forum?id=tjKqLY1hfu",
        "abstract": "Wikidata represents an incredible revolution in open knowledge curation, yet its complexity and structure often limit its visibility and usage—particularly when it comes to data visualization. Transforming Wikidata’s SPARQL query outputs into meaningful, understandable graphics can be challenging due to the intricacies of data preparation and the need for suitable visualization models.\n\nIn this proposal, we introduce RAWGraphs 2.0, a tool designed to bridge the gap between structured data and visual representations. Building on the seminal work by Jacques Bertin in 1968 and subsequent research, RAWGraphs 2.0 leverages a constructive, template-based approach. It relies on the concept of a “visualization template”: each template (e.g., bar chart, pie chart, Sankey diagram) identifies the visual variables available (such as color, size, shape, order) and the types of data dimensions that can be used to control them. This approach encourages a modular, extensible framework for visualization, currently encompassing 22 templates, and is readily adaptable to incorporate new ones. We will discuss how RAWGraphs 2.0’s open and customizable architecture integrates seamlessly with Wikidata SPARQL queries. By doing so, it supports a consistent, transparent pipeline—preserving data provenance and the transformation process. The result is an environment where visualizations remain openly editable, fostering collaborative refinement and adaptation to evolving data or research interests. This proposal first contextualizes the current state of data visualization on Wikipedia and related Wikimedia platforms. We examine both template-based and Wikimedia Commons–hosted visualizations, comparing their strengths and limitations, especially concerning traceability, data provenance, and adaptability. We then introduce the RAWGraphs 2.0 conceptual framework and detail how SPARQL outputs from Wikidata can be seamlessly prepared to fit this paradigm. We demonstrate how to align data queries with visualization templates, enabling researchers, developers, and Wikimedia contributors to quickly prototype and iterate on a broad range of graphical forms. From there, we outline common challenges encountered when working directly with the results of Wikidata queries, and propose good practices for their composition. Finally, we present a series of “visualization template families” as practical exemplars of the potential for Wikidata-driven representations. These include templates for time series, correlation matrices, hierarchical layouts, proportional comparisons, networks, and distribution plots. By mapping Wikidata results into these forms, we reveal the versatility of RAWGraphs 2.0 in handling diverse data scenarios and research questions. Through the integration of Wikidata, SPARQL, and RAWGraphs 2.0, this proposal emphasizes the potential of a flexible, open, and transparent visualization ecosystem. We aim to inspire the Wikidata community and researchers to adopt more intuitive, reproducible, updatable, and visually engaging methods of exploring and understanding open knowledge data."
    },
    {
        "title": "Wikidata as a Backend for Research MediaWikis: A Case Study from the P-CITIZENS Project in documenting Amateur Theatre",
        "url": "https://openreview.net/forum?id=f0RnNTpqL0",
        "abstract": "The ERC-funded project P-CITIZENS - Performing Citizenship explores the social and political roles of amateur theatre in Europe between 1780 and 1850. To support this research, the project has developed the Amateur Theatre Wiki, a platform dedicated to documenting historical and contemporary amateur theater groups. By adopting a WikiFAIR approach, the project implements an efficient, low-overhead solution that uses Wikidata as the structured data backend for the Wiki.\n\nThe Wiki hosts textual and media content written about and by amateur theatre groups, while Wikidata functions as the repository for structured data, such as geographic locations, timelines, membership details, and affiliations. Leveraging Wikidata’s interconnected nature, the project integrates this data into additional knowledge networks, enriching the broader cultural heritage landscape and enabling extensive data reuse.\n\nA innovative technical feature of the project is the automated rendering of Wikidata information into local MediaWiki pages via Infobox templates. This ensures seamless data presentation for end-users while maintaining a centralized external dataset. The separation of content (text and media hosted in the Wiki) from data (stored in Wikidata) enhances reusability, interoperability, and collaborative potential. The paper will explore the following themes:\n\nStrategies for Data Management: Techniques for scraping, importing, organizing, and curating amateur theatre groups and actors in Wikidata using tools like OpenRefine and BeautifulSoup.\n\nCopyright and Community engagement: The process of determining the correct hosting location for media files, and notability criteria for Wikidata\n\nImpact and Accessibility: How Wikidata integration enhances reusability, research opportunities, and public engagement with the dataset.\n\nThis case study focuses on the potential synergy between Wikidata and digital humanities research, showcasing how open data platforms can support academic and cultural initiatives. It offers a replicable model for leveraging Wikidata to lower the complexities of hosting and maintaining a structured dataset, while promoting the dissemination of cultural knowledge."
    },
    {
        "title": "Reimagining Digital Gazetteers: A Wikidata-Powered Approach",
        "url": "https://openreview.net/forum?id=Bdsqs0Yyvy",
        "abstract": "As an open, multilingual, and collaborative knowledge base, Wikidata is increasingly essential for academic research, particularly in Digital Humanities (DH). Its capacity to centralize data from multiple sources, structure it using interoperable standards, and enrich it through collaboration makes it invaluable for DH projects that both import and export data directly on the platform.\n\nA prevalent type of project in DH is the development of digital gazetteers. A gazetteer is traditionally a directory of location names and coordinates. However, in its digital form, it links locations to enriched data such as historical descriptions, spatial coordinates, and temporal information. Digital gazetteers have increase in popularity since the early 2000s for their ability to publish geographic data using Semantic Web standards. Nevertheless, numerous reports from the scientific community indicate that the publication of Linked Open Data (LOD) through digital gazetteers remains hindered by several barriers, including high technical skill requirements and significant financial costs.\n\nThis paper demonstrates Wikidata’s potential for creating state-of-the-art digital gazetteers through two case studies from classical studies and archaeology. These examples illustrate how Wikidata supports both micro- and macro-scale gazetteer projects, enabling advanced data integration, spatial analysis, and collaboration.\n\nThe first case study focuses on the International (Digital) Dura-Europos Archive (IDEA) project, which uses Wikidata to build an urban gazetteer of Dura-Europos, an ancient city in Syria. The city’s cultural heritage is under threat due to the ongoing civil war. By leveraging Wikidata’s multilingual capabilities and Linked Open Data principles, IDEA aims to reassemble fragmented data from Dura-Europos located in collections worldwide. This effort addresses historical and archival biases from colonial-era excavations, promoting more equitable access to heritage. Wikidata’s collaborative nature enables for the first time Syrian researchers and the public to contribute to and benefit from the project.\n\nThe second case study examines our doctoral research on sacred spaces in Roman Britain. As part of the Wikiproject Temples in Roman Britain, we are cataloging temples and sanctuaries in the Roman province of Britannia (43–410 AD), with metadata such as construction and destruction dates, geographic coordinates, connections to other gazetteers, and interpretative frameworks. Furthermore, the project uses the SPARQLing Unicorn plugin for QGIS, enabling dynamic integration of GeoJSON layers directly from Wikidata’s LOD ecosystem, facilitating spatial analysis and visualization.\n\nBoth projects follow similar methodologies, using legacy data to transform disparate archival records into structured and interoperable datasets. It includes extracting information into spreadsheets, modeling the data to fit Wikidata’s ontology, and preparing it for upload using OpenRefine. Schemas are then used to ensure consistency, and the data is exported through QuickStatements for quality control before batch uploads to Wikidata. This workflow ensures data accuracy and integration into the Linked Open Data ecosystem.\n\nBy highlighting these case studies, this paper argues that Wikidata is not only a reliable platform for digital gazetteers but also a transformative tool for DH. Its ability to democratize data creation, integrate Semantic Web technologies, and foster global collaboration represents a significant advancement in the creation and publication of linked geographic data."
    },
    {
        "title": "Developing a creative model for Wikidata analysis in the GLAM sector",
        "url": "https://openreview.net/forum?id=TR7v6Okb9q",
        "abstract": "In 2019, the author started developing a research project exploring the visualization of specific datasets from Wikidata for artistic practice at the University of Salford’s Digital Curation Lab. Initially, the research involved an analysis of gender representation in the University of Salford’s Art Collection through Wikidata. This led to the development of an inquiring model for application on other art or museum collections. Subsequently, this model was applied to datasets that include about 99 university art collections across the UK. During the period 2021 – 2023, a similar approach was adopted on Heritage Malta’s collection of prehistoric female figurines, held at two museums in Malta and Gozo. The project brought together the research work conducted over the previous years, towards a coherent conclusion. Structured on Wikidata, these datasets have been demonstrated through data visualizations, and a data sound art installation (data sonification) accompanied by physical art objects, created through the author’s artistic practice. In the process, reflections on data representations in art collections and/or museums – regardless of whether it is data visualization or data sonification – have provided opportunities to explore concrete ways to look into a collection (through data about it) rather than at a collection as a set of artefacts. This data science point of view aims to enable the discovery of relationships between items within the dataset while stitching them together through shared properties, including in creative ways."
    },
    {
        "title": "A visualization system based on Wikidata for supporting and monitoring the Wiki Loves Monuments Italian contest",
        "url": "https://openreview.net/forum?id=INxfzXeHAb",
        "abstract": "This paper introduces the Wiki Loves Monuments Observatory, an interactive information and visualization system designed to leverage Wikidata (Vrandečić & Krötzsch, 2014) in support of the contest Wiki Loves Monument Italy (WLM Italy). Since 2012, WLM Italy (Azizifard et al., 2023; Bertacchini & Pensa, 2023) has been organized by the local chapter of Wikimedia to enhance the documentation of Italian cultural heritage using content curated in Wikidata and Wikimedia Commons. The observatory facilitates the work of organizers, volunteers, and participants, and helps increase the quality of the materials produced. Specifically, it provides greater support in the following areas: (1) the organization of the national contest as well as regional and local competitions; (2) the engagement of volunteers, both to enhance the database and to produce new photographs; (3) the promotion of the contest through social media and other communication channels; (4) the monitoring of the coverage of Italian cultural heritage documentation on Wikidata and Wikimedia Commons. The platform consists of a database (DB) and a user interface (UI). The UI (https://data.wikilovesmonuments.it/) empowers volunteers, heritage professionals, and local organizers to plan outreach campaigns, coordinate activities (e.g., wiki-expeditions), and enrich digital cultural repositories leveraging a map visualization and a filterable list of cultural properties. It offers both aggregated and granular views, enabling to understand coverage patterns, identify areas in need of documentation, reveal temporal trends in community-driven efforts, and produce visual reports. The DB (https://wlm-it-visual.wmcloud.org/api/schema/swagger-ui/) is automatically updated using specific Wikidata Sparql queries and API requests to Wikimedia Commons. It is accessible via unrestricted APIs and currently used by the presented observatory and other projects. The relevance of this work lies not only in the technical demonstration of using Wikidata as a foundational infrastructure, but also in the participatory design approach (Dörk et al., 2020; Morelli et al., 2021; Sanders & Stappers, 2008) that underpins its implementation. The richness of Wikidata content and the heterogeneity of WLM stakeholders enable virtually limitless possibilities for data analysis. Therefore, identifying and prioritizing the features to be implemented is a complex and non-trivial task that requires activities designed to define, with the final users, what needs to be made available and how. The design methodology entailed desk research, participant observation and structured interviews, which are conducted to inform the realization of a participatory workshop. This step was crucial, as it allowed for the early identification of all necessary features, facilitating the design of a robust, modular, and upgradable infrastructure capable of adapting to the future needs. The resulting system is not merely a dashboard visualization, but a flexible framework where community contributions, iterative refinement, and open data principles converge. By synthesizing these technical and participatory insights, the contribution offers a replicable model for future efforts to leverage Wikidata in projects related to cultural heritage and volunteer engagement. On one hand, it provides examples of data integration and curation strategies; on the other, it exposes the need for participatory design methods to accommodate and anticipate multiple stakeholders’ needs and perspectives."
    },
    {
        "title": "Authoritative Practices and Collective Validation: Wikidata within the Collaborative Digital Edition of the Greek Anthology",
        "url": "https://openreview.net/forum?id=UxixyYsYSU",
        "abstract": "The management and preservation of research data in the Humanities increasingly raises questions about its sustainability, sharing, and validation. In this context, Wikidata constitutes a powerful and collaborative tool. By challenging traditional models where researchers act as both producers and gatekeepers of authority, Wikidata redefines these issues and fosters new paradigms of collaboration.\n\nThis paper will explore these dynamics of collaboration and shifting authority through the case study of the collaborative digital edition of the Greek Anthology (the AG project, hosted at the Canada Research Chair on Digital Textualities since 2014), implemented on a collaborative platform (https://anthologiagraeca.org/) where everyone is invited to participate according to their own knowledge.\n\nWikidata is used in many ways within the AG project. First, all keywords (place names, authors, metrical forms, literary genres, etc.) used to annotate the platform have a Wikidata identifier or is created accordingly. Indeed, when a user participates in the editing of the corpus and wishes to add a keyword to an epigram, if the keyword does not exist, he or she must create it on Wikidata and then link it to the platform. Second, Wikidata has been used in a more intensive way to address inconsistencies in our list of authors. Like Wikidata, our data model is multilingual. However, the gaps and inconsistencies in Wikidata ---such as missing authors, duplicate entries, and inconsistent information across languages--- were directly mirrored on our platform (https://anthologiagraeca.org/authors/). This alignment made it essential to tackle these issues systematically to ensure the accuracy of our data. We started by searching for the names of these authors in various languages (at least in French, English, Italian, Ancient Greek and Latin). We then uploaded this information to Wikidata, and subsequently fetched it back to integrate it into the AG platform. Almost immediately after our data dump on Wikidata, its community quickly reviewed and corrected it to align our contribution with Wikidata's standards and guidelines. This process means we not only retrieved our data but also benefited from the community's improvements.\n\nWe are making a conscious strategic choice: rather than positioning ourselves as the sole custodian of authority, we are delegating that responsibility to a wider community. Our presentation invites reflection on the implications of this shift toward distributed authority. How can that shift in authority benefit academic research projects? Is Wikidata's epistemological paradigm coherent with ours? Can we think of a generic epistemological framework to be effectively applied to specific academic endeavors?\n\nBased on the experiments carried out and the choices made as part of the AG project, this presentation will provide practical and conceptual answers to the questions of (distributed) authority, validation and collaboration in the use of Wikidata, opening up prospects for other projects in the Humanities. We suggest that Wikidata is not merely a technical tool but rather a space where methodological and epistemological debates can unfold. By engaging with this dynamic, researchers can enhance their projects while contributing to the creation of a more sustainable, inclusive, and collaborative knowledge base."
    },
    {
        "title": "Una proposta per la gestione dei fondi personali in Wikidata",
        "url": "https://openreview.net/forum?id=jXtR2U51ce",
        "abstract": "I fondi personali sono “complessi organici di materiali editi e/o inediti raccolti e/o prodotti da persone significative del mondo della cultura, delle professioni e delle arti prevalentemente dalla seconda metà del XIX secolo in poi”. Anche se tali complessi si declinano in tipologie documentarie differenti (biblioteche d’autore, archivi di persona, archivi culturali), l’elemento aggregatore rimane l’individuo e dunque il corpus è documento e testimone degli interessi, delle attività e delle relazioni della persona nel contesto storico e culturale in cui ha operato. La maggiore criticità nella gestione di tali complessi è la descrizione catalografica, a causa delle tipologie differenti di documenti e oggetti da descrivere, il che presuppone l’ utilizzo di standard specifici per ciascuna tipologia di documento. Gli esperimenti portati avanti per testare la validità di Wikidata per la descrizione analitica, esemplare per esemplare, di tali fondi hanno portato alla conclusione che non è opportuno usare Wikidata in tal senso. Migliore risulta essere infatti l’uso di un’istanza Wikibase adattata a questo scopo. Tuttavia Wikidata è molto efficace nell’attività di descrizione di un fondo personale trattato nel suo insieme. Descrivere un fondo personale in Wikidata permette di inserire il fondo in una rete di relazioni i cui nodi sono rappresentati dal soggetto produttore, dal soggetto conservatore, dai precedenti possessori, dal luogo di conservazione, dagli autori di note, dediche e postille rintracciate nei volumi ecc. Tali relazioni varcano i confini di interesse dell’ente conservatore per ripristinare connessioni andate perdute per varie cause, tra cui lo smembramento dei fondi personali e la loro conservazione presso enti culturali diversi. Altro vantaggio è la possibilità di aumentare la conoscenza sul materiale documentario e sul suo possessore, grazie ai dati provenienti da fonti di informazioni differenti, che convergono in Wikidata.\n\nDalle ricerche effettuate risulta che, pur essendo molti i fondi descritti in Wikidata, tuttavia tali item utilizzano un data model non standardizzato e un “vocabolario” dei termini usati per le etichette non controllato. Questa disomogeneità provoca una grande dispersione dei dati e la difficoltà nel rintracciare tutti gli elementi che si riferiscono alla tipologia di fondo personale. La creazione di un Wikidata:Wikiproject dedicato potrebbe risolvere tale problematica, fungendo da punto di raccordo e fonte di buone pratiche per quanti desiderano inserire un fondo personale in Wikidata. La relazione proporrà la creazione di tale Wikiproject e un data model “fondo personale”, che andrà a ricalcare le informazioni presenti nella scheda di rilevazione fondi a cui la Commissione nazionale Biblioteche speciali, archivi e biblioteche d’autore AIB sta lavorando e che sarà resa disponibile ai colleghi bibliotecari nel corso del 2025. In questo modo le due attività e la loro comunicazione potrebbero camminare parallelamente riuscendo ad intercettare più professionisti delle istituzioni GLAM sia nell’uso della scheda rilevazione fondo personale all’interno della propria istituzione, sia nella corrispondente creazione di un item in Wikidata relativo al fondo personale a cui si sta lavorando. Gli scopi del progetto saranno: l'inserimento, arricchimento e valorizzazione di dati sui fondi personali in Wikidata; l'implementazione e il mantenimento di ontologie e thesauri multilingue relativi alla descrizione dei fondi personali; l'interconnessione tra i cataloghi delle collezioni e Wikidata; l'inclusione dei dati in Wikipedia e nei suoi progetti gemelli."
    },
    {
        "title": "Hunting for Lost Heritage on Wikimedia Commons and Wikidata",
        "url": "https://openreview.net/forum?id=T2HJkuxsBw",
        "abstract": "A lot of content has been produced with crowdsourcing by Wikimedia users, not only on Wikipedia. Hundreds of thousands of photographs of cultural heritage were uploaded in the last 20 years and are available on Wikimedia Commons, still not linked to structured data on Wikidata, often of unknown historical buildings, like ruined churches in Southern Italy. We will demostrate that is possible to use data mining techniques on Wikimedia projects and build on Wikidata a more comprehensive open catalogue of Cultural heritage in Italy from crowdsourced contents. We will present the result of phase 1 and 2 of this project, where OpenRefine was used to create thousands of new items on Wikidata about \"lost heritage\" in Italy, and discuss of the possible use of AI to speed-up the process."
    },
    {
        "title": "Il progetto OpenAcolit, un repertorio delle biblioteche italiane realizzato con Wikibase",
        "url": "https://openreview.net/forum?id=EsdsPN5aj4",
        "abstract": "OpenAcolit, successore di Acolit, è un progetto innovativo sviluppato da ABEI, PUSC, BCE-CEI e DMBC-UniPV. Non si propone di trascrivere i volumi cartacei di Acolit, ma di portarne i dati nel web semantico tramite Wikibase Cloud. Questa opera digitale è impostata sulla collaborazione tra bibliotecari e studiosi, offre nuove modalità di fruizione e favorisce l'interazione con altri software."
    },
    {
        "title": "Middle Aramaic epigraphy in Wikidata. Case of inscriptional material from Dura-Europos.",
        "url": "https://openreview.net/forum?id=dgGa5wjATX",
        "abstract": "As a consultant for the inscriptional material in various dialects of Aramaic for the IDEA (International Digital Dura-Europos Archive) project at Bard College and Yale University, I realised how the Aramaic epigraphy is underrepresented in the digital databases. There are a lot of databases which gather Ancient Greek (e.g. packhum, Trismegistos) and Latin (e.g. EDH), but the easily accessible online database which would be focused only on the Aramaic evidence is missing. The works for the IDEA project has on focus, among others, to collect the entire epigraphic evidence in the Linked Open Data datasets and create the Wikidata entries. It is an excellent tool for providing all the editions of the inscriptions with different readings and translations to present the complexity of the field. My paper highlights the role of Wikidata for making the innovative inscriptional corpus, accessible for the broad community. Furthermore, it reflects new perspectives for the studies of the Aramaic inscriptions from the time period between 300 BCE and 300 CE."
    },
    {
        "title": "Whose History We Keep: Benchmarking Wikidata's Record of the Past's Protagonists",
        "url": "https://openreview.net/forum?id=j71riDLzjg",
        "abstract": "Historical science compounds millennia of records of human activity, which have become available in digital, searchable form. Focusing on the largest such dataset—Wikidata—we attempt to find quantitative answers to the questions: Whose periods and which places' people do we know most about? What interests us about them? Who are we forgetting? What trends underlie the number of people registered and written about over time? We examine the individuals who have ended up in these datasets in relation to their readership, thereby shedding light not only on history itself but also on the practice of writing it.\n\nWe begin with our finding that not only the number of people registered in Wikidata, but surprisingly also the fraction of the world population that is registered, follows an exponential increase over time. This is further amplified by accelerations in certain critical periods (such as around 600 BCE, 100 CE, 1500 CE, and 1740 CE), resulting in superexponential growth. In contrast, we analyze the precision of recorded birth dates and find a linear increase in the availability of birth dates precise to the decade and year. Here, we also observe a sudden increase in precision around 1500 CE, possibly due to the introduction of the printing press. Curiously, we find a statistically significant overrepresentation of certain birth months but no such effect for weekdays.\n\nThe spatio-temporal analysis, based on an annotation by Laouenan et al., poignantly shows the shift of cultural centers over time, from the Middle East and China to Central Europe and later to North America. We point out that large sections of human history are staggeringly absent from the dataset; for instance, the Mughal Empire and premodern China after the Three Kingdoms Era have few representatives, despite constituting a significant portion of the world population of their time.\n\nWe then turn our attention to the relationship between Wiki editors, readers, and the people they describe. First, we quantify a highly significant effect of the spoken language of associated Wikipedia entries and the country of origin of the person written about, which somewhat extends to geographically proximal countries as well. While the number of Wikidata entries per person over time follows a monotonous, quasi-exponential growth, the article reads per person over time are not monotonous at all, with, for instance, people from 500 BCE receiving more reads than those from 1400 CE, despite likely being fewer in number. A clear exponential growth in readership only starts around 1750 CE. Further, women and non-binary individuals, conditional on having an article about them, receive consistently more reads than men.\n\nFinally, we reflect on future developments of these metrics. Projecting our data forward, we expect that more than 1 in 1,000 people born today will receive a Wikidata entry (for comparison, more than 1 in 5,000 people have a Wikidata entry today). We hope these representatives will reflect what we care about, and we look forward to an era where societal-scale historical science can study everyone who wants to be studied."
    },
    {
        "title": "A Wikibooks-Wikidata Integration for Crowdsourced Curatorship at the Museu Paulista in Brazil",
        "url": "https://openreview.net/forum?id=bZCXeDzPU6",
        "abstract": "The proposal is to describe and analyse an initiative for the digital dissemination of the Museu Paulista collection at USP, based on the reuse of data available on WIkidata for the creation of a Wikibook in the Portuguese version of the Open Book platform, where works are written collaboratively and published under a free licence. The Museu Paulista was founded in 1894 and is the oldest public museum in the state of São Paulo. In 1963, the museum was integrated into the structure of the University of São Paulo. Between 2013 and 2022, the historic building that houses the museum was closed to visitors because its facilities had to be renovated. During this time, the museum team expanded its digital activities. The Paulista Museum had already had a digital catalogue of its collections since 1993 and in 2017 entered into a partnership with WikiMovimento Brasil, which led to the creation of the USP Paulista Museum GLAM page, where more than 30,000 objects from the museum's collection are now available. Based on the database made available on Wikidata and with the support of the Banco do Brasil Foundation (2020-2022), the museum's digital strategies have developed activities to reuse the data, such as marathons and competitions to edit and create entries for Wikipedia and the production of three wiki books - Marcas nas fotografias de Werner Haberkorn, As fotografias de Guilherme Gaensly no acervo do Museu Paulista and Audiodescrição de obras do Museu do Ipiranga. The Wikibook we have selected for this analysis is Guilherme Gaensly Photographs in Museu Paulista Collection -https://pt.wikibooks.org/wiki/As_fotografias_de_Guilherme_Gaensly_no_acervo_do_Museu_Paulista. The wikibook The photographs of Guilherme Gaensly in the collection of the Museu Paulista was launched in the live edition marathon “São Paulo Photographic” - Museu Paulista da USP’ on YouTube on 8 May 2020. The Wikibook features a set of 140 photographs and postcards by Gaensly, especially urban landscapes of São Paulo and portraits taken in the studio, and encourages readers to collaborate in the description using digital tools. In the technical infrastructure of the wikibook about Gaensly, each page was created by just one person, but the information on it comes from edits made either directly on Wikidata or in applications that indirectly generate edits on Wikidata. The page created initially contains an image of Gaensly and its title, as well as predefined text and data sheets that either have gaps to be filled in according to the specifics of each image or are generated as information is entered into Wikidata. The page also contains a section on participatory curatorship, a section on connections with other images and two sections in the footer with contextual references to Gaensly's work and the Paulista Museum's GLAM-Wiki. Keywords: Wikibooks; GLAM; Digital dissemination; Educational dissemination"
    },
    {
        "title": "Enhancement and sustainable fruition of Cultural Heritage in the era of ecological transition: open data and citizen science",
        "url": "https://openreview.net/forum?id=aRDufrGP3m",
        "abstract": "The doctoral research project is developed in collaboration with Wikimedia Italia and focuses on Cultural Heritage in the Italian context. The project investigates the correlation between Cultural Heritage preservation and the ecological transition, emphasizing open knowledge, sustainability, and citizen science. Climate change poses significant risks to Cultural Heritage, including accelerated degradation of materials in both indoor and outdoor environments. The research adopts an interdisciplinary approach to analyze these phenomena through the development of three case studies representing different display environments and contexts: a Green museum, a museum storage room, and an outdoor heritage site. Particularly, these studies focus on monitoring environmental parameters (both atmospheric and pollution ones), assessing material conservation, and developing sustainable strategies for heritage management. Wikimedia communities play a pivotal role in ensuring the accessibility and dissemination of research findings through Open Access, Open Data, and free licensing practices. By integrating citizen science into the project, the initiative empowers communities to actively participate in data collection and analysis, fostering a broader public understanding of the impact of climate change on cultural heritage. An in-depth analysis of the museums and the cultural institutions on Wikidata highlights the centrality of this platform as a tool for collecting, organising and accessing cultural data. The project overview emphasises how museums, with their already available data and those in the process of being opened, can be mapped, enriched and interconnected through Wikidata. This approach enables the selection of new relevant data and the updating of an open and accessible information network, fostering the valorisation of cultural heritage. Indeed, a key component of the project deals with organizing Wikimedia events in order to involve citizens in the research process. These events aim to raise awareness, share best practices in sustainable heritage conservation, and democratize access to scientific knowledge. The activities that are going to be planned are known as “wikiexcursions” and “editathons”, and they are going to be focused on updating Wikidata and Wikimedia Commons through specific themes focused on cultural heritage and preventive conservation. Within the editathons, it is also possible to plan activities aimed at updating specific points or interesting areas of OpenStreetMap, through the correct geolocation of the outdoor cultural heritage goods under study. By combining cultural heritage preservation, open knowledge dissemination, and community involvement, the project highlights the transformative potential of Wikimedia-driven initiatives in addressing global challenges like climate change while reinforcing the relevance of cultural heritage in society’s ecological transition. In addition, as a result of the monitoring of all the environmental data acquired and the state of conservation of the works, it is possible to obtain the definition of guidelines and best-practices aimed at the preventive conservation of the works of art under study."
    },
    {
        "title": "Modeling Architectural Heritage in Wikidata: A Case Study of Early Modern European Hospitals",
        "url": "https://openreview.net/forum?id=ucYfDgIRFC",
        "abstract": "The ERC-funded project ARCHIATER explores the visual and architectural culture of premodern hospitals in Europe, addressing a key challenge in the digital humanities: sustainable research data. Rather than building a custom, short-lived infrastructure, ARCHIATER embeds its data in Wikidata as a collaborative, long-term Virtual Research Environment. Using open, structured data models, the project documents hospital typologies, artistic networks, and historical developments. ARCHIATER demonstrates how Wikidata supports the FAIR principles and offers a robust infrastructure for future-oriented humanities research. It also fosters interdisciplinary and international collaboration through shared, multilingual data spaces."
    },
    {
        "title": "Using Wikidata to describe Neo-Latin architecture-related authors, texts and words",
        "url": "https://openreview.net/forum?id=GfrpYhoXu4",
        "abstract": "Neo-Latin literature denotes writing in Latin during the Early Modern period. On selected Neo-Latin writings related to architecture, I will demonstrate how I use Wikidata to connect authors, texts and words from a text collection (Croatiae auctores Latini, CroALa) with information in Wikidata – and what we do when the information does not exist in Wikidata yet (in the process of connecting the existing knowledge is made explicit and added to Wikidata, and especially to Wikidata Lexemes). Wikidata is used as a platform for an effort to make computationally manipulable what we know and understand about words and texts, with the larger ambition to make philological research as reproducible as possible."
    },
    {
        "title": "A data visualization tool for heritage buildings in India and Bangladesh: the project of a wiki for ID-SCAPES",
        "url": "https://openreview.net/forum?id=K5wwBLacpM",
        "abstract": "In late 2023, the European Research Council approved the funding for the ID-SCAPES project to study and document the early modern religious architecture of the Christian minorities in India and Bangladesh. These religious sites are often multi-layered and contested heritage, and some buildings have suffered from increasing neglect, erasure and even effacement during recent decades. Furthermore, after the countries’ independence, many within the Church hierarchies sought to distance themselves from colonial legacies, and many of the older churches were radically transformed or completely rebuilt with contemporary designs. This politically influenced process gained pace during the 1960s and continues today, evincing conflicting notions of heritage and identity. Taking into consideration the risks arising from these processes and the progressive erasure of cultural heritage, ID-SCAPES aims to produce a Social History of the Built Environment of India and Bangladesh’s medieval and early modern churches and sacral landscapes (built before ca. 1800), including both functioning and ruined buildings. One of the principal outputs of the project will be herichurch.org, a wiki platform intended to provide a digital map, connected to the visual database, accessible to a wider audience. Following the idea of “preservation by record” the project wikibase will combine 3D visualizations and CAD drawings as fundamental tools for cultural heritage conservation and research. Challenging the European-centric historiographical framework that has been commonly employed for these themes, and through extensive fieldwork and the analysis of visual and written documents that remain unexplored, the project will advance a new methodological approach that embraces the buildings’ complex histories. Addressing issues such as caste, cultural “accommodation,” “indigenous” agency, and local spatial and artistic traditions, ID-SCAPES will uncover the impact of such factors on church architecture. Hence, the aim of ID-SCAPES’ wiki is to visualize historical information gathering and processing data from research and fieldwork. In turn, such digital tools for endangered/contested sites can have an immediate impact on heritage management interventions. For the Wikidata and Research 2025 conference, ideally in a “lightning talks” format, we will share our idea of how visual and written historical knowledge can be structured, possibly gathering suggestions from researchers who have achieved comparable outputs. As we are in the initial phase of the project, qualified feedback is of crucial importance to advance in the right direction. The structure of knowledge should consist of a spatial database/digital map, visually organized for accessible consultation, and linked individual entries that constitute the main objects of research of ID-SCAPES. This system should work on a web mapping platform. The authors have been involved in two comparable projects of heritage mapping, the hpip.org and eviterbo.fcsh.unl.pt platforms. In a similar vein, the Herichurch.org platform is expected to generate entries for a selection of about 50 representative sites during the project’s timeline. Further content will be uploaded at later dates. This wiki platform is also one of the milestones of the project. In summary, the ID-SCAPES wiki is set to advance the understanding and preservation of early modern religious architecture in India and Bangladesh through an innovative digital platform that captures and engages with the complex histories of these significant cultural sites.\n\nThemes:\n\nData visualisations and tools.\nProjects and proposals.\n\nLanguage of the presentation: English and Italian (both if necessary)\n\nConfirmation of the physical presence in Florence of at least one of the authors: Yes\n\nAuthors bibliography: see pdf"
    },
    {
        "title": "Integrating Projects Working Around An Open Database Of Published Music Recordings: a call for collaboration",
        "url": "https://openreview.net/forum?id=PQSOgMCpGX",
        "abstract": "Outside the immediate purview of the expansive WikiProject Music on Wikipedia, there are at least three other projects relating to capturing structured data about published music recordings through Wikidata, Wikibase, and other Wikimedia platforms. One is the AfroSounds project, led by Oreoluwa (User:ReoMartins) from Nigeria since 2022. Another is the proposal by Daniel Antal (presented at the 2024 CEE Meeting) to build a music data sharing space with Wikibase starting with music published in Slovakia, inspired by the Luxembourg Shared Authority File project. And the third is the work of the Malta Music Memory Project, developed by the author with the M3P Foundation since 2009, using a MediaWiki site and Wikidata. This is a call for other academic researchers to collaborate on the development of an integrated data structure and workflow model – including possibilities for automation through bots – for published music recordings that is applicable to Wikidata. The aim is to enable systematic data gathering on a global level, building on existing datasets currently held by music publishing platforms and organisations who seek to make it more findable. Considerations for restrictive database rights that sometimes preclude integration into Wikimedia's open knowledge ecosystem, may require staging via Wikibase, rather than Wikidata, in the first instance."
    },
    {
        "title": "Mapping UNIMARC to BIBFRAME: The SHARE Catalogue Knowledge Base on Wikibase.cloud",
        "url": "https://openreview.net/forum?id=UaBDWHMWJa",
        "abstract": "(EN) SHARE Catalogue Mapping Knowledge Base[1] is a project, hosted on Wikibase.Cloud, that aims to represent the mapping between the UNIMARC bibliographic format and the BIBFRAME ontology. This mapping, carried out by the SHARE Catalogue technical team between 2022 and 2023, is primarily intended to enable SHARE Catalogue to adopt the Share Family's new LOD Platform[2], along with its advanced bibliographic entity representation and Linked Data Editor. In accordance with the SHARE family's principles of openness and interoperability, the technical team chose Wikibase technology to make the processed mapping accessible for widespread use in the professional and research community. Furthermore, the SHARE Catalogue team sought to experiment with a modeling of UNIMARC structured in statements, defining all the key elements of the format (tags, subfields, indicators, etc.), their sources (primarily UNIMARC 3rd edition updates), possibly their correspondence in external projects (e.g., iflastandards website), and their matching with BIBFRAME attributes.\n\n(IT) SHARE Catalogue Mapping Knowledge Base[1] è un progetto, ospitato su Wikibase.Cloud, che si propone di rappresentare la mappatura tra il formato bibliografico UNIMARC e l'ontologia BIBFRAME. Questa mappatura, realizzata dal gruppo tecnico di SHARE Catalogue tra il 2022 e il 2023, ha lo scopo principale di consentire a SHARE Catalogue di adottare la nuova piattaforma LOD della Share Family[2], con la sua rappresentazione avanzata delle entità bibliografiche e il Linked Data Editor. In conformità con i principi di apertura e interoperabilità della Share Family, il gruppo tecnico ha scelto Wikibase per rendere la mappatura disponibile per un uso diffuso nella comunità professionale e di ricerca. Inoltre, il team di SHARE Catalogue ha cercato di sperimentare una modellazione di UNIMARC strutturata in dichiarazioni, definendo tutti gli elementi chiave del formato (tag, sottocampi, indicatori, ecc.), le loro fonti (principalmente gli aggiornamenti della terza edizione di UNIMARC), eventualmente la loro corrispondenza in progetti esterni (ad esempio, il sito web iflastandards) e la loro corrispondenza con gli attributi BIBFRAME.\n\n[*] The presentation will be held in Italian [1] https://unimarc2bibframe.wikibase.cloud/ [2] https://www.share-family.org/#technology"
    },
    {
        "title": "La bibliografia di Giacomo Caputo e il ruolo di Wikidata per la sua valorizzazione",
        "url": "https://openreview.net/forum?id=pGhAqNJQMB",
        "abstract": "La bibliografia di Giacomo Caputo e il ruolo di Wikidata per la sua valorizzazione\n\nIl fondo librario di Giacomo Caputo (1901-1992), di proprietà dell’Università di Firenze e conservato presso il Museo e Istituto Fiorentino di Preistoria \"Paolo Graziosi\", costituisce una risorsa preziosa per indagare la produzione scientifica e il contesto intellettuale di uno dei protagonisti dell’archeologia del Novecento. Il progetto di ricerca “Il fondo archivistico e librario di Giacomo Caputo: archeologia e restauro architettonico in una biblioteca d’autore” si propone anche di valorizzare la sua bibliografia, includendo contributi inediti o poco noti della sua produzione letteraria emersi durante il censimento del fondo: si tratta di interventi, recensioni, note metodologiche appartenenti alla cosiddetta “letteratura grigia”, nascosti all’interno di riviste, periodici e bollettini, ma che concorrono alla ricostruzione della bibliografia completa dell’autore. Attraverso Wikidata, il progetto mira a trasformare la bibliografia di Caputo in un ecosistema dinamico grazie all’interazione dei dati con altri elementi rilevanti, favorendo l’accessibilità globale, l’interoperabilità e la data visualization, superando quindi i limiti delle bibliografie statiche. Questa strategia non solo potenzia il corpus bibliografico, ma permette anche di restituire al vasto pubblico dati verificati e interrogabili, alimentando nuove suggestioni e contribuendo a tracciare nuove strade per la ricerca. Il progetto si inserisce nel dibattito attuale sull’utilizzo di metodologie innovative per la valorizzazione di bibliografie e biblioteche d’autore, proponendo un modello replicabile per la gestione e la valorizzazione di queste importanti raccolte.\n\n[La presentazione avverrà in italiano.]"
    },
    {
        "title": "ProgrEFR: diffondere e promuovere i programmi di ricerca dell’Ecole française de Rome con Wikidata",
        "url": "https://openreview.net/forum?id=wESwjiOqgz",
        "abstract": "Il nostro progetto mira a - promuovere e dare visibilità alle attività scientifiche e ai programmi di ricerca della nostra istituzione - creare dei link tra contenitori di dati di natura complementare (LOD) - rendere accessibili le risorse prodotte dai ricercatori dell’EFR in un’ottica di open data Abbiamo scelto di concentrarci su venti programmi di ricerca, detti strutturanti, che costituiscono una parte trainante dell’attuale attività scientifica dell’EFR. Sono articolati in diverse tematiche di ricerca, ricoprono un ampio arco cronologico (dall’antichità alla storia contemporanea) e, attingendo a diverse metodologie e fonti di ricerca, adottano un approccio multidisciplinare. I programmi strutturanti hanno una durata di quattro o cinque anni. Dato il carattere internazionale dell'istituzione, essi sono realizzati in partenariato con una o più istituzioni straniere o italiane. Per raggiungere questo obiettivo il nostro progetto prevede - la descrizione dei programmi nel database della rete bibliografica delle università francesi (SUDOC/IdRef) - la descrizione dei programmi in HAL (data base in cui i ricercatori affiliati al Ministero della Ricerca e dell’Insegnamento superiore francese versano la loro produzione scientifica con libertà di renderne accessibile o meno i contenuti) - l’inserimento e la descrizione dei programmi in Wikidata Concretamente, per ogni programma sono stati creati gli identificativi IdRef, Hal e gli item Wikidata con i rispettivi rinvii e descrittori adeguati alle caratteristiche proprie ad ogni data base. Gli identificativi propri ad ogni contenitore di dati permettono il dialogo tra i vari sistemi nell’ottica dell’interoperabilità, ricavando dati e informazioni diverse a seconda delle proprietà/ambiti dei vari contenitori. Così IdRef consente essenzialmente una descrizione del programma come un ente autore, Hal raccoglie i relativi studi depositati dai singoli ricercatori e Wikidata diventa soprattutto l’aggregatore che collega le diverse interfacce.\n\nProgrEFR: Disseminating and promoting the research programmes of the Ecole française de Rome with Wikidata Our project aims to\n\npromote and give visibility to the scientific activities and research programmes of our institution\ncreate links between data containers of a complementary nature (LOD)\nmake the resources produced by EFR researchers accessible in an open data perspective We have chosen to focus on twenty research programmes, known as structuring programmes, which form a driving force in EFR's current scientific activity. They are divided into different research themes, cover a broad chronological span (from antiquity to contemporary history) and, drawing on different research methodologies and sources, adopt a multidisciplinary approach. The structuring programmes have a duration of four or five years. Given the international character of the institution, they are realised in partnership with one or more foreign or Italian institutions. To achieve this goal, our project involves\nthe description of programmes in the database of the bibliographical network of French universities (SUDOC/IdRef)\nthe description of the programmes in HAL (a database in which researchers affiliated to the French Ministry of Research and Higher Education deposit their scientific production with freedom to make its contents accessible or not)\nthe entry and description of programmes in Wikidata Specifically, IdRef, Hal and Wikidata item identifiers were created for each programme, with the respective references and descriptors adapted to the characteristics specific to each data base. The identifiers proper to each data container allow dialogue between the various systems with a view to interoperability, obtaining different data and information depending on the properties/environments of the various containers. Thus IdRef essentially allows a description of the programme as an corporate name heading, Hal collects the relevant studies deposited by individual researchers, and Wikidata becomes above all the aggregator linking the different interfaces."
    },
    {
        "title": "Automatic Verification of References of Wikidata Statements",
        "url": "https://openreview.net/forum?id=LeDpCKLh3D",
        "abstract": "Wikidata is one of the world's most important data assets. It is used by search engines, virtual assistants, fact checkers, and in over 800 Wikimedia projects. Wikidata contains 1.65 billion statements about over 116 million data items, edited by nearly 25 thousand editors. Manually checking whether Wikidata statements are supported by references is a slow process that does not scale with size. Given the overall number of statements to check, the collaborative nature of Wikidata, and the fact that referenced documents can change over time, preserving the quality of the references is an onerous process requiring continuous intervention. This paper present ProVe – a tool to assist in the automatic verification and assessment of the quality of the references of Wikidata items."
    },
    {
        "title": "Wikidata and research: a decadal view",
        "url": "https://openreview.net/forum?id=lYwXKrxmGj",
        "abstract": "The breadth, depth and diversity of interactions between the research ecosystem and the ecosystem around Wikidata have evolved considerably over the years. These interactions include, for instance, research about Wikidata and Wikibase matters as well as research-related content, infrastructure or activities with a Wikidata or Wikibase component.\n\nThis contribution is intended for researchers of any background as well as science communicators and other stakeholders in the research landscape. It aims to shine some spotlights on key facets of these interactions and their evolution, considering developments within and beyond both ecosystems, such as the trends towards increased openness in research workflows or towards using Wikibase instances as a platform for collaborative and multilingual curation of structured data.\n\nBased on patterns of opportunities and challenges observed for such interactions so far, we will explore some of their major drivers and ponder a set of potential pathways into the future, taking into account multiple dimensions, including alignment between workflows in both ecosystems as well as the sustainability of their respective infrastructure, content and communities. Last but not least, the accepted submissions to the conference will be situated in this evolving landscape and considered together with facets receiving less attention."
    },
    {
        "title": "From data to images: OpenRefine for Wikidata and Wikimedia Commons",
        "url": "https://openreview.net/forum?id=Q1DaWDXKrY",
        "abstract": "Many cultural institutions are increasingly embracing the open data movement by making their collections, research, and archival materials accessible to the public, matching the Wikimedia movement mission that aims to provide free access to the human knowledge. Through platforms like Wikidata and Wikimedia Commons, institutions can share structured data on artworks, historical events and figures, making them interoperable. By 'freeing' data and images for their use and re-use, they are therefore facilitating research, education, and collaboration. From this context moved the pilot project \"Progetto Dati Lombardia\", developed focusing practices, methods and use possibilities of OpenRefine's extensions. Starting from the dataset concerning cultural buildings from the Lombardia region, released in public domain, data has been before wrangled through OpenRefine, checked using Quickstatements and then has been uploaded to Wikidata. Transforming data values as property values has minimised the loss of information. After that, in parallel to the specific implementation of OpenRefine for Wikimedia Commons, The Egyptian Museum of Turin released, not just data, but also pictures of the collection, already available on their website. The fist step was similar to what was done for the previous project about data. For the images it was necessary to link the item page on Wikidata for each artwork, so that the metadata were already structured, before uploading on Wikimedia Commons. These projects showed how cultural institutions can contributing to Wikidata and Wikimedia Commons, to preserve and disseminate cultural knowledge, enhance the visibility of cultural heritage globally."
    },
    {
        "title": "The role of Wikidata in DAMEIP Project (DAta and MEtadata for Implementing Peer Review)",
        "url": "https://openreview.net/forum?id=11aBQ27ovR",
        "abstract": "The research project DAMEIP (Data and Metadata to Implement Peer Review) is funded by the University of Florence for the years 2025–2026. It will be carried out by a research team consisting of Rossana Morriello, a tenure-track researcher (RTD-b) in the academic field HIST-04/C - Archival Science, Bibliography, and Library Science at the Department of History, Archaeology, Geography, Arts, and Performing Arts (SAGAS); Donatella Selva, a tenure-track researcher (RTD-b) in the field GSPS-06/A - Sociology of Cultural and Communicative Processes at the Department of Political and Social Sciences (DSPS), and two research fellows in their respective academic fields. The aim of the research is to analyze the dynamics and practices of peer review in the journals of Florence University Press (FUP), selected as a significant sample of Italian academic publishing. With full respect for privacy and all related ethical aspects, the project seeks to identify and analyze patterns, procedures, quantitative measurements, and timelines of peer review, with particular attention to gender differences. Peer review is one of the research evaluation systems that operates in two phases of the research cycle: prior to the publication of research results in a journal or other type of publication, and later in research evaluation systems implemented by national evaluation agencies, such as ANVUR. This approach is predominantly applied in the humanities and social sciences (HSS) and partially in STEM disciplines. As part of its planned activities, the project also includes updating the essential metadata of FUP journals in Wikidata, with a focus, in this case as well, on gender representation, which is sometimes absent or ambiguous due to the common practice of using initials for authors' first names. Wikidata is a crucial reference tool for projects and applications requiring metadata, from the simplest to the most complex, which today increasingly rely on artificial intelligence. The quality of data in Wikidata is therefore an essential element for effective knowledge organization in the digital world and in scholarly communication. Adding accurate metadata to publications also facilitates their global findability and the assignment of reviewers in internal journal processes. The contribution of the two researchers, in the form of a lightning talk, will aim to present the ongoing project, which will officially start in January 2025 and will therefore not yet have results suitable for a full paper. Nevertheless, we believe it is important to begin disseminating it, particularly regarding the project component related to Wikidata, and the June conference “Wikidata and Research” is certainly the appropriate venue for this purpose."
    },
    {
        "title": "HQWiki: A Pipeline for High-Quality Monolingual Datasets",
        "url": "https://openreview.net/forum?id=RNRGwg4dTq",
        "abstract": "Creating high-quality monolingual datasets poses significant challenges due to cross-language contamination, metadata noise, and inconsistent quality control, especially for less-resourced languages. We present HQWiki, a pipeline for extracting and validating clean monolingual content. While initially developed for Wikipedia data, our framework introduces universal approaches to dataset preparation that can be applied to various content sources.\n\nThe pipeline implements a novel multi-stage approach combining character set validation, context-aware filtering, and adaptive language detection. Each processing stage addresses specific quality aspects through configurable filters: structural analysis for metadata removal, language boundary detection, and content validation. The system supports integration of multiple language identification tools with customisable confidence thresholds, making it particularly effective for languages where standard tools provide insufficient accuracy.\n\nEvaluation across multiple language families demonstrated the effectiveness of our approach, with processed datasets showing significant reduction in noise compared to raw Wikipedia exports. The framework maintained high accuracy in language identification even for closely related languages. Importantly, the modular architecture allows researchers to adapt filtering criteria and extend the pipeline for specific research requirements.\n\nThe toolkit provides a foundation for systematic dataset creation methodology that can be applied across different language families and research purposes. By open-sourcing both our framework and creating monolingual datasets, we enable researchers to create and maintain their own high-quality datasets while ensuring transparency and reproducibility in the data preparation process."
    },
    {
        "title": "Filologia e Wikidata. Per una riorganizzazione del lessico delle Edizioni critiche",
        "url": "https://openreview.net/forum?id=IBIXAzbctF",
        "abstract": "Il lessico della filologia, in particolare quello legato alle edizioni critiche, trova su Wikidata una rappresentazione scarsa e frammentaria. Gli item relativi ai concetti fondamentali di questo ambito sono pochi, spesso sommari, difficili da reperire tramite query SPARQL e privi delle sfumature concettuali e del dibattito intellettuale che la filologia ha sviluppato nel corso del tempo. Questa lacuna compromette l’efficacia di Wikidata come strumento per la ricerca accademica e la condivisione interdisciplinare della conoscenza. La proposta mira a riorganizzare e strutturare il lessico delle edizioni critiche utilizzando sia Wikidata sia Wikibase, la piattaforma open source alla base di Wikidata. Wikibase consentirà di creare un ambiente controllato e personalizzabile per la modellazione e il test del lessico, garantendo una maggiore flessibilità nell’organizzazione dei dati e facilitando un eventuale trasferimento su Wikidata. Il progetto utilizza come punto di partenza il LexiconSe (Lexicon of Scholarly Editing), una risorsa collaborativa, aperta e multilingue che raccoglie definizioni provenienti da articoli, monografie e altre fonti accademiche sulle edizioni critiche e sulla filologia in senso lato. LexiconSe sarà integrato in una istanza di Wikibase, dove i termini saranno organizzati prendendo come punto di riferimento ontologie consolidate come la Critical Apparatus Ontology (CAO) e la Scholarly Editing Ontology. La proposta intende coinvolgere la comunità accademica, la comunità Wikidata e Wikimedia, in senso più ampio, favorendo un approccio collaborativo che porti a riflettere sulla necessità di elaborare linee guida per i contributori relativamente alle tematiche della filologia."
    },
    {
        "title": "Using Wikidata in the European Literary Bibliography: A Reproducible Approach",
        "url": "https://openreview.net/forum?id=DQu8ELtJbH",
        "abstract": "GLAM institutions (Galleries, Libraries, Archives, and Museums) have been exploring new ways to make available their digital collections. Wikidata has emerged as a leading approach with which to enrich their digital collections [1]. In parallel, new trends such as Labs and Collections as data promote the publication of digital collections suitable for computational use [2] as well as the use of reproducible code in the form of Jupyter Notebooks [3].\n\nThe European Literary Bibliography (ELB) is a project of the Institute of Czech Literature (Czech Academy of Sciences) and the Institute for Literary Research (Polish Academy of Sciences). It intends to open bibliographic data for literary studies at the European level holding resources from several institutions.\n\nFollowing the Collections as data principles and focusing on the ELB, this work provides a reproducible framework including several steps for publishing and reusing digital collections based on literary bibliographies made available by GLAM institutions [4]. It also presents a collection of DH research scenarios to show how data can be explored and reused. This work is the result of an ATRIUM Transnational Access Scheme Grant. The results are available in the form of a repository of reproducible code.\n\nA reproducible framework to transform bibliographic metadata to Collections as data - This section presents the framework to publish and reuse digital collections in the form of Collections as data [4].\n\nData extraction refers to the selection of data relevant to a specific topic (e.g., author, organization or theme). Data modelling aims at ensuring machine-readable bibliographic metadata, using ontologies and vocabularies. The transformation and enrichment step refers to the transformation of the data into Linked Open Data (LOD) using RDF to describe metadata as triples as well as the use of Wikidata to enrich the metadata. The data quality step ensures the high quality of the RDF data. The publication requires the inclusion of additional documentation including aspects such as provenance and licensing. Finally, the published datasets can be reused in various ways (e.g., prototypes or research scenarios defined by DH scholars).\n\nDefining research scenarios - After applying the proposed framework to the ELB and exploring new uses of the data, a selection of research scenarios were defined to illustrate data reuse and integration using Wikidata as a main repository with which to enrich the metadata: i) comparative analysis of provincial vampire novels in Spain; ii) republican writers who emigrated during the Spanish Civil War; and iii) geographical distribution of publications about specific Spanish writers. Limitations were identified in terms of scope and completeness to meet researchers’ needs\n\nConclusions - This work advances the publishing of digital collections in computationally usable forms describing how Wikidata can be used to explore new ways of analysis of the data. Future research directions include extending and implementing the research scenarios, and applying and adapting the framework to other domains.\n\nBibliography - [1] Candela, G., Cuper, M., Holownia, O. et al. A Systematic Review of Wikidata in GLAM Institutions: a Labs Approach. TPDL (2) 2024: 34-50 [2] Candela, G., Gabriëls, N., Chambers, S., et al. (2023), \"A checklist to publish collections as data in GLAM institutions\", Global Knowledge, Memory and Communication, Vol. ahead-of-print No. ahead-of-print. https://doi.org/10.1108/GKMC-06-2023-0195 [3] Candela, G., Chambers, S., Sherratt, T. An approach to assess the quality of Jupyter projects published by GLAM institutions. J. Assoc. Inf. Sci. Technol. 74(13): 1550-1564 (2023) [4] Candela, G., Rosiński, C., & Margraf, A. (2024). A reproducible framework to publish and reuse Collections as data: the case of the European Literary Bibliography. https://doi.org/10.5281/zenodo.14106707"
    },
    {
        "title": "The Journal of Open Humanities Data. Bridging open data and Wikidata for the Humanities",
        "url": "https://openreview.net/forum?id=YzLzBkneKb",
        "abstract": "Wikidata serves as a critical tool for enriching and interconnecting datasets, enabling researchers to explore relationships across diverse domains (Farda-Sarbas and Müller-Birn 2019; Neubert 2017). It offers a centralized platform for integrating identifiers, metadata, and semantic links, and allows for the creation of interoperable and reusable datasets supporting advanced analysis and interdisciplinary research. At the Journal of Open Humanities Data (JOHD), we embrace the potential of Wikidata to amplify the impact of open data for research in the humanities. Through the publication of data papers (peer-reviewed articles describing datasets, their methodologies, and their reuse potential), JOHD ensures that humanities datasets are accessible and reusable. Our mission aligns with the principles of platforms like Wikidata, emphasizing transparency, accessibility, and collaboration to elevate the role of data in advancing scholarly work and public engagement (Wigdorowitz et al. 2024). This poster highlights the synergies between JOHD and Wikidata, focusing on how the journal’s principles of open access, reusability, and reproducibility complement Wikidata’s capabilities as a linked open data hub. This collaboration can enhance the value and impact of humanities research in the digital age with JOHD acting as a bridge to encourage humanities scholars to engage with Wikidata by providing guidance on integrating datasets into Wikidata. We present case studies of data papers published in JOHD, showing how they have used Wikidata for dataset creation. For instance, linking place names in historical newspapers to Wikidata (Coll Ardanuy et al. 2022) enhances cultural heritage accessibility. Multilingual cultural heritage information, such as historical Chinese kung fu masters, can be integrated with Wikidata into reusable and human-centered knowledge graphs (Hou and Yuan 2023). Further, Wikidata ensures the reusability and transparency of bibliographical data, supporting JOHD’s emphasis on reproducible research (Malínek et al. 2024). We also comment on published datasets that do not mention Wikidata but could potentially benefit from its integration to enhance interdisciplinarity (e.g., Farina 2023). Finally, we explore how datasets published in JOHD and integrated with Wikidata can enhance the visibility and discoverability of research (cf. McGillivray et al. 2022) by tracking dataset reuse and citation within the Wikidata ecosystem.\n\nReferences Coll Ardanuy, M., Beavan, D., Beelen, K., Hosseini, K., Lawrence, J., McDonough, K., Nanni, F., van Strien, D., & Wilson, D. C. S. (2022). A Dataset for Toponym Resolution in Nineteenth- Century English Newspapers. Journal of Open Humanities Data, 8(1), 3, pp. 1–7. DOI: https://doi.org/10.5334/johd.56 Farda-Sarbas, M., & Müller-Birn, C. (2019). Wikidata from a Research Perspective - A Systematic Mapping Study of Wikidata. ArXiv, abs/1908.11153. https://doi.org/10.48550/arXiv.1908.11153 Farina, A. (2023). Lost at Sea: A Dataset of 25+ SEA Words Morpho-Semantically Annotated in Ancient Greek and Latin. Journal of Open Humanities Data, 9: 24, pp. 1–7. DOI: https://doi.org/10.5334/johd.139 Hou, Y., & Yuan, L. (2023). Building a Knowledge Graph of Chinese Kung Fu Masters From Heterogeneous Bilingual Data. Journal of Open Humanities Data, 9: 27, pp. 1–12. DOI: https://doi.org/10.5334/johd.136 Malínek, V., Umerle, T., Gray, E., Heibi, I., Király, P., Klaes, C., Korytkowski, P., Lindemann, D., Moretti, A., Panušková, Ch., Péter, R., Tolonen, M., Tomczyńska, A., & Vimr, O. (2024). Open Bibliographical Data Workflows and the Multilinguality Challenge. Journal of Open Humanities Data, 10: 27, pp. 1–14. DOI: https://doi.org/10.5334/johd.190 McGillivray, B., Marongiu, P., Pedrazzini, N., Ribary, M., Wigdorowitz, M., & Zordan, E. (2022). Deep Impact: A Study on the Impact of Data Papers and Datasets in the Humanities and Social Sciences. Publications, 10(4), 39. https://doi.org/10.3390/publications10040039 Neubert, J. (2017). Wikidata as a Linking Hub for Knowledge Organization Systems? Integrating an Authority Mapping into Wikidata and Learning Lessons for KOS Mappings. NKOS@TPDL, 1–12. Wigdorowitz, M., Ribary, M., Farina, A., Lima, E., Borkowski, D., Marongiu, P., Sorensen, A. H., Timis, C., & McGillivray, B. (2024). It Takes a Village! Editorship, Advocacy, and Research in Running an Open Access Data Journal. Publications, 12(3), 24. https://doi.org/10.3390/publications12030024"
    },
    {
        "title": "THE \"MARE MAGNUM\" ON WIKIDATA",
        "url": "https://openreview.net/forum?id=qqynzN8K5P",
        "abstract": "The Marucelli's \"Mare Magnum\" is one of the most interesting and least studied bibliographic repertories in the world. Thanks to an agreement between the SAGAS Department of the University of Florence and the Biblioteca Marucelliana, which holds the eighteenth-century manuscript, a series of studies by students of bibliography and the history of the book has brought to light the authors and printers cited by Marucelli. These data, checked against authority files, formed the basis of a massive insertion in wikidata, linking the \"Mare Magnum\" to its bibliographical content."
    },
    {
        "title": "Wikidata e Thesaurus Nuovo soggettario: insieme per costruire un’ontologia di dominio nell’ambito della fotografia",
        "url": "https://openreview.net/forum?id=AnH21anTtE",
        "abstract": "This paper discusses the collaboration between the Wikidata Group at the University of Florence, librarians from the Research and Semantic Indexing Tools Department at the National Central Library of Florence (BNCF), and Wikimdata volunteers, aimed at creating a domain ontology for photography in Wikidata. This project involves aligning Wikidata with the Thesaurus Nuovo soggettario through semantic and technical integration. The collaboration leverages the open, structured data of the Thesaurus, adhering to international standards and the FAIR principles (Findability, Accessibility, Interoperability, and Reusability).\n\nKey challenges include:\n\nStructural and functional differences between the two tools. The cultural scope, user base, and mission of the participating institutions. The complex, specific, and sometimes ambiguous nature of the specialized terminology involved. These challenges revealed semantic misalignments, prompting revisions in both databases to better align their meanings and terms."
    },
    {
        "title": "Breaking the pattern",
        "url": "https://openreview.net/forum?id=EeyzfPT9Gf",
        "abstract": "The Philosophy Library of the University of Milan holds about 80 thousand volumes. Most of the collection is open-shelved and divided into two main parts: “History of Philosophy” gathers all publications up to the end of the 19th century and is organized in a chronological order; \"Contemporary Philosophy” comprises texts by authors from 20th century onwards and is divided along both a thematic and a linguistic criterion. Over the years, the lack of updates in the thematic sectors has led to a growing imbalance in the distribution of the volumes in contemporary philosophy section: those belonging to innovative areas of research or fields that are not covered by the current subdivision have mostly been assigned to the linguistic sectors, which have consequently become extremely large. Therefore, the content affinities between contiguous volumes are lost and user orientation is compromised. With the aim of revising this scheme, we decided to increase the number and variety of thematic sectors. By so doing, we could prioritize the allocation of new volumes within them, allow an easier relocation of those placed in the linguistic sectors, minimize the internal variance within each sector while maximizing the external variance between sectors. The main problem we faced was reducing the amount of work and arbitrariness involved in identifying the volumes to be moved into the new thematic sectors. Therefore, we identified a test sector and designed a workflow capable of automating at least part of the selection work. To decide which texts should be moved to the test sector, we compared the database used in the library to assign shelfmarks (containing a list of authors and the related sectors) with lists drawn from two qualified disciplinary sources. A Wikidata dataset was used to maximize matches between lists and reduce noise. The SPARQL query was based on the Date of birth, Field of work, and Occupation properties. Wikidata was also used to identify, in our local database, only the entries having the property Person and obtain data in a format useful for subsequent analysis. After normalizing the data to make them comparable, we cross-referenced the four lists to obtain a matrix in which each author was either present or absent. All entries that did not appear in any of the three external lists (the two qualified sources and the Wikidata dataset) were eliminated, thus leaving us with a preliminary list of approximately 400 authors that were candidates for the new sector. Each of these names was then reviewed individually to define whether it belonged to the test sector or a different one. The list was organized chronologically according to date of birth of the authors and used to plan the transfer of the volumes to the new shelves. The use of Wikidata thus enabled us to reduce data analysis from an initial dataset of over 20 thousand entries to just 400 with a high degree of precision and retrieval."
    },
    {
        "title": "Committenze semantiche: Wikidata per la ricerca storico-artistica",
        "url": "https://openreview.net/forum?id=GraNomoG42",
        "abstract": "Negli ultimi anni le scienze umane hanno esplorato le potenzialità delle tecnologie digitali e in particolare le applicazioni del Web semantico per studiare fenomeni culturali complessi come le committenze artistiche. La ricerca intende presentare una possibile applicazione di Wikidata in ambito storico-artistico per rappresentare in un ambiente semantico e collaborativo il mecenatismo di una famiglia nobile, la famiglia Buonaccorsi di Macerata, una delle dinastie più influenti dello Stato Pontificio tra il XVII e XVIII secolo. Attraverso lo spoglio sistematico delle fonti è stato possibile realizzare in Wikidata un dataset di oltre 400 elementi ascrivibili al contesto della casata maceratese. Ogni elemento relativo alla committenza Buonaccorsi è stato correttamente descritto e relazionato attraverso le proprietà Wikidata ritenute più idonee, permettendo così di restituire una visione diacronica e interconnessa del fenomeno artistico. Inoltre, la possibilità di referenziare le asserzioni ha rinforzato l'affidabilità dei dati, rendendo in questo modo più autorevole il loro riuso in ambito scientifico. Questo studio dimostra come Wikidata, attraverso i Linked Open Data, possa offrire nuove prospettive per la ricerca storico-artistica, raccogliendo in un’unica piattaforma - in forma di dati - fonti disseminate presso diverse sedi, ottimizzando le attività di legate al recupero delle informazioni. Inoltre, evidenzia come le piattaforme aperte possano innovare la ricerca nelle scienze umane, favorendo la costruzione e la diffusione di nuove conoscenze e strumenti di ricerca per gli storici e per le comunità d’interesse."
    },
    {
        "title": "Linking European Commission data with Wikidata: Unlocking the potential of linked open data for all",
        "url": "https://openreview.net/forum?id=6BSZLEJA7f",
        "abstract": "The lack of uniformity in codes and names to identify the same entity is inefficient and hinders the implementation of interoperable IT systems. To address this issue, the European Commission has prioritised the development of data policies and guidelines for reference data to set high-level principles for ensuring data interoperability, user-friendly and data-driven administration, and digital-ready policymaking. The Publications Office of the European Union (OP), in its capacity as data steward, bears the responsibility of maintaining the Commission’s corporate reference data, ensuring that the data is FAIR and accessible in all European Union official languages. Although the data is free and open to everyone, further commitments are needed from the OP to ensure the interoperability of EU data with other open linked data resources.\n\nIn the autumn of 2024, OP completed the alignment between Wikidata and its standardised corporate list of countries and territories, which was endorsed as a corporate data asset by the European Commission in 2023. The aim of this exercise was to test the matching workflow of an AI-based alignment tool, developed for the Directorate-General for Communications Networks, Content and Technology, in a rather specific domain, and to explore the possibility of incorporating Wikidata’s external data.\n\nDuring the exercise, 319 exact matches were successfully identified out of a total of 336 entities in the data asset. The alignment package is available in SKOS format, retrievable from Cellar, OP’s common data repository, and the matches are shown on the individual entities when browsing the website. The tools used by OP to create and validate alignments are presented. As the data asset follows the conventions of the EU’s Interinstitutional Style Guide for writing country names, and includes politically sensitive and disputed territories, some special cases required manual matching and further verification. Some territories disputed by the parties to a different extent showed the limitations of exact matches when the two datasets defined them differently (if at all).\n\nPotential EU data assets for future alignment are introduced, including the authority list of currencies and currency subunits, and EuroVoc, the EU’s multidisciplinary and multilingual thesaurus covering the activities of the EU. EuroVoc is explored with specific focus on its already existing Wikidata property can be used to enhance the content available. The process of aligning a multidisciplinary thesaurus presents many challenges, and this paper presents some possible solutions, such as processing data in smaller batches, focusing on related domains, or following the structure of the thesaurus.\n\nThe Publications Office recognises the value of active community engagement to maximise the potential of alignment between its data assets and Wikidata, while at the same time, by examining its process for publishing and maintaining up-to-date alignments, the Wikidata community will gain a deeper understanding of how it can provide the necessary support and expertise to facilitate more efficient collaboration with public bodies. The aim of the presentation would be to explore the potential of EU data for Wikidata and to see how these data assets could be better aligned and used for mutual enrichment."
    },
    {
        "title": "Wikidata e biblioteche per l'AI",
        "url": "https://openreview.net/forum?id=uzYOK3x69D",
        "abstract": "L'intervento propone l'utilizzo di Wikidata come un indice di database e come punto di accesso per la scoperta e la raccolta automatizzata di base dati. Inoltre si propone di presentare delle proposte sul ruolo che Wikidata e biblioteche potranno avere all'interno degli ambiti di ricerca relativi al Natural Language Processing, al Machine Learning e all'Intelligenza Artificiale.\n\nLa natura aperta dell'ecosistema di Wikidata e la possibilità di editare e creare 'liberamente' nuovi elementi, se da un lato sembra permettere una maggior rappresentatività delle entità del mondo reale rispetto ad altri sistemi chiusi, dall'altra pone una serie di criticità per quanto riguarda l'attendibilità e la qualità dei dati disponibili. Per superare queste criticità si propone l'utilizzo di Wikidata come indice, ovvero come sistema di scoperta e aggregazione di dati autorevoli in grado di condurre, partendo da una entità, a dati ospitati esternamente accessibili in forma machine-readable e direttamente importabili nel proprio sistema in quanto espressi sotto forma di Linked Open Data. Utilizzando l’esempio della collezione iconografica della Biblioteca dell’Accademia di architettura di Mendrisio, questo intervento propone una serie di strategie in grado di condurre a un miglioramento della ricercabilità e del riuso delle collezioni digitali delle biblioteche. Le ricadute che questo approccio consente spaziano dalla ottimizzazione della gestione automatizzata degli authority files, fino alla disseminazione di dati nevralgici ai fini della valorizzazione del patrimonio culturale.\n\nIl secondo punto di cui si vuole discutere riguarda il ruolo che le biblioteche potrebbero rivestire nello sviluppo dell’Intelligenza Artificiale, ponendosi come soggetti attivi dello sviluppo della componente software e non solo come utilizzatori finali di prodotti commerciali. In una fase storica, e in un paese come l’Italia, in cui la fase di addestramento delle macchine è una questione aperta sulla quale è possibile agire positivamente, riteniamo sia possibile e necessario ipotizzare un coinvolgimento delle biblioteche. La qualità del risultato prodotto da Modelli Linguistici di grandi dimensioni (LLMs), così come da modelli di dimensioni ridotte più specifici, dipende inevitabilmente dalla quantità e qualità dei dati utilizzati per l’addestramento. Nel nuovo ecosistema digitale le biblioteche possono supportare i propri utenti agendo come data provider per i ricercatori come per i servizi commerciali generalisti."
    },
    {
        "title": "Behind the Edits: Exploring Human-Bot Collaboration in Wikipedia",
        "url": "https://openreview.net/forum?id=hjkVICSOMy",
        "abstract": "Content in Wikipedia is added by both human volunteers and automated bots. Along the spectrum from purely human edits to purely automated edits, humans also use an array of automated tools that could be classified as light automation (e.g. spell checking) to heavier automation (e.g. content translation from one language to another) to generative AI tools with light human review. Unlike many other corpora of data, Wikipedia keeps detailed metadata about the source of edits (human or bot) and the tools used in the editing process. This offers a unique opportunity to explore how automation impacts content quality, breadth, and update frequency. Moreover, such categorization empowers researchers to make informed, data-driven decisions about the appropriate balance between bot and human involvement when using the resulting data for various purposes. Besides the high level classification of human or bot, Wikipedia tracks a wide variety of special tags that also shed light on the range of automation even for edits tagged as human edits. In this study, we categorize the special tags added in Wikipedia according to which tags indicate information about the level of automation-created content versus human-created content and which do not. We started off with a list of over 300 tags, each representing a unique type of edit or action logged in the metadata of a Wikipedia page. These tags ranged from general metadata indicators to more specific labels highlighting the use of tools, automated processes, or manual interventions. We first identify over 50 tags that we classify as relevant for placing edits to Wikipedia content on a spectrum from mostly human to mostly automated. Our categorization process was guided by several criteria, including whether a tag explicitly indicated the use of automation (e.g., tags associated with bots like \"IABot\" or “AWB\") or manual edits requiring human oversight (e.g., \"Manual revert\" or \"DiscussionTools\"). Second, we classified tags on a scale of 1-5 based on the degree of automation implied by the tag. Special tags in Wikipedia are added voluntarily and in that sense tracking is not perfect, but they still represent some of the best attempts at tracking the process by which content is created. Unlike many corpora which do not invest at all in such detailed tracking of the creation process, Wikipedia’s metadata allows researchers to include or exclude certain types of content based on how it was created and depending on the goals of the downstream task. For some tasks, automated or bot-created content could be perfectly useful where for other tasks including bot-generated content could degrade the quality of any models or articles produced. Our research aims to establish a clearer understanding of how content in Wikipedia is generated and then to assess its suitability for a variety of test cases."
    },
    {
        "title": "The visual side of knowledge: the role of images in Wikipedia and Wikidata",
        "url": "https://openreview.net/forum?id=0ja5iGNMbd",
        "abstract": "This proposal addresses a critical yet underexplored aspect of Wikimedia projects: the visual representation of knowledge. While extensive research has examined the textual content and its technical and social dimensions, the study of images as representations of knowledge remains significantly less developed. Often treated as supplementary information, images however play a crucial role in conveying information and shaping the representation and understanding of concepts. This is true not only for photographs and digitized artifacts, such as portraits, but also for composited images like maps, charts, and diagrams.\n\nThis study aims to bridge this gap by identifying the most viewed images on Wikimedia Commons and analyzing their usage across Wikipedia and Wikidata.\n\nKey questions include: Which images are used to represent specific concepts? Which images are designated as the primary representation of Wikidata items through the image (P18) property? And which images have the greatest visual impact across Wikimedia projects?\n\nMethodologically, the study is structured as follows: The first step involves identifying the “most seen” images. Unlike Wikimedia pages, it is challenging to determine precisely when an image is “viewed,” as no publicly accessible data directly addresses this. Instead, the study uses a proxy metric: mediacounts, which measure the number of times an image is delivered to a client computer, either as a thumbnail or in high resolution. While this metric does not fully account for technical factors like preloading by client browsers, it serves as a reliable approximation.\n\nBy collecting and aggregating mediacount data over the past year, the study compiles a list of the 10,000 most requested images, both as thumbnails and in high-quality formats. Images are then classified based on their content (e.g., portraits, insignia, diagrams, maps).\n\nFinally, each image’s usage on Wikidata is analyzed, focusing on which properties link them.\n\nThe study’s results highlight how the most viewed images play an identity-defining role, providing unique representations for various concepts. This role is highly correlated with their usage on Wikidata. Images such as portraits, flags, insignia, and chemical representations feature prominently among the most viewed. These findings underscore the critical role of Wikidata in elevating certain images as primary representations of items, often facilitated through their automatic inclusion in templates."
    },
    {
        "title": "Monitoring researchers profiles on Wikidata: a comparative analysis of country-specific metrics",
        "url": "https://openreview.net/forum?id=KEwR5YvKj0",
        "abstract": "Wikidata, the open and structured data repository of Wikimedia projects, shows an increasingly significant potential as a platform for organizing and sharing metadata in the academic ecosystem. One aspect is the information related to researchers and authors of peer-reviewed content.\n\nThis analysis details the use of the QLever SPARQL query service to describe and monitor researchers’ items on Wikidata, and related fundings. By analyzing the distribution and quality of key metrics such as statements and external identifiers, the study provides a comparative analysis across profiles linked to different countries, focusing on researchers’ educational backgrounds, affiliations, and employers.\n\nKey variations in data coverage, completeness, and quality across different academic ecosystems are investigated, specifically: • Item distribution: how many researcher items can be associated with each country? • Statement distribution: which properties are most added to researcher items? • Identifier distribution: which databases are commonly connected to Wikidata? This comparative approach enables us to reveal patterns and potential biases in how research communities are represented on Wikidata, shedding light on the global academic landscape.\n\nThese findings have potential implications for data accuracy, reliability, and equity on Wikidata, as well as for broader discussions on the digital identity of researchers. We propose strategies for further improving data quality, enhancing representation, and fostering collaboration between all stakeholders, primarily Wikidata editors, academic institutions, and research organizations based on existing active projects. By addressing these challenges, we aim to strengthen the framework for supporting the academic community on Wikidata and contribute to building a more comprehensive and accurate global knowledge base."
    },
    {
        "title": "Automatic Verification of References of Wikidata Statements",
        "url": "https://openreview.net/forum?id=UBGHipMmb5",
        "abstract": "Wikidata is one of the world's most important machine-readable data assets. It is used by web search engines, virtual assistants such as Siri and Alexa, fact checkers, and in over 800 projects in the Wikimedia ecosystem. Wikidata contains information about 100 million of topics edited daily by 24 thousand active editors. Manually checking whether an individual reference supports the claim of a Wikidata statement is not very difficult, but it is a slow and somewhat tedious process. Given the overall number of statements to check, the collaborative nature of the knowledge graph, and the fact that referenced documents can change over time, preserving the quality of the references is an onerous process requiring continuous intervention. In this paper, we present ProVe - a reference verification tool for Wikidata statements developed from state-of-the-art research for quality assurance in collaboratively constructed knowledge graphs. ProVe harnesses the power of LLMs to automatically verify and assess the quality of the references in Wikidata."
    },
    {
        "title": "Amharic Audio Data Search Engine using Text-Based Spoken Term Detection with Models",
        "url": "https://openreview.net/forum?id=HPYI1sHybV",
        "abstract": "The generation of audio files from various sources, including the internet and social media, has increased significantly in the rapidly expanding digital landscape. It is difficult to efficiently access specific spoken words from this vast collection of Amharic audio data. To address this, we propose a novel method that combines Text-Based Spoken Term Detection (STD) with models. Our methodology includes speech segmentation with pydub, the development of an ASR model, and the implementation of keyword-based STD. The ASR model successfully transcribes audio files, allowing meaningful keywords to be extracted for more accurate and frequent search queries. An analysis of 37 audio files reveals that the sentence error rate (SER) is 91.7 percent (33 of 36 sentences have errors) and the word error rate (WER) is 98.3 percent (285 of 290 words have errors). It improved search accuracy and efficiency for specific spoken terms, significantly improving search capabilities for users of Amharic multimedia resources. However, the study emphasizes the need for a larger dataset to improve transcription capabilities and reduce errors, with the potential to revolutionize Amharic audio search engines and empower users in accessing precise information from Amharic audio data, ultimately transforming how we interact with and use Amharic audio resources."
    },
    {
        "title": "test",
        "url": "https://openreview.net/forum?id=qIawraxzwP",
        "abstract": "test"
    }
]