## 资料

== Vibe Coding 零基础教程 02：自建信息流 ==

如何构建自己的信息流和数据库？

我自今年年初就开始用 Hacker News 的高质量数据构建自己的信息流，减少信息茧房。两个月前我上线了 http://newshacker.me 这个网站，将个人的信息流产品化。

我认为构建自己的信息流是一件极为重要的事情，因为它决定了我的视野和思考广度。因此，我希望把开发 http://newshacker.me 中积累的经验分享给大家。

1️⃣ 找到高质量信息源并实现自动化

这个世界的信息纷繁复杂，来自媒体、论坛、社交网络等各种渠道。随着网络爬虫的滥用，原本开放的数据也逐渐提高了获取门槛。

如今，有了 Coding Agent，构建自动化信息抓取流程反而比找到真正高质量的信息源更简单。因此，我们可以首先对信息源进行分类，然后逐步实现自动化。

🟢 第一类：提供 API 的服务

例如 Hacker News、GitHub、Reddit 或 Product Hunt 等网站，这些服务通常有面向开发者的 API 接口。你可以使用 Research Agent（如 GPT-5）帮你判断某个网站是否提供 API 及其使用方式，然后开发一个最小可行的程序，从 API 获取数据。

🟢 第二类：提供 RSS 的服务

比如论文期刊或知名媒体，大多提供 RSS 或 ATOM 格式的 Web Feed。这种方式获取数据成本低，AI 可以帮你快速构建 RSS 抓取工具，定期自动获取最新信息。

🟢 第三类：需要网页爬虫

有些内容平台未提供 API 或 RSS，此时可请 AI 利用 Python 的第三方库实现网页爬虫，抓取网页并提取所需数据。如果难度较高或平台限制明显，可考虑下一条的方案。

🟢 第四类：类似推特的平台

推特曾经极为开放，但近年来 API 成本显著增加，目前更推荐通过 AI 辅助，利用 Python 和 Playwright 等工具实现。这一部分难度较高，值得单独展开为一篇教程。

🟢 第五类：付费获取的信息

尽量避免使用解锁 Paywall 插件，更合规的方式是付费订阅后再利用前述方法自动抓取数据。

2️⃣ 将数据存入数据库

上一篇教程已经介绍了如何构建数据库，方案包括本地或云端部署 MySQL 或 SQLite 等数据库。

我个人更喜欢 Cloudflare 的 Worker，每个 Worker 可以设置为定期自动运行，比如每 5 分钟获取 RSS 或 API 数据并存入数据库。你可以让 AI 告诉你如何在 Cloudflare D1 数据库（SQLite）中创建数据库、设计表结构，并指导编写 Worker 脚本。经过几次调试后，你就能快速搭建出可用的数据存储方案。

3️⃣ 持续完善业务逻辑

刚开始编写 Worker 代码时，不可能一步到位。对于新手来说，完整设计所有产品逻辑和规则是困难的，这是正常的。

比如最初设计的规则是「抓取 RSS 并存入数据库」，但你很快会发现数据重复存储的问题。这时需要更新规则为「抓取 RSS 后只存入新增数据」。如何定义「新增」数据，就是一个不断迭代优化的过程。

一开始没想清楚没关系，在实际使用过程中逐步完善逻辑即可，很多成功的产品也是这样不断迭代优化的。

4️⃣ 数据输出和应用

拥有数据后，如何高效使用这些数据呢？这里提供几个快速实现的方案。

最简单的方法是搭建网页进行输出。如果使用 Cloudflare，你可以请 Coding Agent 帮你搭建 Cloudflare Pages，自动从 D1 数据库取数据，以信息流的方式展现。

增加一点难度的话，可以利用 LLM 实现内容翻译。流程是构建一个新的 Worker，定期调用 OpenAI 或 Claude API，翻译数据并存入数据库的相应字段中。

另外，如果你想将每一条新信息自动推送到 Telegram 频道中，也完全可行。只需要创建 Telegram bot 和频道，通过 API 自动实现内容推送，这个流程 AI 同样非常熟悉。

更进阶的玩法，是对多信息源的数据进行整合和聚类。这需要运用大语言模型，并学习结构化输出技巧，以便高效处理和呈现大量数据。这一内容也值得后续专门展开讲解。

5️⃣ 总结

自建信息流的核心，是明确需求、确定信息源、构建自动化抓取流程，并实现数据的高效存储与应用。即便一开始并不完美，通过不断的迭代和优化，你一定能构建出一个属于自己的信息流产品。

## 思路

将RSS订阅与全文解析、大模型初评部署到cloudflare上，最终结果保存到 D1 数据库