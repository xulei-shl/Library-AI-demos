# 提取器实现指南

本文档提供BigThink和Wikipedia提取器的实现思路和示例代码。

## BigThink提取器实现

### 需求分析
- **输入**: article['link'] - 文章链接
- **策略**: 使用Playwright无头浏览器爬取全文
- **难度**: ⭐⭐⭐

### 实现步骤

1. **安装依赖**
```bash
pip install playwright
playwright install chromium
```

2. **实现代码示例**

```python
# src/core/subject_bibliography/content_extractors/bigthink.py
from typing import Dict, Any
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeout
from bs4 import BeautifulSoup
from .base import BaseContentExtractor
from src.utils.logger import get_logger

logger = get_logger(__name__)


class BigThinkExtractor(BaseContentExtractor):
    """Big Think全文提取器
    
    策略: 使用无头浏览器从link爬取全文
    """
    
    def __init__(self):
        self.timeout = 30000  # 30秒超时
        self.headless = True
    
    def can_handle(self, source_name: str) -> bool:
        """判断是否能处理该RSS源"""
        return "big think" in source_name.lower() or "bigthink" in source_name.lower()
    
    def extract(self, article: Dict[str, Any]) -> Dict[str, Any]:
        """
        使用无头浏览器提取全文
        
        Args:
            article: 文章字典，需包含 link 字段
            
        Returns:
            包含 full_text, extract_status, extract_error 的字典
        """
        result = {
            "full_text": "",
            "extract_status": "failed",
            "extract_error": ""
        }
        
        link = article.get("link", "")
        if not link:
            result["extract_status"] = "skipped"
            result["extract_error"] = "link字段为空"
            return result
        
        try:
            with sync_playwright() as p:
                # 启动浏览器
                browser = p.chromium.launch(headless=self.headless)
                page = browser.new_page()
                
                # 访问页面
                logger.info(f"正在访问: {link}")
                page.goto(link, timeout=self.timeout)
                
                # 等待主要内容加载
                # 根据Big Think网站结构调整选择器
                page.wait_for_selector("article", timeout=self.timeout)
                
                # 获取页面HTML
                html = page.content()
                browser.close()
                
                # 解析HTML
                soup = BeautifulSoup(html, "html.parser")
                
                # 提取文章正文
                # 根据Big Think网站结构调整选择器
                article_content = soup.find("article")
                if not article_content:
                    result["extract_error"] = "未找到article标签"
                    return result
                
                # 移除script和style标签
                for tag in article_content(["script", "style"]):
                    tag.decompose()
                
                # 提取纯文本
                text = article_content.get_text(separator="\n", strip=True)
                
                # 清理多余空行
                lines = [line.strip() for line in text.split("\n") if line.strip()]
                full_text = "\n".join(lines)
                
                if full_text:
                    result["full_text"] = full_text
                    result["extract_status"] = "success"
                    logger.info(f"成功提取文章: {article.get('title')} (长度: {len(full_text)})")
                else:
                    result["extract_error"] = "提取的文本为空"
        
        except PlaywrightTimeout as e:
            result["extract_error"] = f"页面加载超时: {e}"
            logger.error(f"提取失败(超时): {link}")
        except Exception as e:
            result["extract_error"] = str(e)
            logger.error(f"提取失败: {link}, 错误: {e}")
        
        return result
```

### 配置调整

在 `config/subject_bibliography.yaml` 中可以添加提取器特定配置：

```yaml
extraction_settings:
  timeout: 30  # 爬取超时时间(秒)
  retry_times: 3  # 失败重试次数
  browser_headless: true  # 无头浏览器模式
  
  # BigThink特定配置
  bigthink:
    article_selector: "article"  # 文章内容选择器
    wait_selector: "article"     # 等待加载的选择器
```

---

## Wikipedia提取器实现

### 需求分析
- **输入**: article['content'] - RSS内容
- **策略**: 从content提取Wikipedia文章URL，然后用requests爬取
- **难度**: ⭐⭐

### 实现步骤

1. **实现代码示例**

```python
# src/core/subject_bibliography/content_extractors/wikipedia.py
from typing import Dict, Any, Optional
import requests
from bs4 import BeautifulSoup
from .base import BaseContentExtractor
from src.utils.logger import get_logger

logger = get_logger(__name__)


class WikipediaExtractor(BaseContentExtractor):
    """Wikipedia每日首页文章全文提取器
    
    策略: 从content字段提取全文URL，然后爬取
    """
    
    def __init__(self):
        self.timeout = 30
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
    
    def can_handle(self, source_name: str) -> bool:
        """判断是否能处理该RSS源"""
        return "wikipedia" in source_name.lower() or "维基" in source_name
    
    def _extract_article_url(self, content: str) -> Optional[str]:
        """从RSS content中提取Wikipedia文章URL"""
        try:
            soup = BeautifulSoup(content, "html.parser")
            
            # 查找链接
            links = soup.find_all("a", href=True)
            for link in links:
                href = link["href"]
                # Wikipedia文章链接通常包含 /wiki/
                if "wikipedia.org/wiki/" in href and ":" not in href.split("/wiki/")[1]:
                    return href
            
            return None
        except Exception as e:
            logger.error(f"提取Wikipedia URL失败: {e}")
            return None
    
    def _fetch_article_content(self, url: str) -> Optional[str]:
        """从Wikipedia URL爬取文章内容"""
        try:
            response = requests.get(url, headers=self.headers, timeout=self.timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, "html.parser")
            
            # Wikipedia文章内容在 id="mw-content-text" 的div中
            content_div = soup.find("div", {"id": "mw-content-text"})
            if not content_div:
                return None
            
            # 只保留段落
            paragraphs = content_div.find_all("p")
            
            # 提取文本
            texts = []
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text:
                    texts.append(text)
            
            return "\n\n".join(texts)
        
        except Exception as e:
            logger.error(f"爬取Wikipedia内容失败: {url}, 错误: {e}")
            return None
    
    def extract(self, article: Dict[str, Any]) -> Dict[str, Any]:
        """
        从content提取URL并爬取全文
        
        Args:
            article: 文章字典，需包含 content 字段
            
        Returns:
            包含 full_text, extract_status, extract_error 的字典
        """
        result = {
            "full_text": "",
            "extract_status": "failed",
            "extract_error": ""
        }
        
        content = article.get("content", "")
        if not content:
            result["extract_status"] = "skipped"
            result["extract_error"] = "content字段为空"
            return result
        
        try:
            # 步骤1: 从content提取Wikipedia文章URL
            article_url = self._extract_article_url(content)
            if not article_url:
                result["extract_error"] = "未找到Wikipedia文章URL"
                return result
            
            logger.info(f"提取到Wikipedia URL: {article_url}")
            
            # 步骤2: 爬取文章内容
            full_text = self._fetch_article_content(article_url)
            if not full_text:
                result["extract_error"] = "爬取文章内容失败"
                return result
            
            result["full_text"] = full_text
            result["extract_status"] = "success"
            logger.info(f"成功提取Wikipedia文章: {article.get('title')} (长度: {len(full_text)})")
        
        except Exception as e:
            result["extract_error"] = str(e)
            logger.error(f"提取Wikipedia文章失败: {article.get('title')}, 错误: {e}")
        
        return result
```

---

## 测试提取器

### 单元测试示例

```python
# tests/test_extractors.py
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.core.subject_bibliography.content_extractors import ExtractorFactory


def test_bigthink_extractor():
    """测试BigThink提取器"""
    extractor = ExtractorFactory.get_extractor("Big Think")
    
    article = {
        "title": "Test Article",
        "link": "https://bigthink.com/some-article/"
    }
    
    result = extractor.extract(article)
    
    assert result["extract_status"] in ["success", "failed"]
    if result["extract_status"] == "success":
        assert len(result["full_text"]) > 0
        print(f"✓ 成功提取: {result['full_text'][:100]}...")
    else:
        print(f"✗ 提取失败: {result['extract_error']}")


def test_wikipedia_extractor():
    """测试Wikipedia提取器"""
    extractor = ExtractorFactory.get_extractor("Wikipedia每日首页文章")
    
    # 模拟RSS content
    article = {
        "title": "Test Article",
        "content": '<a href="https://en.wikipedia.org/wiki/Python_(programming_language)">Python</a>'
    }
    
    result = extractor.extract(article)
    
    assert result["extract_status"] in ["success", "failed"]
    if result["extract_status"] == "success":
        assert len(result["full_text"]) > 0
        print(f"✓ 成功提取: {result['full_text'][:100]}...")
    else:
        print(f"✗ 提取失败: {result['extract_error']}")


if __name__ == "__main__":
    print("测试BigThink提取器...")
    test_bigthink_extractor()
    
    print("\n测试Wikipedia提取器...")
    test_wikipedia_extractor()
```

---

## 调试技巧

### 1. 查看中间结果

运行阶段1和阶段2后，可以打开Excel文件查看：
- `extract_status` 列：查看哪些文章提取成功/失败
- `extract_error` 列：查看失败原因
- `full_text` 列：查看提取的文本内容

### 2. 单独测试提取器

```python
from src.core.subject_bibliography.content_extractors import ExtractorFactory

# 获取提取器
extractor = ExtractorFactory.get_extractor("Big Think")

# 测试文章
article = {"title": "测试", "link": "https://..."}

# 提取
result = extractor.extract(article)

# 查看结果
print(result)
```

### 3. 调整超时和重试

在提取器中添加配置支持：

```python
def __init__(self, config=None):
    self.config = config or {}
    self.timeout = self.config.get("timeout", 30)
    self.retry_times = self.config.get("retry_times", 3)
```

---

## 常见问题

### Q1: 如何处理需要登录的网站？

A: 使用Playwright的持久化上下文：

```python
browser = p.chromium.launch_persistent_context(
    user_data_dir="./browser_data",
    headless=False  # 首次运行时手动登录
)
```

### Q2: 如何处理动态加载的内容？

A: 使用Playwright等待特定元素：

```python
page.wait_for_selector(".article-content", timeout=30000)
# 或等待网络空闲
page.wait_for_load_state("networkidle")
```

### Q3: 如何提高爬取速度？

A: 使用异步并发：

```python
from playwright.async_api import async_playwright
import asyncio

async def extract_async(self, articles):
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        tasks = [self._extract_one(browser, article) for article in articles]
        results = await asyncio.gather(*tasks)
        await browser.close()
        return results
```

---

## 总结

按照以上指南实现BigThink和Wikipedia提取器后：

1. 将代码复制到对应的 `.py` 文件
2. 运行测试验证功能
3. 无需修改其他任何代码
4. 直接运行模块7即可使用

提取器之间完全解耦，互不影响！
