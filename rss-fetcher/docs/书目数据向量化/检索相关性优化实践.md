# 图书检索相关性提升最佳实践

> 适用阶段：基于现有向量化/检索代码进行策略迭代，不直接改动底层架构。

## 1. 背景与现状

- **向量语料**：每本书仅存储一条长文本，由“书名 + 作者 + 简介×2 + 目录(截断)”拼接而成 `src/core/book_vectorization/vectorizer.py:305-332`，目录可长达 3000 字 `config/book_vectorization.yaml:56-69`。
- **检索链路**：`scripts/retrieve_books.py` 调用 `BookRetriever.search_by_text`，单次 dense 检索后直接按 `1 - distance` 排序 `src/core/book_vectorization/retriever.py:51-105`。
- **LLM 组件**：`src/utils/llm` + `config/llm.yaml` 已封装统一调用，可直接在新任务中复用。
- **可用语义资产**：文章交叉分析 Markdown 中的「关键词」「标签」「提及书籍」及段落摘要，尚未系统用于检索。

## 2. 主要瓶颈

1. **语义错位**：文章报告中的概念（如“数字身份流动性”“情感部落”）往往不出现在 Douban 简介/目录，单轮查询难以召回。
2. **结果稀疏**：向量化阶段按较高评分阈值过滤，导致候选书目数量有限 `src/core/book_vectorization/filter.py:70-108`。
3. **噪声向量**：长目录文本稀释主题，dense 相似度被“泛文化研究”语义主导。
4. **缺乏融合策略**：没有多查询合并、关键词加权或 rerank，难以压制伪相关结果。

## 3. 平衡性能 / 成本的最佳实践

### 3.1 从分析报告提取检索资产

1. **结构化解析**：为 `runtime/outputs/cross_analysis/*.md`（如 `20251211_091043_宇宙物质起源与认知边界的双重追问_g1.md`）编写解析器，逐篇文章抽取：
   - 每篇的「关键词」「标签」「深度洞察」列表（支持多组）。
   - 每篇内的「提及书籍」条目（title + context，可能为多本或“无”）。
2. **查询包管理**：生成下列子查询，写入检索上下文：
   - `primary`: 原始母题/摘要句。
   - `tags`: 每个标签或关键词逐条作为短查询。
   - `books`: 提及书籍标题（用于召回“相似书”）。
   - `insight`: 由深度洞察提炼的 1~2 行描述。
3. **优先级**：保证 `primary` 与 `tags` 至少返回 5 条结果，其余子查询可用较小 `top_k` 以控成本。
4. **交互模式**：在 CLI 中提供除 `--query/--query-file` 之外的 `--from-md <path>` 入口，自动触发“解析 → 关键词/提及书籍提取 → 多轮检索 → 去重融合”的整套流程。

### 3.2 多轮检索 + 结果融合

1. **执行顺序**：`primary → tags → insight → mentioned_books`，每轮调用 `BookRetriever.search_by_text`。
2. **结果合并**：
   - 以 `book_id` 为键，记录各子查询得分。
   - 计算 `score = w1 * max(similarity) + w2 * avg(similarity) + w3 * match_count`，建议 `w1=0.6, w2=0.2, w3=0.2`。
3. **评分约束保持**：继续沿用向量化阶段的豆瓣评分阈值，必要时可在融合环节通过 `min_rating` 或加权惩罚进一步提升结果质量。
4. **结果数**：单轮 `top_k=20` 足够；若四轮检索则最多 80 次向量距离，每次调用成本可控。

### 3.3 借助 LLM 生成补充母题

1. **复用统一客户端**：通过 `src/utils/llm` 的 `UnifiedLLMClient` 加载 `config/llm.yaml` 里现有 `article_analysis` 或新增任务，避免重新接入模型。
2. **任务目标**：
   - 将 `关键词/深度洞察` 重写为 2~3 条“检索提示语”。
   - 为每本书生成 1 句“主题摘要”以补充向量文本（后续迭代使用）。
3. **成本控制**：
   - 批量处理多个报告，单任务温度维持 0.3~0.4，最大 token 约 400。
   - 只在检索效果不佳或新增主题时触发。

### 3.4 引入轻量级重排序（Rerank）

1. **模型**：SiliconFlow `Qwen/Qwen3-Reranker-8B`（同一平台便于密钥复用）。
2. **调用策略**：
   - 对融合后的前 30 条候选执行 rerank，获取交叉编码分数。
   - 新增字段 `reranker.score`，最终得分 `0.7 * fused_score + 0.3 * reranker_score`。
3. **配置扩展**：在 `config/book_vectorization.yaml` 中添加：
   ```yaml
   reranker:
     provider: "siliconflow"
     model: "Qwen/Qwen3-Reranker-8B"
     api_key: "env:SiliconFlow_API_KEY"
     base_url: "https://api.siliconflow.cn/v1"
     top_n: 30
     timeout: 15
   ```
   并在 `BookRetriever` 结果合并阶段可选启用。
4. **开销**：单次 rerank 请求的 token 成本低于重新查询，可在需要高精准度的主题上按需开启。

### 3.5 文档表示与元数据优化

1. **目录摘要化**：使用简单启发式（仅保留章节标题）或 LLM 生成 300 字摘要后再拼接，避免冗长噪声。
2. **Metadata 扩展**：
   - 增加 `topics`, `keywords`, `mentioned_books` 等字段，便于 `vector_store.search` 使用 `where` 过滤。
   - 可将提及书籍标题作为 metadata 数组，帮助快速定位关联书。
3. **缓存与回溯**：将 `query → result set` 缓存在本地 JSON，便于调试与离线评估。

### 3.6 成本 / 性能折中

| 策略 | 召回提升 | 开销 | 建议 |
| --- | --- | --- | --- |
| 多子查询密集检索 | 高 | 增加 3~4 倍向量调用 | 仅对高价值报告启用，或限制 `tags` 数量 |
| LLM 生成检索提示 | 中 | 受 token 数影响 | 批量运行，输出缓存 |
| Reranker | 高 | 每次 30×交叉编码 | 只对最终候选 rerank |
| Metadata 扩展 | 中 | 一次性向量重建 | 结合向量重跑计划执行 |

## 4. 推荐落地步骤

1. **阶段 1（本周）**：完成 Markdown 解析 + 多子查询检索 + 结果融合脚本，离线评估召回。
2. **阶段 2**：上线 reranker 配置并集成 CLI 选项，同时在现有评分阈值基础上调试多子查询融合效果。
3. **阶段 3**：对热门主题重建向量，注入 LLM 生成的主题摘要，逐步扩展到全库。
4. **阶段 4**：沉淀评估集（报告句子 → 理想书目），建立自动回归测试。

## 5. 参考实现建议

- **接口设计**：为 `scripts/retrieve_books.py` 新增 `--query-mode multi`、`--enable-rerank`、`--assets-file`/`--from-md` 等参数，既支持传统手动输入，也支持指定某个分析 Markdown 后自动完成“解析→多轮检索→融合”的一键流程。
- **模块划分**：
  - `src/core/book_vectorization/query_assets.py`：负责解析报告、LLM 扩写、生成查询包。
  - `src/core/book_vectorization/reranker.py`：封装 SiliconFlow rerank API，接口与 `EmbeddingClient` 对齐。
- **监控**：记录每次查询的子查询命中数、融合前后得分差，帮助判断哪种策略贡献最大。

通过以上流程，可以在不大幅改动底层架构的前提下，将文章“标签、关键词、提及书籍”与 LLM 推理、SiliconFlow Reranker 有机结合，实现兼顾性能与成本的高相关性检索。