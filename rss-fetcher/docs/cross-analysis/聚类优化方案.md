# 交叉分析聚类优化方案

## 问题描述

12篇文章被聚类为2组（g1: 11篇，g2: 1篇），聚类粒度太粗，母题（如"探索与认知的边界"）过于空泛。

### 根本原因

1. **`batch_size=6` 导致聚类数过少**：12 ÷ 6 = 2 个簇
2. **TF-IDF 特征全部来自 LLM 输出**，而 LLM 被提示词引导"抽象化"
3. **`thematic_essence` 太"升维"**：所有文章都用类似的哲学词汇描述，导致语义高度重叠
4. **未使用原始文章内容**：`summary`（RSS原始摘要）、`full_text` 等具体字段未参与聚类

---

## 优化方案

### 方案一：调整聚类特征（推荐，改动最小）

**文件**：`src/core/cross_analysis/clustering.py`

**修改 `_build_feature_text` 方法**：

```python
def _build_feature_text(self, article: Dict, index: int) -> str:
    """拼接特征字段，作为向量化的输入文本。"""
    title = str(article.get("title") or f"文章{index+1}")

    # 新增：优先用 full_text 前1000字，fallback 到 summary_long
    # full_text 包含完整文章内容，特征更丰富；截取前1000字避免性能问题和尾部噪音
    raw_content = str(article.get("full_text") or "")[:1000]
    if not raw_content.strip():
        raw_content = str(article.get("summary_long") or "")

    # LLM 标签（具体，权重高）
    tags = article.get("llm_tags") or ""
    if isinstance(tags, (list, tuple, set)):
        tags_text = " ".join(str(tag) for tag in tags)
    else:
        tags_text = str(tags)

    # LLM 长摘要（适中权重）
    summary_long = str(article.get("summary_long") or "")

    # 提及的书籍
    mentioned = article.get("llm_mentioned_books") or ""
    mentioned_text = " ".join(mentioned) if isinstance(mentioned, (list, tuple, set)) else str(mentioned)

    # 注意：移除 thematic_essence，因为太抽象会干扰聚类
    # 标签和标题重复以提高权重
    return " ".join(filter(None, [
        title, title,           # 标题权重 x2
        raw_content,            # 原始内容（full_text前1000字或summary）
        tags_text, tags_text,   # 标签权重 x2
        summary_long,
        mentioned_text
    ]))
```

**关键改动**：
- ✅ 优先使用 `full_text[:2000]`（全文前2000字，特征丰富）
- ✅ Fallback 到 `summary_long`（LLM 生成的结构化摘要，质量更高）
- ✅ 移除 `llm_thematic_essence`（太抽象，干扰聚类）
- ✅ `title` 和 `tags` 重复以提高权重

**为什么用 `full_text[:1000]` 而不是 `summary`**：
- RSS 原始 `summary` 可能为空或仅100字，特征稀疏
- `full_text` 前1000字通常包含导语和核心观点
- 截取避免全文太长拖慢向量化
- 避免文末广告、推荐等噪音

**为什么 fallback 用 `summary_long` 而不是 `summary`**：
- `summary_long` 来自 LLM 解析的 `llm_summary` 字段，是结构化摘要
- 质量比 RSS 原始 `summary` 更高，内容更完整

**影响范围**：仅 `clustering.py`，不涉及提示词和 JSON 字段

---

### 方案二：改用层次聚类自动分组（推荐）

**问题**：当前 KMeans 必须预设簇数 k，通过 `batch_size` 人为计算，不够智能。

**解决**：改用 **Agglomerative（层次聚类）**，通过距离阈值自动确定分组数量。

**文件**：`src/core/cross_analysis/clustering.py`

**算法对比**：

| 算法 | 是否需要预设簇数 | 适合场景 |
|------|------------------|----------|
| KMeans | ✅ 需要 | 已知要分几组 |
| HDBSCAN | ❌ 不需要 | 大数据集，可能有噪音点 |
| **Agglomerative** | ❌ 不需要 | 小数据集（10-50篇），需要全部分组 |

**为什么选 Agglomerative**：
- 不需要预设簇数，通过 `distance_threshold` 控制"多相似才算一组"
- 阈值越小 → 分组越细 → 主题越聚焦
- 无噪音点问题（HDBSCAN 在小数据集可能把大部分标为噪音）
- scikit-learn 原生支持，无需额外安装依赖

**代码修改**：

```python
from sklearn.cluster import AgglomerativeClustering

def cluster(self, articles: Sequence[Dict]) -> List[List[Dict]]:
    # ... TF-IDF 向量化代码不变 ...

    # 改用层次聚类，自动确定簇数
    model = AgglomerativeClustering(
        n_clusters=None,                    # 不预设簇数
        distance_threshold=self.distance_threshold,  # 控制分组粒度
        metric='cosine',                    # 余弦相似度，适合文本
        linkage='average'                   # 平均链接
    )

    # 需要将稀疏矩阵转为密集矩阵
    labels = model.fit_predict(tfidf_matrix.toarray())
    # ... 后续分组逻辑不变 ...
```

**配置变更**：

```yaml
# config/subject_bibliography.yaml
cross_analysis:
  min_score: 92
  # batch_size: 6  # 移除，不再需要
  distance_threshold: 0.8  # 新增：越小分组越细（建议 0.5-1.2）
```

**`distance_threshold` 调参建议**：
- `0.5`：分组很细，主题高度聚焦（可能每组只有1-2篇）
- `0.8`：适中，主题相近的文章归为一组（推荐起点）
- `1.2`：分组较粗，允许主题有一定差异

**影响范围**：
- `clustering.py`：算法替换
- `config/subject_bibliography.yaml`：配置项变更
- `manager.py`：读取新配置项

---

### 方案三：修改提示词增加 `topic_focus` 字段（需谨慎）

> ⚠️ **注意**：此方案会修改 JSON 输出字段，需要同步修改解析逻辑和 Excel 列定义

**文件**：`prompts/article_analysis.md`（或对应的深度分析提示词）

**新增输出字段**：

```markdown
5. **提炼主题聚焦点 (Topic Focus)**：50字以内
   - *要求*：保留文章的**具体讨论对象**，去掉人名地名等细节
   - *抽象度*：介于"具体事件"和"哲学本质"之间
   - *示例*：
     - ✗ "张三在公园散步治好焦虑" → 太具体
     - ✗ "存在主义虚无与注意力异化" → 太抽象
     - ✓ "城市散步与心理疗愈"
     - ✓ "鲸鱼声学通讯研究"
     - ✓ "游戏叙事中的乌托邦建构"
```

**JSON 输出格式变更**：

```json
{
  "score": 92,
  "primary_dimension": "激发思维",
  "reason": "...",
  "topic_focus": "鲸鱼声学通讯与跨物种交流",  // 新增字段
  "thematic_essence": "...",
  "tags": ["..."],
  "mentioned_books": [...]
}
```

**需要同步修改的文件**：

| 文件 | 修改内容 |
|------|----------|
| `src/core/storage.py` | `STANDARD_COLUMNS["analyze"]` 增加 `llm_topic_focus` |
| `src/core/analysis/analyst.py` | 解析 JSON 时提取 `topic_focus` |
| `src/core/cross_analysis/manager.py` | `_prepare_article` 传递 `topic_focus` |
| `src/core/cross_analysis/clustering.py` | `_build_feature_text` 使用 `topic_focus` |

---

### 方案四：修改交叉分析提示词限制母题粒度

**文件**：`prompts/article_cross_analysis.md`

**在 STEP 2 增加约束**：

```markdown
**STEP 2: 维度对齐与定义**
- 定义母题名称。
- 确定其主导维度（Pleasure/Perspective/Thinking/Knowledge）。
- ⚠️ **粒度要求**：母题必须足够具体，能明确区分于其他文章组
  - ✗ "探索与认知的边界" → 太空泛，几乎所有知识类文章都能归入
  - ✗ "技术与人类" → 太宽泛，缺乏辨识度
  - ✓ "粒子物理与宇宙起源之谜"
  - ✓ "海洋哺乳动物的声学通讯"
  - ✓ "数字平台的情绪操控机制"
  - ✓ "动画电影中的政治乌托邦"
```

**影响范围**：仅提示词文本，不涉及 JSON 字段

---

## 推荐实施顺序

| 优先级 | 方案 | 改动量 | 风险 | 效果 |
|--------|------|--------|------|------|
| 1 | 方案一：调整聚类特征 | 小 | 低 | 提高特征区分度 |
| 2 | 方案二：改用层次聚类 | 中 | 低 | 自动确定分组数，无需手动调 batch_size |
| 3 | 方案四：修改交叉分析提示词 | 小 | 低 | 限制母题粒度 |
| 4 | 方案三：增加 topic_focus 字段 | 大 | 中 | 需同步多处，暂不推荐 |

**建议**：先实施方案一 + 方案二 + 方案四，观察效果后再决定是否需要方案三。

---

## 附：当前聚类特征字段对比

| 字段 | 当前是否使用 | 内容特点 | 建议 |
|------|--------------|----------|------|
| `title` | ✅ 使用 | 具体、区分度高 | 保留，提高权重 |
| `full_text` | ❌ 未使用 | 完整文章内容，特征丰富 | **新增**（截取前1000字） |
| `summary_long` | ✅ 使用 | LLM生成摘要，质量高 | 保留，作为 `full_text` 的 fallback |
| `summary` | ❌ 未使用 | RSS原始摘要，可能为空 | 不使用 |
| `llm_thematic_essence` | ✅ 使用 | 抽象哲学描述，词汇重叠 | **移除** |
| `llm_tags` | ✅ 使用 | 具体标签 | 保留，提高权重 |
| `llm_mentioned_books` | ✅ 使用 | 提及书籍 | 保留 |

---

*文档生成时间：2025-12-10*
