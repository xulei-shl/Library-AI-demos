# 批次大小配置化改造 - 完成总结

## ✅ 改造完成

**完成时间**: 2025-11-19  
**改造目标**: 将硬编码的批次大小改为可配置,支持动态调整

---

## 📋 修改清单

### 1. 配置文件修改

**文件**: [`config/llm.yaml`](file:///f:/Github/Library-AI-demos/book-echoes/config/llm.yaml)

```yaml
tasks:
  theme_initial:
    parameters:
      max_batch_size: 20  # ✅ 新增:初评批次大小
      recommend_quota:
        # ...
  
  theme_final:
    parameters:
      mode_threshold: 30
      top_n: 20
      max_batch_size: 30  # ✅ 新增:终评批次大小(预留)
```

### 2. 配置读取函数

**文件**: [`src/core/recommendation/config.py`](file:///f:/Github/Library-AI-demos/book-echoes/src/core/recommendation/config.py)

新增函数:
- ✅ `get_initial_batch_size()` - 读取初评批次大小
- ✅ `get_final_batch_size()` - 读取终评批次大小

### 3. 业务逻辑修改

**文件**: [`src/core/recommendation/controller.py`](file:///f:/Github/Library-AI-demos/book-echoes/src/core/recommendation/controller.py)

修改函数:
- ✅ `run_theme_recommendation_initial()` - 使用配置的批次大小
- ✅ `_retry_failed_reviews()` - 使用配置的批次大小

### 4. 测试验证

**文件**: [`tests/test_batch_size_config.py`](file:///f:/Github/Library-AI-demos/book-echoes/tests/test_batch_size_config.py)

测试结果:
```
✓ 初评批次大小: 20
✓ 终评批次大小: 30
✓ 终评模式阈值: 30
✓ 终评推荐数量: 20
✅ 所有配置加载测试通过!
```

---

## 🎯 问题解答

### Q1: `mode_threshold: 30` 的作用是什么?

**答**: `mode_threshold` **不是**控制批次大小,而是控制终评模式选择:
- 候选数 > 30: 使用**锦标赛模式**(先分组半决赛,再决赛)
- 候选数 ≤ 30: 使用**直接终评模式**(一次性评选)

### Q2: 每次调用大模型处理多少条数据?

**答**: 现在可以通过配置文件控制:

| 阶段 | 配置项 | 默认值 | 说明 |
|------|--------|--------|------|
| 初评 | `tasks.theme_initial.parameters.max_batch_size` | 20 | ✅ 已实现分批 |
| 主题内决选 | - | 一次性 | 数量较少,无需分批 |
| 半决赛 | - | 一次性 | 锦标赛已分组 |
| 终评 | `tasks.theme_final.parameters.max_batch_size` | 30 | ⚠️ 预留,暂未实现 |

### Q3: 如何调整批次大小?

**答**: 直接修改 `config/llm.yaml`:

```yaml
tasks:
  theme_initial:
    parameters:
      max_batch_size: 25  # 从20改为25
```

**效果**:
- 原来: 40条书目 → 2批(20+20)
- 修改后: 40条书目 → 2批(20+20) 或 更少批次

---

## 📊 各阶段调用逻辑总结

### 1️⃣ 初评阶段 (theme_initial)

```
候选书目 → 按主题分组 → 每组按 max_batch_size 分批 → 调用LLM
```

**示例** (max_batch_size=20):
- 主题T: 45本 → 3批(15+15+15)
- 主题H: 18本 → 1批(18)

### 2️⃣ 主题内决选 (theme_runoff)

```
初评通过书目 → 按主题分组 → 每组一次性调用LLM
```

**逻辑**:
- 数量 ≤ 配额(8): 自动晋级,不调用LLM
- 数量 > 配额(8): 一次性调用LLM选出配额数量

### 3️⃣ 终评阶段 (theme_final)

**模式选择**:
```
候选数 > mode_threshold(30) → 锦标赛模式
候选数 ≤ mode_threshold(30) → 直接终评模式
```

**锦标赛模式**:
```
候选书目 → 随机分A/B组 → 半决赛(各选50%) → 决赛(选top_n)
```

**直接终评模式**:
```
候选书目 → 一次性调用LLM → 选出top_n本
```

---

## 🔧 使用建议

### 批次大小调优

| 场景 | 推荐值 | 理由 |
|------|--------|------|
| 书目数量少(<100本) | 15-20 | 快速处理,减少等待 |
| 书目数量中等(100-500本) | 20-25 | 平衡性能和质量 |
| 书目数量多(>500本) | 25-30 | 减少调用次数,降低成本 |
| 大模型性能强 | 30-40 | 充分利用模型能力 |
| 大模型性能弱 | 15-20 | 避免超时和质量下降 |

### 注意事项

⚠️ **批次太小**:
- 调用次数多,总耗时长
- 成本增加(每次调用都有固定开销)
- 评选上下文不足

⚠️ **批次太大**:
- 单次调用慢,超时风险
- Token消耗大,可能超限
- 大模型难以处理过多选项

✅ **推荐范围**: 15-30条

---

## 📝 后续优化建议

### 1. 终评分批逻辑

如果终评候选数经常超过配置的批次大小,可以实现分批逻辑:

```python
def run_direct_final_with_batches(excel_path, finalists, top_n):
    batch_size = get_final_batch_size()
    
    if len(finalists) <= batch_size:
        # 直接评选
        return executor.final(finalists, top_n)
    
    # 分批评选
    batches = split_batches(finalists, batch_size)
    batch_results = []
    
    for batch in batches:
        # 每批选出 top_n * (len(batch) / len(finalists))
        quota = max(1, int(top_n * len(batch) / len(finalists)))
        result = executor.final(batch, quota)
        batch_results.extend(result['selected_books'])
    
    # 最终评选
    return executor.final(batch_results, top_n)
```

### 2. 自适应批次大小

根据实际情况动态调整:

```python
def get_adaptive_batch_size(total: int, base: int = 20) -> int:
    """根据总数自适应调整批次大小"""
    if total <= base:
        return total
    elif total <= base * 3:
        # 中等规模,均分
        return total // 2
    else:
        # 大规模,使用配置值
        return base
```

---

## ✅ 验证清单

- [x] 配置文件添加 `max_batch_size` 参数
- [x] 实现配置读取函数
- [x] 修改初评阶段使用配置值
- [x] 修改兜底重试使用配置值
- [x] 创建测试脚本
- [x] 运行测试验证
- [x] 编写文档说明

---

## 📚 相关文档

- [批次大小配置化改造详细说明](file:///f:/Github/Library-AI-demos/book-echoes/docs/changelogs/模块4-大模型评选/批次大小配置化改造.md)
- [配置文件](file:///f:/Github/Library-AI-demos/book-echoes/config/llm.yaml)
- [测试脚本](file:///f:/Github/Library-AI-demos/book-echoes/tests/test_batch_size_config.py)

---

## 🎉 总结

本次改造成功将硬编码的批次大小改为可配置,提升了系统的灵活性:

1. ✅ **配置化**: 通过修改 YAML 文件即可调整批次大小
2. ✅ **向后兼容**: 配置缺失时使用默认值,不影响现有功能
3. ✅ **易于维护**: 集中管理配置,便于后续调优
4. ✅ **测试验证**: 通过测试确保配置正确加载

**现在你可以通过修改 `config/llm.yaml` 中的 `max_batch_size` 参数来控制每次调用大模型处理的书目数量!** 🎊
