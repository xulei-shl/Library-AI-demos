# 批次大小配置化改造

**日期**: 2025-11-19  
**修改人**: AI Assistant  
**关联Issue**: 终评阶段批次大小配置问题

## 问题背景

用户在查看大模型调用日志时发现:
1. 终评阶段每次选择20条候选书目调用大模型
2. 这个"20"是硬编码在代码中的 `MAX_BATCH_SIZE = 20`
3. 用户希望能够通过配置文件动态调整批次大小

## 问题分析

### 原有逻辑

1. **初评阶段**: 使用 `split_batches(books)` 按 `MAX_BATCH_SIZE = 20` 分批调用大模型 ✅
2. **主题内决选**: 每个主题一次性调用,不分批 ✅ (数量较少,无需分批)
3. **半决赛**: 锦标赛模式已分A/B组,每组数量较少 ✅ (无需分批)
4. **终评**: 一次性调用,不分批 ⚠️ (候选数较多时可能需要分批)

### `mode_threshold` 参数的真实作用

用户原本以为 `mode_threshold: 30` 是控制"每30条数据一组调用大模型",但实际上:
- **真实作用**: 控制终评模式选择
  - 候选数 > 30: 使用锦标赛模式(先分组半决赛,再决赛)
  - 候选数 ≤ 30: 使用直接终评模式(一次性评选)

## 解决方案

### 1. 配置文件修改

在 [`config/llm.yaml`](file:///f:/Github/Library-AI-demos/book-echoes/config/llm.yaml) 中添加 `max_batch_size` 配置:

```yaml
tasks:
  theme_initial:
    parameters:
      max_batch_size: 20  # 初评阶段每批处理的最大书目数
      recommend_quota:
        gt20: 6
        # ...
  
  theme_final:
    parameters:
      mode_threshold: 30  # 终评模式切换阈值
      top_n: 20          # 最终推荐数量
      max_batch_size: 30  # 终评阶段每批处理的最大书目数(预留,暂未使用)
```

### 2. 代码修改

#### 2.1 新增配置读取函数

在 [`config.py`](file:///f:/Github/Library-AI-demos/book-echoes/src/core/recommendation/config.py) 中添加:

```python
def get_initial_batch_size() -> int:
    """从配置文件加载初评阶段的批次大小"""
    try:
        loader = ConfigLoader(LLM_CONFIG_PATH)
        settings = loader.load()
        batch_size = settings.get("tasks", {}).get("theme_initial", {}).get("parameters", {}).get("max_batch_size", MAX_BATCH_SIZE)
        return int(batch_size)
    except Exception as e:
        logger.warning("加载初评批次大小失败，使用默认值 %d: %s", MAX_BATCH_SIZE, e)
        return MAX_BATCH_SIZE

def get_final_batch_size() -> int:
    """从配置文件加载终评阶段的批次大小"""
    # 类似实现
```

#### 2.2 修改调用处

在 [`controller.py`](file:///f:/Github/Library-AI-demos/book-echoes/src/core/recommendation/controller.py) 中:

**修改前**:
```python
batches = split_batches(books)  # 使用默认的 MAX_BATCH_SIZE = 20
```

**修改后**:
```python
batch_size = get_initial_batch_size()  # 从配置读取
batches = split_batches(books, batch_size)
```

### 3. 影响范围

修改影响以下函数:
- ✅ `run_theme_recommendation_initial`: 初评主流程
- ✅ `_retry_failed_reviews`: 初评兜底重试

**未修改的函数**:
- ❌ `run_theme_runoff`: 主题内决选(无需分批)
- ❌ `run_tournament_final`: 锦标赛终评(已通过分组控制规模)
- ❌ `run_direct_final`: 直接终评(候选数≤30,无需分批)

## 使用方法

### 调整初评批次大小

修改 `config/llm.yaml`:
```yaml
tasks:
  theme_initial:
    parameters:
      max_batch_size: 25  # 从20改为25
```

**效果**:
- 每个主题的书目会按25条一批进行分批
- 减少调用次数,但单次调用的token消耗增加
- 适合书目数量较多的场景

### 调整终评批次大小

修改 `config/llm.yaml`:
```yaml
tasks:
  theme_final:
    parameters:
      max_batch_size: 40  # 从30改为40
```

**注意**: 终评阶段目前**暂未实现分批逻辑**,此配置预留待后续使用。

## 配置建议

### 批次大小选择原则

1. **太小** (如10):
   - ✅ 单次调用快,失败影响小
   - ❌ 调用次数多,总成本高
   - ❌ 评选上下文不足,质量可能下降

2. **适中** (如20-30):
   - ✅ 平衡性能和质量
   - ✅ 大模型有足够上下文进行比较
   - ✅ 调用次数适中

3. **太大** (如50+):
   - ✅ 调用次数少
   - ❌ 单次调用慢,超时风险高
   - ❌ Token消耗大,可能超出限制
   - ❌ 大模型难以处理过多选项

### 推荐配置

```yaml
tasks:
  theme_initial:
    parameters:
      max_batch_size: 20  # 初评:20条一批(默认值)
  
  theme_final:
    parameters:
      mode_threshold: 30  # 候选数>30时使用锦标赛模式
      top_n: 20          # 最终推荐20本
      max_batch_size: 30  # 预留配置
```

## 验证方法

### 1. 检查日志

运行初评后,查看日志中的批次信息:
```
主题 [T]: 45 条 -> 3 批 (批次大小: 15, 15, 15)
```

如果配置 `max_batch_size: 20`,应该看到:
```
主题 [T]: 45 条 -> 3 批 (批次大小: 15, 15, 15)
```

如果配置 `max_batch_size: 25`,应该看到:
```
主题 [T]: 45 条 -> 2 批 (批次大小: 23, 22)
```

### 2. 测试配置加载

```python
from src.core.recommendation.config import get_initial_batch_size, get_final_batch_size

print(f"初评批次大小: {get_initial_batch_size()}")
print(f"终评批次大小: {get_final_batch_size()}")
```

## 后续优化建议

### 1. 终评分批逻辑

如果终评候选数经常超过30本,可以考虑在 `run_direct_final` 中添加分批逻辑:

```python
def run_direct_final(excel_path: str, finalists: List[Dict], top_n: int = 10):
    batch_size = get_final_batch_size()
    
    if len(finalists) > batch_size:
        # 分批处理逻辑
        # 1. 分批评选,每批选出 top_n * (batch_size / total)
        # 2. 汇总所有批次的结果
        # 3. 最终评选出 top_n 本
        pass
    else:
        # 原有逻辑
        result = executor.final(finalists, top_n)
```

### 2. 动态批次大小

根据书目数量动态调整批次大小:
```python
def get_adaptive_batch_size(total_books: int, base_size: int = 20) -> int:
    """根据总数动态调整批次大小"""
    if total_books <= base_size:
        return total_books
    elif total_books <= base_size * 2:
        return total_books // 2
    else:
        return base_size
```

## 总结

本次改造实现了:
1. ✅ 将硬编码的 `MAX_BATCH_SIZE = 20` 改为可配置
2. ✅ 支持初评阶段动态调整批次大小
3. ✅ 为终评阶段预留批次大小配置
4. ✅ 保持向后兼容(配置缺失时使用默认值)

**配置优先级**:
1. `config/llm.yaml` 中的 `tasks.*.parameters.max_batch_size`
2. 代码默认值 `MAX_BATCH_SIZE = 20`
