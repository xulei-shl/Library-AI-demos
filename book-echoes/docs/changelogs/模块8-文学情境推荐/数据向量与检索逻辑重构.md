### 1. 数据入库策略 (Indexing Strategy)

在检索之前，必须确保存储的数据结构对“情境”是敏感的。

#### 核心字段加权与向量化
建议为每本书构建一个**“语义索引文本 (Semantic Index Text)”**，用于生成向量。不要直接把所有字段拼在一起，要有侧重：

*   **高权重字段 (Weight: 1.5)**:
    *   `llm_reasoning`: **这是最核心的字段！** 因为你在打标阶段已经让 LLM 解释了“为什么这本书适合这个场景”。这段文字的语义与你的“主题描述”是最接近的。
    *   `tags` (所有维度的标签文本): 显式特征。
*   **中权重字段 (Weight: 1.0)**:
    *   `douban_summary` (简介): 原始语义。
    *   `douban_title` + `douban_subtitle` + `douban_author`.
*   **低权重字段 (Weight: 0.5)**:
    *   `douban_author_intro` / `douban_catalog`

**入库动作：**
将上述字段按权重组合成一段文本，调用 Embedding API 生成向量，存入向量数据库。同时，将 JSON 格式的 Tags 单独存储为**标量字段 (Scalar Fields)**，用于 Filter 过滤。

---

### 2. 核心检索流程设计 (The Retrieval Flow)

我们将采用 **“LLM 查询意图转换 + 混合检索 (Hybrid Search)”** 的模式。

#### Step 1: Query Translation (查询意图转换)
**这是最重要的一步。**
不要直接用 `description` 去搜。使用 LLM 作为一个 **Query Agent**，分析 Step 1 生成的主题 JSON，将其拆解为三个检索要素：

1.  **Tag Filters (结构化过滤)**：必须包含或优先包含哪些标签？
2.  **Keywords (关键词)**：用于 BM25 全文检索。
3.  **Synthetic Query (合成向量查询词)**：重写一段“像图书推理/简介”的文本，用于向量匹配。

#### Step 2: Parallel Retrieval (并行双路检索)
*   **路 A：Vector Search (语义召回)**
    *   使用 `Synthetic Query` 进行向量搜索。
    *   **关键点**：如果数据库支持，将 Step 1 提取的 Tags 作为 **Pre-filter (前置过滤)** 或 **Post-filter (后置过滤)**。
        *   *建议策略*：使用 **Should (Or)** 逻辑过滤。例如：`reading_context` 包含 "深夜独处" OR "正念深读"。
*   **路 B：Keyword Search (字面召回)**
    *   使用提取的 `Keywords` 对书名、作者、简介进行 BM25 检索。

#### Step 3: Reranking & Fusion (重排序与融合)
*   使用 **RRF (Reciprocal Rank Fusion)** 算法合并两路结果。
*   **可选高级操作**：如果对精度要求极高，可以引入 **Cross-Encoder** 模型对 Top 50 结果进行重打分，或者再次调用 LLM 挑选 Top 5。

---

### 3. "Query Agent" 的提示词设计

这个 Prompt 用于将“主题”转化为“检索条件”。

```markdown
# Role
你是一个精通数据库检索逻辑的搜索工程师。你的目标是将一个“感性的文学策展主题”翻译成“结构化的数据库查询条件”。

# Input Data
你将接收到一个主题对象，包含：
1. Theme Name (主题名)
2. Slogan (副标题)
3. Description (情境描述)
4. Target Vibe (预期氛围)

# Database Schema (标签词表)
我们的数据库中书籍包含以下 5 个维度的标签字段（每个字段包含多个值）：
- reading_context (e.g., 深夜独处, 情绪急救, 碎片隙间...)
- reading_load (e.g., 酣畅易读, 需正襟危坐...)
- text_texture (e.g., 细腻工笔, 冷峻克制...)
- spatial_atmosphere (e.g., 私密避难所, 都市孤独...)
- emotional_tone (e.g., 温暖治愈, 自由反叛...)

# Task
请分析输入的主题，输出一个 JSON 对象，包含以下 3 部分：

1.  **filter_conditions** (过滤条件):
    *   分析该主题最核心的“硬性约束”。
    *   例如：如果是“睡前读物”，`reading_load` 必须避开 "需正襟危坐"。
    *   例如：如果是“治愈系”，`emotional_tone` 优先匹配 "温暖治愈"。
    *   格式：{"field_name": ["tag1", "tag2"], "operator": "OR/NOT"}

2.  **search_keywords** (关键词):
    *   提取 3-5 个具体的名词或形容词，用于 BM25 检索（如：冬天, 圣诞, 独处, 温暖）。

3.  **synthetic_query** (合成向量查询句):
    *   **这是最重要的。** 请根据主题描述，模仿我们数据库中 `llm_reasoning` (推理字段) 的口吻，写一段话。
    *   这段话应该描述：“这是一本什么样的书，适合什么人，在什么场景读，读完有什么感觉。”
    *   *目的*：让这段话的向量与目标书籍的向量在语义空间中尽可能接近。

# Example Input
Theme: "周日晚上七点，为精神续航"
Desc: "对抗周一焦虑...不适合读沉重的大部头...轻盈、治愈..."

# Example Output
{
  "filter_conditions": [
    {
      "field": "reading_load",
      "values": ["酣畅易读", "随时可停", "细水长流"],
      "operator": "SHOULD"  // 优先匹配这些
    },
    {
      "field": "reading_load",
      "values": ["需正襟危坐"],
      "operator": "MUST_NOT" // 绝对不要这些
    },
    {
      "field": "reading_context",
      "values": ["情绪急救", "枕边安神"],
      "operator": "SHOULD"
    }
  ],
  "search_keywords": ["治愈", "轻松", "焦虑", "温暖", "睡前"],
  "synthetic_query": "这本书非常适合在感到焦虑或疲惫的周日晚上阅读。它不需要读者动用太多脑力（酣畅易读），情节轻松或文字温暖，具有很强的治愈功能（情绪急救），能像温开水一样抚平内心的不安。"
}
```

---

### 4. 为什么这样设计？(Design Rationale)

1.  **解决“语义断层”**：
    *   主题描述：“*当全世界都在狂欢，我只想与一本书相安无事*”
    *   如果直接向量化，可能匹配到“狂欢”、“派对”相关的书。
    *   通过 **Query Agent** 生成的 `synthetic_query` 会变成：“*这本书适合孤独的人在节日阅读，氛围冷峻克制，强调独处的自洽...*” —— 这与目标书籍的 `llm_reasoning` 向量高度重合。

2.  **利用“结构化标签”**：
    *   `reading_load` (阅读体感) 是很难通过纯向量检索控制的。向量可能觉得《纯粹理性批判》和《苏菲的世界》语义很近（都是哲学），但用户的脑力负担天差地别。
    *   通过 `filter_conditions` 进行硬性或软性过滤，可以保证推出来的书**“读得动”**。

3.  **双路互补**：
    *   向量负责“味儿对不对”。
    *   关键词负责“内容准不准”（比如主题里明确提到了“圣诞”，关键词能保证召回带有圣诞元素的书）。

### 5. 总结：执行路径

1.  **离线层**：
    *   图书入库 -> LLM 打标 -> 生成 `llm_reasoning` -> 组合文本 -> 向量化存储。
2.  **在线层**：
    *   用户/系统生成主题 (Step 1)。
    *   **Query Agent** 将主题拆解为 `Tags` + `Keywords` + `Synthetic Query`。
    *   **Vector DB** 执行：`Vector(Synthetic Query)` + `Filter(Tags)`。
    *   **ES/Lucene** 执行：`Match(Keywords)`。
    *   **Merge**：合并去重，返回 Top N 书籍。