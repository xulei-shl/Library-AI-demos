# 书海回响图书推荐系统 - 实验流程分析报告

## 概述

本报告详细记录了"书海回响"图书推荐系统从数据采集到AI决策的完整实验流程，重点分析数据驱动决策的技术实现路径。

## 一、数据采集与清洗流程分析

### 1.1 数据源架构

系统采用多源数据融合架构，主要包括：

**内部数据源**:
- **月归还数据** (`data/月归还.xlsx`): 图书馆月度归还记录
- **借阅数据** (`data/近三月借阅.xlsx`): 近三个月详细借阅记录
- **新书验收数据** (`data/new/验收.xlsx`): 新采购图书清单
- **新书借阅数据** (`data/new/近四月借阅.xlsx`): 新书借阅情况

**外部数据源**:
- **豆瓣图书数据**: 评分、作者、简介、封面等元数据
- **FOLIO系统数据**: 图书馆集成系统的ISBN信息

### 1.2 数据清洗核心算法

**位置**: `src/core/data_cleaner.py`

#### 1.2.1 统一数据清洗器架构

```python
class UnifiedDataCleaner:
    """统一数据清洗器类"""
    
    def __init__(self, config: DataCleanerConfig = None):
        self.config = config or DataCleanerConfig()
        self.chinese_book_prefix = "中文图书"
```

**关键清洗流程**:
1. **中文图书筛选**: 基于"类型/册数"列筛选中文图书
2. **索书号标准化**: 去除特殊符号，统一格式
3. **时间格式标准化**: 统一时间列格式为datetime
4. **无效记录移除**: 基于必需字段验证数据完整性

#### 1.2.2 索书号清洗算法

```python
def clean_call_number(self, call_number: str) -> str:
    """标准化索书号处理规则"""
    # 1. 去除末尾的#和*及后面的数字，如 #10*2, #8 等
    cleaned = re.sub(r'[#*]\d+(?:[*]\d+)?$', '', cleaned)
    
    # 2. 如果有两个以上/，保留到第二个/为止
    parts = cleaned.split('/')
    if len(parts) > 2:
        cleaned = '/'.join(parts[:2])
    
    return cleaned.strip()
```

**清洗效果统计**:
- 原始唯一索书号: `original_unique`
- 清洗后唯一索书号: `cleaned_unique`
- 合并效果: `original_unique - cleaned_unique` 个索书号被合并

### 1.3 数据质量控制机制

#### 1.3.1 必需字段验证

```python
def remove_invalid_records(self, data: pd.DataFrame) -> pd.DataFrame:
    """移除无效记录"""
    valid_mask = pd.Series(True, index=data.index)
    
    for col in self.config.required_columns:
        if col == '清理后索书号':
            valid_mask &= (data[col] != "") & (data[col].notna())
        elif col == '提交时间':
            valid_mask &= data[col].notna()
        else:
            valid_mask &= data[col].notna()
```

#### 1.3.2 数据清洗统计报告

```python
def get_cleaning_statistics(self, original_data, cleaned_data):
    """获取清洗统计信息"""
    stats = {
        "原始数据量": len(original_data),
        "清洗后数据量": len(cleaned_data),
        "数据保留率(%)": round(len(cleaned_data) / len(original_data) * 100, 2),
        "数据损失量": len(original_data) - len(cleaned_data)
    }
```

## 二、用户/书目画像构建逻辑

### 2.1 借阅统计核心算法

**位置**: `src/core/statistics.py`

#### 2.1.1 修正版借阅统计器

```python
class BorrowingStatisticsCorrected:
    """修正版借阅统计器类"""
    
    def __init__(self):
        self.new_columns = [
            '近三个月总次数',
            '第一个月借阅次数', 
            '第二个月借阅次数',
            '第三个月借阅次数',
            '借阅人数'
        ]
```

**核心统计逻辑**:
1. **时间范围计算**: 基于配置自动确定近三个月时间窗口
2. **月度分组统计**: 按索书号分组统计各月借阅次数
3. **借阅人数计算**: 统计不同用户对同一索书号的借阅次数

#### 2.1.2 高性能统计实现

**优化策略**:
- **预过滤**: 预先过滤非空索书号记录
- **向量化操作**: 使用pandas的value_counts()进行快速统计
- **GroupBy优化**: 使用groupby一次性计算所有索书号的借阅人数

```python
# 使用groupby一次性计算所有索书号的借阅人数
borrowers_count = borrowing_data.groupby(cleaned_call_column)[user_id_column].nunique().to_dict()
```

#### 2.1.3 统计数据映射机制

```python
def _assign_statistics_to_monthly_records(self, monthly_data, statistics_dict, monthly_call_column):
    """为月归还数据的每个记录分配对应的统计数据"""
    for idx, row in monthly_data.iterrows():
        call_number = row[cleaned_call_column]
        
        if call_number in statistics_dict:
            stats = statistics_dict[call_number]
            result_data.loc[idx, '近三个月总次数'] = stats['total']
            result_data.loc[idx, '第一个月借阅次数'] = stats['month1']
            # ... 其他字段映射
```

### 2.2 主题分组算法

**位置**: `src/core/recommendation/theme_grouper.py`

#### 2.2.1 索书号主题映射

系统基于索书号首字母进行主题分类:

```python
# 主题分类映射
THEME_MAPPING = {
    'A': '马克思主义、哲学、宗教',
    'B': '哲学、宗教', 
    'C': '社会科学总论',
    'D': '政治、法律',
    'E': '军事',
    'F': '经济',
    'G': '文化、科学、教育、体育',
    'H': '语言、文字',
    'I': '文学',
    'J': '艺术',
    'K': '历史、地理',
    'N': '自然科学总论',
    'O': '数理科学与化学',
    'P': '天文学、地球科学',
    'Q': '生物科学',
    'R': '医药、卫生',
    'S': '农业科学',
    'T': '工业技术',
    'U': '交通运输',
    'V': '航空、航天',
    'X': '环境科学、安全科学',
    'Z': '综合性图书'
}
```

## 三、智能筛选算法分析

### 3.1 多维度筛选架构

**位置**: `src/core/data_filter.py`

#### 3.1.1 动态筛选器注册机制

```python
class BookFilterFinal:
    """图书筛选器 - 支持模块化和动态扩展"""
    
    def _init_filters(self):
        """初始化所有筛选器"""
        self.filters = []
        
        # 从配置文件动态创建筛选器
        filtering_config = self.config.get('filtering', {})
        
        for rule_name, rule_config in filtering_config.items():
            if rule_config.get('enabled', False):
                filter_instance = FilterRegistry.create_filter(filter_type, rule_config)
                self.filters.append((filter_name, filter_instance))
```

#### 3.1.2 四层筛选体系

**Rule A: 热门图书排除**
- **目标**: 识别并排除借阅频率最高的"顶流热门"图书
- **方法**: 分位数方法 + 绝对次数方法
- **配置**: 排除前15%最高借阅次数的图书

**Rule B: 特定类型排除**
- **题名关键词筛选**: 基于配置的关键词列表
- **索书号/CLC号筛选**: 使用正则表达式模式匹配

**Rule C: 列值筛选**
- **附加信息格式校验**: 验证9位数字格式
- **备注关键词排除**: 排除包含"不要"、"废书"等关键词的记录

**Rule D: 数据库查重**
- **目标**: 过滤已在评选阶段入选的书目
- **逻辑**: 查询recommendation_results表中manual_selection='通过'的记录

### 3.2 筛选结果追踪机制

```python
def filter_books(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]:
    """执行完整的筛选流程"""
    # 记录每条记录被过滤的原因：{索引: 过滤原因列表}
    exclusion_reasons = {}
    
    for filter_name, filter_instance in self.filters:
        # 获取筛选前后数据的索引差集
        excluded_indices = before_indices - after_indices
        
        # 为被排除的记录添加过滤原因
        filter_reason = self._get_filter_reason_description(filter_name, filter_result, filter_instance)
        for index in excluded_indices:
            if index not in exclusion_reasons:
                exclusion_reasons[index] = []
            exclusion_reasons[index].append(filter_reason)
```

## 四、AI决策引擎深度分析

### 4.1 LLM客户端架构

**位置**: `src/utils/llm/client.py`

#### 4.1.1 统一LLM客户端

```python
class UnifiedLLMClient:
    """读取配置、组织提示词、应用重试并调用 Provider"""
    
    def __init__(self, config_path: str = "config/llm.yaml"):
        self.config_loader = ConfigLoader(config_path)
        self.retry_manager = RetryManager(self.settings)
        self.prompt_loader = PromptLoader()
        self.json_handler = JSONHandler()
```

**核心功能**:
- **配置驱动**: 基于YAML配置文件管理所有LLM调用
- **错误处理**: 指数退避重试机制
- **结构化响应**: JSON解析和修复
- **可观测性**: 集成Langfuse监控

#### 4.1.2 多模型容错机制

```python
api_providers:
  text:
    primary:
      name: "oneapi-qwen3"
      model: "qwen3-max"
    secondary:
      name: "oneapi-4.1" 
      model: "gpt-4.1"
```

### 4.2 三阶段评选流程

**位置**: `src/core/recommendation/controller.py`

#### 4.2.1 初评阶段 (Initial Review)

```python
def run_theme_recommendation_initial(self, excel_path: str) -> Dict[str, List[Dict]]:
    """主题初评：按主题分组进行初筛"""
    
    # 1. 候选数据过滤
    df = df[df.apply(_needs_initial_review, axis=1)].copy()
    
    # 2. 主题分组
    groups = group_by_theme(df)
    
    # 3. 批次处理
    batches = split_batches(books, batch_size)
    
    # 4. LLM调用
    result = executor.initial(theme, batch)
    
    # 5. 结果写入
    writer.write_initial(result)
```

**核心算法**:
- **自适应配额**: 根据分组内书目数量动态调整推荐配额
- **批次控制**: 避免单次调用过多数据影响模型性能
- **断点续传**: 跳过已有合法初评结果的数据

#### 4.2.2 决选阶段 (Runoff)

```python
def run_theme_runoff(self, excel_path: str) -> List[Dict]:
    """主题内决选：智能控制各主题晋级终评的数量上限"""
    
    # 1. 筛选初评通过的书目
    passed_df = df[initial_results == "通过"].copy()
    
    # 2. 按主题分组
    groups = group_by_theme(pending_df)
    
    # 3. 智能决选逻辑
    if theme_count <= quota:
        # 自动晋级
        writer.df.at[idx, "主题内决选结果"] = "自动晋级"
    else:
        # 调用LLM进行决选
        result = executor.runoff(theme, books, quota)
```

#### 4.2.3 终评阶段 (Final Review)

```python
def run_adaptive_final(self, excel_path: str, finalists: List[Dict], top_n: int = 10):
    """自适应终评（漏斗模式）"""
    
    # 场景1: 直接终评
    if total_candidates <= max_batch_size:
        result = executor.final(finalists, top_n)
    
    # 场景2: 海选 + 终评
    else:
        # 阶段1：海选
        for batch in batches:
            result = executor.semifinal(batch, quota_per_batch)
        
        # 阶段2：终评
        final_result = executor.final(semifinal_survivors, top_n)
```

### 4.3 提示词工程设计

**位置**: `src/core/recommendation/prompt_builder.py`

#### 4.3.1 动态提示词构建

```python
def build_initial_prompt(theme: str, books: List[Dict]) -> str:
    """构建初评提示词"""
    quota = recommend_quota(len(books))
    header = f"主题:{theme}\n在所有符合条件的书籍中，请精中选优，最终推荐的书目数量不得超过 {quota} 本。"
    
    blocks = [build_book_block(b) for b in books]
    return header + "\n\n" + "\n---\n".join(blocks)
```

#### 4.3.2 书目信息块构建

```python
def build_book_block(book: Dict) -> str:
    """构建书目信息块"""
    lines = [
        f"书目条码id:{book.get('书目条码','')}",
        f"书名:{book.get('书名','')}",
        f"副标题:{book.get('豆瓣副标题','')}",
        f"作者:{book.get('豆瓣作者','')}",
        f"丛书:{book.get('豆瓣丛书','')}",
        f"内容简介:{book.get('豆瓣内容简介','')}",
        f"作者简介:{book.get('豆瓣作者简介','')}",
        f"目录:{book.get('豆瓣目录','')}",
    ]
    return "\n".join(lines)
```

## 五、MVP原型技术实现细节

### 5.1 系统架构特点

#### 5.1.1 模块化设计

系统采用高内聚、低耦合的模块化架构:

```
src/
├── core/                    # 核心业务逻辑
│   ├── data_loader.py      # 数据加载器
│   ├── data_cleaner.py     # 数据清洗器
│   ├── statistics.py       # 统计分析器
│   ├── data_filter.py      # 智能过滤器
│   ├── recommendation/     # 推荐引擎
│   ├── douban/            # 豆瓣数据获取
│   ├── card_generator/    # 卡片生成器
│   └── new_sleeping/      # 新书分析
├── utils/                  # 工具模块
│   ├── config_manager.py  # 配置管理器
│   ├── logger.py         # 日志系统
│   └── llm/              # LLM客户端
└── scripts/               # 脚本工具
```

#### 5.1.2 配置化管理

**核心配置**:
- `config/setting.yaml`: 主配置文件，包含所有业务逻辑参数
- `config/llm.yaml`: LLM配置，定义模型参数和提示词
- `config/data_filter.yaml`: 数据过滤配置
- `config/.env`: 环境变量配置

### 5.2 数据持久化策略

#### 5.2.1 SQLite数据库设计

**位置**: `src/core/douban/database/database_manager.py`

**核心表结构**:
```sql
CREATE TABLE books (
    barcode TEXT PRIMARY KEY,
    call_no TEXT,
    book_title TEXT,
    isbn TEXT,
    douban_url TEXT,
    douban_rating REAL,
    douban_title TEXT,
    douban_author TEXT,
    douban_summary TEXT,
    douban_cover_image TEXT,
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE recommendation_results (
    barcode TEXT PRIMARY KEY,
    evaluation_batch TEXT,
    preliminary_result TEXT,
    preliminary_score REAL,
    preliminary_reason TEXT,
    final_result TEXT,
    final_score REAL,
    final_reason TEXT,
    manual_selection TEXT,
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### 5.2.2 查重机制

```python
def check_duplicates(self, barcodes: List[str]) -> Dict[str, bool]:
    """批量查重"""
    placeholders = ','.join(['?' for _ in barcodes])
    query = f"SELECT barcode FROM books WHERE barcode IN ({placeholders})"
    
    existing_barcodes = {row[0] for row in self.cursor.execute(query, barcodes)}
    return {barcode: barcode in existing_barcodes for barcode in barcodes}
```

### 5.3 外部API集成策略

#### 5.3.1 豆瓣API集成

**位置**: `src/core/douban/api/`

**技术特点**:
- **速率限制**: QPS控制在2，避免触发反爬
- **User-Agent轮换**: 移动端User-Agent池
- **重试机制**: 指数退避策略
- **缓存机制**: SQLite本地缓存避免重复请求

#### 5.3.2 FOLIO系统集成

```python
class FOLIOISBNProcessor:
    """FOLIO系统ISBN解析器"""
    
    def __init__(self, base_url: str, username: str, password: str):
        self.base_url = base_url
        self.username = username
        self.password = password
        self.session = requests.Session()
```

### 5.4 性能优化策略

#### 5.4.1 异步处理机制

```python
async def process_isbn_batch(self, isbn_list: List[str]):
    """异步处理ISBN批次"""
    tasks = [self.process_single_isbn(isbn) for isbn in isbn_list]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results
```

#### 5.4.2 内存优化

- **流式处理**: 大文件使用pandas的chunksize参数
- **及时释放**: 处理完成后立即释放大型DataFrame
- **增量更新**: 支持断点续传和增量数据处理

## 六、实验流程创新点总结

### 6.1 数据驱动决策的透明性

1. **可追溯的数据链路**: 从原始数据到最终推荐的每一步都有详细记录
2. **配置化的决策逻辑**: 所有筛选和评分标准都通过配置文件管理
3. **量化评估机制**: 提供详细的统计报告和性能指标

### 6.2 AI与规则的协同

1. **规则预筛选 + AI精评**: 先用规则过滤，再用AI评分
2. **多阶段AI决策**: 初评→决选→终评的分层决策机制
3. **人机协同**: AI提供建议，人工进行最终审核

### 6.3 工程实践的创新

1. **配置即代码**: 业务逻辑通过配置文件管理，便于调优
2. **模块化设计**: 高内聚低耦合的架构便于维护和扩展
3. **容错机制**: 多重备用方案确保系统稳定性

## 七、实验局限性与改进方向

### 7.1 当前局限性

1. **数据质量依赖**: 原始数据质量直接影响推荐效果
2. **冷启动问题**: 新书缺乏历史数据支持
3. **外部依赖**: 豆瓣API稳定性影响系统可用性
4. **计算成本**: LLM调用产生一定的API成本

### 7.2 改进方向

1. **数据质量提升**: 增加数据验证和清洗规则
2. **推荐多样性**: 引入多样性约束避免过度集中
3. **用户反馈机制**: 建立用户反馈循环优化推荐效果
4. **模型本地化**: 考虑部署本地模型减少外部依赖

## 结论

"书海回响"系统通过数据驱动和AI技术的深度融合，实现了一个完整的图书馆智能推荐解决方案。系统在数据处理、AI决策、工程实现等方面都有创新之处，是一个具有实际应用价值的学术研究原型。