#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ISBNå¼‚æ­¥æ‰¹é‡å¤„ç†å™¨ - ç²¾ç®€ç‰ˆ

åŸºäºç®€å•çˆ¬è™«é€»è¾‘çš„é«˜æ•ˆå®ç°ï¼š
1. å¤šä¸ªæµè§ˆå™¨å®ä¾‹å¹¶å‘å¤„ç†
2. å•å®ä¾‹ç™»å½•é‡ç”¨
3. å®æ—¶ä¿å­˜åˆ°Excel
4. å¯é…ç½®å¹¶å‘å‚æ•°
"""

import asyncio
import os
import random
import re
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set
import pandas as pd
from playwright.async_api import async_playwright, Browser, BrowserContext, Page, TimeoutError
from playwright.async_api import Error as PlaywrightError

# æ·»åŠ é¡¹ç›®è·¯å¾„
import sys
project_root = Path(__file__).resolve().parent.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

from src.utils.logger import get_logger
from src.utils.config_manager import get_douban_config
from src.core.douban.isbn_processor_config import ProcessingConfig
from .exceptions import FolioNeedsRestart

logger = get_logger(__name__)

ISBN_ALLOWED_PATTERN = re.compile(r'^[0-9-]+$')


class AsyncBrowserWorker:
    """å•ä¸ªå¼‚æ­¥æµè§ˆå™¨å·¥ä½œå™¨"""

    def __init__(
        self,
        worker_id: int,
        username: str,
        password: str,
        base_url: str,
        *,
        min_delay: float = 0.5,
        max_delay: float = 2.0,
        retry_times: int = 3,
        request_timeout: int = 15,
        browser_startup_timeout: int = 180,
        page_navigation_timeout: int = 180,
    ):
        self.worker_id = worker_id
        self.username = username
        self.password = password
        self.base_url = base_url
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None
        self.is_logged_in = False
        self.playwright = None

        self.min_delay = max(0.1, min(min_delay, max_delay))
        self.max_delay = max(self.min_delay, max_delay)
        self.retry_times = max(1, retry_times)
        self.request_timeout_ms = max(1_000, int(request_timeout * 1000))
        self.browser_startup_timeout = max(30, browser_startup_timeout)
        self.page_navigation_timeout_ms = max(self.request_timeout_ms, int(page_navigation_timeout * 1000))
        # æ¯ä¸ªå·¥ä½œå™¨ç‹¬å ä½¿ç”¨æµè§ˆå™¨é¡µï¼Œé¿å…è¾“å…¥/ç‚¹å‡»æ—¶çš„å¹¶å‘å†²çª
        self._page_lock = asyncio.Lock()

    async def _adaptive_delay(self, multiplier: float = 1.0) -> None:
        """æ ¹æ®é…ç½®å»¶è¿Ÿï¼Œé¿å…å¯¹ç›®æ ‡ç«™ç‚¹è¯·æ±‚è¿‡å¿«"""
        base_delay = random.uniform(self.min_delay, self.max_delay)
        await asyncio.sleep(max(0.05, base_delay * max(multiplier, 0.1)))

    async def start(self) -> bool:
        """å¯åŠ¨æµè§ˆå™¨å¹¶ç™»å½•"""
        try:
            logger.info(f"å·¥ä½œè€…{self.worker_id}: å¯åŠ¨æµè§ˆå™¨..")

            async def _launch_and_login():
                self.playwright = await async_playwright().start()
                self.browser = await self.playwright.chromium.launch(
                    headless=False,
                    args=['--no-sandbox', '--disable-setuid-sandbox'],
                )
                self.context = await self.browser.new_context()
                self.page = await self.context.new_page()
                await self._login()
                logger.info(f"å·¥ä½œè€…{self.worker_id}: å¯åŠ¨å®Œæˆ")
                return True

            return await asyncio.wait_for(_launch_and_login(), timeout=self.browser_startup_timeout)

        except asyncio.TimeoutError:
            logger.error(f"å·¥ä½œè€…{self.worker_id}: æµè§ˆå™¨å¯åŠ¨è¶…æ—¶ï¼ˆ>{self.browser_startup_timeout}sï¼‰")
            await self.close()
            return False
        except Exception as exc:
            logger.error(f"å·¥ä½œè€…{self.worker_id}: å¯åŠ¨å¤±è´¥ - {exc}")
            await self.close()
            return False

    async def _login(self) -> None:
        """æ‰§è¡Œç™»å½•æ“ä½œ"""
        try:
            logger.info(f"å·¥ä½œè€…{self.worker_id}: å¼€å§‹ç™»å½•..")
            await self.page.goto(
                self.base_url,
                wait_until='networkidle',
                timeout=self.page_navigation_timeout_ms,
            )

            await self.page.wait_for_selector('#input-username', timeout=self.request_timeout_ms)
            await self.page.fill('#input-username', self.username)

            await self.page.wait_for_selector('#input-password', timeout=self.request_timeout_ms)
            await self.page.fill('#input-password', self.password)

            await self.page.wait_for_selector('#clickable-login', timeout=self.request_timeout_ms)
            await self.page.click('#clickable-login')

            await self.page.wait_for_selector('.ant-input', timeout=self.page_navigation_timeout_ms)
            self.is_logged_in = True
            logger.info(f"å·¥ä½œè€…{self.worker_id}: ç™»å½•æˆåŠŸ")

        except Exception as exc:
            logger.error(f"å·¥ä½œè€…{self.worker_id}: ç™»å½•å¤±è´¥ - {exc}")
            raise

    async def get_isbn(self, barcode: str) -> Optional[str]:
        """è·å–å•ä¸ªæ¡ç çš„ISBN"""
        async with self._page_lock:
            if not self.is_logged_in:
                logger.warning(f"å·¥ä½œè€…{self.worker_id}: æœªç™»å½•çŠ¶æ€ï¼Œé‡æ–°ç™»å½•")
                await self._login()

            for attempt in range(self.retry_times):
                try:
                    logger.debug(f"å·¥ä½œè€…{self.worker_id}: æ¡ç  {barcode} ç¬¬ {attempt + 1} æ¬¡å°è¯•")
                    await self.page.fill('.ant-input', str(barcode))
                    await self.page.wait_for_selector('.ant-input-search-button', timeout=self.request_timeout_ms)
                    await self.page.click('.ant-input-search-button')

                    await self.page.wait_for_selector('tr.ant-descriptions-row', timeout=self.page_navigation_timeout_ms)
                    result_ready = await self._wait_for_barcode_update(str(barcode))
                    if not result_ready:
                        if attempt == self.retry_times - 1:
                            logger.warning(f"å·¥ä½œè€…{self.worker_id}: æ¡ç  {barcode} å¤šæ¬¡åˆ·æ–°å¤±è´¥")
                            return None
                        await self._adaptive_delay()
                        continue

                    data = {}
                    rows = await self.page.query_selector_all('tr.ant-descriptions-row')
                    for row in rows:
                        ths = await row.query_selector_all('th.ant-descriptions-item-label')
                        tds = await row.query_selector_all('td.ant-descriptions-item-content')
                        for i in range(min(len(ths), len(tds))):
                            key = (await ths[i].inner_text()).strip()
                            value = (await tds[i].inner_text()).strip()
                            if key in ['ISBN', 'ISBN/ISSN']:
                                data[key] = value

                    for field in ['ISBN', 'ISBN/ISSN']:
                        if field in data and data[field]:
                            isbn_value = data[field].strip()
                            if isbn_value and isbn_value not in ['/', '-', '']:
                                logger.debug(f"å·¥ä½œè€…{self.worker_id}: æ¡ç  {barcode} -> ISBN: {isbn_value} (ç¬¬{attempt + 1}æ¬¡å°è¯•)")
                                return isbn_value

                    if attempt == self.retry_times - 1:
                        logger.warning(f"å·¥ä½œè€…{self.worker_id}: æ¡ç  {barcode} é‡è¯•ä¸Šé™ä»æœªè·å–ISBN")
                        return None
                    await self._adaptive_delay()

                except TimeoutError as exc:
                    await self._handle_worker_restart(barcode, f"FOLIO æœç´¢è¶…æ—¶: {exc}")
                except PlaywrightError as exc:
                    if self._should_restart_exception(exc):
                        await self._handle_worker_restart(barcode, f"FOLIO æµè§ˆå™¨å¼‚å¸¸: {exc}")
                    if attempt == self.retry_times - 1:
                        logger.error(f"å·¥ä½œè€…{self.worker_id}: Playwrighté”™è¯¯å¯¼è‡´å¤±è´¥ - {exc}")
                        return None
                    logger.warning(f"å·¥ä½œè€…{self.worker_id}: Playwrighté”™è¯¯ï¼Œå‡†å¤‡é‡è¯• - {exc}")
                    await self._adaptive_delay(1.5)
                except Exception as exc:
                    if attempt == self.retry_times - 1:
                        logger.error(f"å·¥ä½œè€…{self.worker_id}: è·å–æ¡ç  {barcode} çš„ISBNå¤±è´¥ - {exc}")
                        return None
                    logger.warning(f"å·¥ä½œè€…{self.worker_id}: æ¡ç  {barcode} ç¬¬ {attempt + 1} æ¬¡å°è¯•å‡ºç°é—®é¢˜ - {exc}")
                    await self._adaptive_delay(1.5)

            return None

    async def close(self) -> None:
        """å…³é—­æµè§ˆå™¨"""
        try:
            if self.page:
                await self.page.close()
                self.page = None
            if self.context:
                await self.context.close()
                self.context = None
            if self.browser:
                await self.browser.close()
                self.browser = None
            if self.playwright:
                await self.playwright.stop()
                self.playwright = None
            self.is_logged_in = False
        except Exception as exc:
            logger.error(f"å·¥ä½œè€…{self.worker_id}: å…³é—­æµè§ˆå™¨å¤±è´¥ - {exc}")
    async def _restart_browser(self) -> None:
        """æ•´ä½“é‡å¯æµè§ˆå™¨å¹¶é‡æ–°ç™»å½•"""
        logger.warning(f"å·¥ä½œè€…{self.worker_id}: æ­£åœ¨é‡å¯FOLIOæµè§ˆå™¨")
        await self.close()
        started = await self.start()
        if not started:
            raise RuntimeError(f"å·¥ä½œè€…{self.worker_id}: æµè§ˆå™¨é‡å¯å¤±è´¥")

    async def _handle_worker_restart(self, barcode: str, message: str) -> None:
        """æ‰§è¡Œæµè§ˆå™¨é‡å¯å¹¶å‘ä¸ŠæŠ›å‡ºéœ€è¦é‡è¯•çš„å¼‚å¸¸"""
        try:
            await self._restart_browser()
        finally:
            raise FolioNeedsRestart(message, barcode=barcode)

    def _should_restart_exception(self, exc: Exception) -> bool:
        """åˆ¤æ–­å¼‚å¸¸ç±»å‹æ˜¯å¦éœ€è¦æ•´ä½“é‡å¯æµè§ˆå™¨"""
        message = str(exc).lower()
        restart_markers = ['target closed', 'browser has been closed', 'context destroyed']
        return any(marker in message for marker in restart_markers)

    async def _wait_for_barcode_update(self, barcode: str, timeout: Optional[int] = None) -> bool:
        """ç­‰å¾…è¯¦æƒ…é¢æ¿åˆ·æ–°åˆ°æŒ‡å®šæ¡ç ï¼Œé¿å…è¯»å–åˆ°ä¸Šä¸€æ¬¡æŸ¥è¯¢çš„æ•°æ®"""
        if not self.page:
            return False
        timeout = timeout or self.page_navigation_timeout_ms

        try:
            await self.page.wait_for_function(
                """
                (targetBarcode) => {
                    const normalize = (text) =>
                        (text || '').trim().replace(/[\\s:ï¼š]/g, '');
                    const targetLabels = new Set([
                        'æ¡ç å·',
                        'æ¡ç ',
                        'é¦†è—æ¡ç ',
                        'é¦†è—æ¡ç å·'
                    ]);

                    const rows = document.querySelectorAll('tr.ant-descriptions-row');
                    for (const row of rows) {
                        const labels = row.querySelectorAll('th.ant-descriptions-item-label');
                        const contents = row.querySelectorAll('td.ant-descriptions-item-content');
                        for (let i = 0; i < Math.min(labels.length, contents.length); i++) {
                            const label = normalize(labels[i].textContent || '');
                            if (!targetLabels.has(label)) {
                                continue;
                            }
                            const content = contents[i] && contents[i].textContent
                                ? contents[i].textContent.trim()
                                : '';
                            if (content === targetBarcode) {
                                return true;
                            }
                        }
                    }
                    return false;
                }
                """,
                arg=str(barcode),
                timeout=timeout,
            )
            return True
        except TimeoutError:
            logger.warning(
                f"å·¥ä½œè€…{self.worker_id}: æ¡ç  {barcode} ç­‰å¾…è¯¦æƒ…åˆ·æ–°è¶…è¿‡ {timeout}ms"
            )
            return False


class ISBNAsyncProcessor:
    """ISBNå¼‚æ­¥å¤„ç†å™¨ä¸»ç±»"""

    def __init__(self, max_concurrent: int = 2, save_interval: int = 25,
                 enable_database: bool = False, db_config: Dict = None,
                 processing_config: Optional[ProcessingConfig] = None):
        """åˆå§‹åŒ–å¼‚æ­¥å¤„ç†å™¨"""

        self.processing_config = processing_config
        self.max_concurrent = (processing_config.max_concurrent
                                if processing_config else max_concurrent)
        self.save_interval = max(0, int(save_interval))
        self.database_enabled = enable_database
        self.db_config = db_config or {}
        self.batch_size = (processing_config.batch_size
                          if processing_config and processing_config.batch_size else 0)
        self.min_delay = processing_config.min_delay if processing_config else 0.5
        self.max_delay = processing_config.max_delay if processing_config else 2.0
        self.retry_times = processing_config.retry_times if processing_config else 3
        self.request_timeout = processing_config.timeout if processing_config else 15
        self.browser_startup_timeout = (processing_config.browser_startup_timeout
                                        if processing_config else 180)
        self.page_navigation_timeout = (processing_config.page_navigation_timeout
                                        if processing_config else 180)

        # ä»é…ç½®æ–‡ä»¶è·å–é»˜è®¤å€¼
        douban_config = get_douban_config()
        isbn_processor_conf = douban_config.get('isbn_processor', {}) if isinstance(douban_config, dict) else {}
        performance_conf = isbn_processor_conf.get('performance', {}) if isinstance(isbn_processor_conf, dict) else {}
        self.save_interval_seconds = max(0, int(performance_conf.get('save_interval_seconds', 180)))
        isbn_config = douban_config.get('isbn_resolver', {})

        self.username = isbn_config.get('username')
        self.password = isbn_config.get('password')
        self.base_url = isbn_config.get('base_url', "https://circ-folio.library.sh.cn/shlcirculation-query/literatureHistory")

        if not self.username or not self.password:
            raise ValueError("è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®FOLIOç”¨æˆ·åå’Œå¯†ç ")

        self.workers: List[AsyncBrowserWorker] = []
        self.stats = {
            'total_processed': 0,
            'successful_isbn': 0,
            'failed_isbn': 0,
            'skipped_count': 0
        }

        # åˆ›å»ºå¼‚æ­¥é”é˜²æ­¢å¹¶å‘å†™å…¥å†²çª
        self.write_lock = asyncio.Lock()
        self._worker_assign_lock = asyncio.Lock()
        self._next_worker_index = 0
        # ç»“æœå­˜å‚¨ï¼ˆé¿å…ç›´æ¥ä¿®æ”¹DataFrameï¼‰
        self.results_buffer = {}
        self._pending_indices: Set[int] = set()
        self._final_only_indices: Set[int] = set()
        self._last_flush_ts = time.monotonic()
        self._row_index_cache: Dict[str, int] = {}
        self._row_cache_column: Optional[str] = None
        self._db_result_cache: Dict[str, str] = {}
        self.output_column: Optional[str] = None
        self.barcode_column: Optional[str] = None

        # åˆå§‹åŒ–æ•°æ®åº“ç»„ä»¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.db_manager = None
        self.data_checker = None
        if self.database_enabled:
            self._init_database_components()

        logger.info(
            f"ISBNAsyncProcessoråˆå§‹åŒ–å®Œæˆï¼Œå¹¶å‘æ•°: {self.max_concurrent}, ä¿å­˜é—´éš”: {save_interval}, æ•°æ®åº“: {'å¯ç”¨' if enable_database else 'ç¦ç”¨'}"
        )

    def _init_database_components(self):
        """åˆå§‹åŒ–æ•°æ®åº“ç»„ä»¶"""
        try:
            from .database.database_manager import DatabaseManager
            from .database.data_checker import DataChecker

            # è·å–æ•°æ®åº“é…ç½®
            db_path = self.db_config.get('db_path', 'books_history.db')
            refresh_config = self.db_config.get('refresh_strategy', {})

            # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
            self.db_manager = DatabaseManager(db_path)
            self.db_manager.init_database()

            # åˆå§‹åŒ–æŸ¥é‡å¤„ç†å™¨
            self.data_checker = DataChecker(self.db_manager, refresh_config)

            logger.info("æ•°æ®åº“ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")

        except Exception as e:
            logger.error(f"åˆå§‹åŒ–æ•°æ®åº“ç»„ä»¶å¤±è´¥: {e}")
            raise

    async def start_workers(self) -> bool:
        """å¯åŠ¨æ‰€æœ‰å·¥ä½œå™¨"""
        try:
            logger.info(f"å¯åŠ¨ {self.max_concurrent} ä¸ªå·¥ä½œå™¨...")

            # åˆ›å»ºå·¥ä½œå™¨
            for i in range(self.max_concurrent):
                worker = AsyncBrowserWorker(
                    worker_id=i + 1,
                    username=self.username,
                    password=self.password,
                    base_url=self.base_url,
                    min_delay=self.min_delay,
                    max_delay=self.max_delay,
                    retry_times=self.retry_times,
                    request_timeout=self.request_timeout,
                    browser_startup_timeout=self.browser_startup_timeout,
                    page_navigation_timeout=self.page_navigation_timeout,
                )
                self.workers.append(worker)

            # å¹¶å‘å¯åŠ¨å·¥ä½œå™¨
            tasks = [worker.start() for worker in self.workers]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            success_count = sum(1 for r in results if r is True)
            if success_count == 0:
                raise Exception("æ‰€æœ‰å·¥ä½œå™¨å¯åŠ¨å¤±è´¥")

            logger.info(f"æˆåŠŸå¯åŠ¨ {success_count}/{self.max_concurrent} ä¸ªå·¥ä½œå™¨")
            self._next_worker_index = 0
            return True

        except Exception as e:
            logger.error(f"å¯åŠ¨å·¥ä½œå™¨å¤±è´¥: {e}")
            await self.stop_workers()
            return False

    async def stop_workers(self):
        """åœæ­¢æ‰€æœ‰å·¥ä½œå™¨"""
        logger.info("åœæ­¢æ‰€æœ‰å·¥ä½œå™¨...")
        for worker in self.workers:
            await worker.close()
        self.workers.clear()
        self._next_worker_index = 0

    async def _processor_delay(self, multiplier: float = 1.0) -> None:
        """æ ¹æ®é…ç½®ä¸ºè°ƒåº¦è¿‡ç¨‹æ·»åŠ è½»é‡èŠ‚æµ"""
        base_delay = random.uniform(self.min_delay, self.max_delay)
        await asyncio.sleep(max(0.05, base_delay * max(multiplier, 0.1)))

    async def _get_next_worker(self) -> AsyncBrowserWorker:
        """æŒ‰é¡ºåºåˆ†é…ä¸‹ä¸€ä¸ªå·¥ä½œå™¨ï¼Œé¿å…æŸäº›å®ä¾‹é•¿æœŸç©ºé—²"""
        if not self.workers:
            raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„æµè§ˆå™¨å·¥ä½œå™¨")
        async with self._worker_assign_lock:
            worker = self.workers[self._next_worker_index]
            self._next_worker_index = (self._next_worker_index + 1) % len(self.workers)
            return worker

    async def process_excel_file(self,
                                excel_file_path: str,
                                barcode_column: str = "ä¹¦ç›®æ¡ç ",
                                output_column: str = "ISBNå·",
                                retry_failed: bool = True) -> Tuple[str, Dict]:
        """
        æ‰¹é‡å¤„ç†Excelæ–‡ä»¶

        Args:
            excel_file_path: Excelæ–‡ä»¶è·¯å¾„
            barcode_column: ä¹¦ç›®æ¡ç åˆ—å
            output_column: è¾“å‡ºISBNåˆ—å
            retry_failed: æ˜¯å¦åœ¨ç¬¬ä¸€è½®å¤„ç†å®Œåé‡è¯•å¤±è´¥çš„æ¡ç ï¼ˆé»˜è®¤Trueï¼‰

        Returns:
            Tuple[str, Dict]: (è¾“å‡ºæ–‡ä»¶è·¯å¾„, ç»Ÿè®¡ä¿¡æ¯)
        """
        try:
            start_time = time.time()
            logger.info(f"å¼€å§‹å¼‚æ­¥æ‰¹é‡å¤„ç†: {excel_file_path}")

            # è¯»å–Excelæ–‡ä»¶
            if not os.path.exists(excel_file_path):
                raise FileNotFoundError(f"Excelæ–‡ä»¶ä¸å­˜åœ¨: {excel_file_path}")

            df = pd.read_excel(excel_file_path)
            total_records = len(df)
            logger.info(f"è¯»å–åˆ° {total_records} æ¡è®°å½•")

            # æ£€æŸ¥å¿…éœ€çš„åˆ—
            if barcode_column not in df.columns:
                raise ValueError(f"Excelæ–‡ä»¶ä¸­ä¸å­˜åœ¨åˆ— '{barcode_column}'")

            # åˆå§‹åŒ–è¾“å‡ºåˆ—
            if output_column not in df.columns:
                df[output_column] = ""

            self.barcode_column = barcode_column
            self.output_column = output_column
            self._initialize_row_index_cache(df, barcode_column)

            valid_isbn_rows = self._collect_rows_with_valid_isbn(df, output_column)
            if valid_isbn_rows:
                logger.info(f"æ£€æµ‹åˆ° {len(valid_isbn_rows)} æ¡è®°å½•çš„ISBNåˆ—å·²å­˜åœ¨åˆæ³•å€¼ï¼Œå°†è·³è¿‡åç»­å¤„ç†")
                self.stats['skipped_count'] += len(valid_isbn_rows)

            pending_indices = [idx for idx in range(total_records) if idx not in valid_isbn_rows]
            pending_index_set = set(pending_indices)

            if not pending_indices:
                stats = {
                    'total_records': total_records,
                    'success_count': self.stats['successful_isbn'],
                    'failed_count': self.stats['failed_isbn'],
                    'skipped_count': self.stats['skipped_count'],
                    'success_rate': self.stats['successful_isbn'] / (total_records - self.stats['skipped_count']) * 100 if total_records > self.stats['skipped_count'] else 0,
                    'processing_time': time.time() - start_time,
                    'retry_enabled': retry_failed,
                    'failed_after_retry': 0
                }
                logger.info("æ‰€æœ‰è®°å½•çš„ISBNåˆ—å‡å·²åŒ…å«åˆæ³•å€¼ï¼Œæ— éœ€ç»§ç»­å¤„ç†")
                return excel_file_path, stats

            # ============================================================
            # æ•°æ®åº“æŸ¥é‡ï¼ˆå¦‚æœå¯ç”¨æ•°æ®åº“åŠŸèƒ½ï¼‰
            # ============================================================
            categories = None
            if self.database_enabled and self.data_checker:
                logger.info("=" * 80)
                logger.info("æ‰§è¡Œæ•°æ®åº“æŸ¥é‡...")
                logger.info("=" * 80)

                # å°†DataFrameè½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                excel_data = df.iloc[pending_indices].to_dict('records')

                # æ‰§è¡ŒæŸ¥é‡åˆ†ç±»
                categories = self.data_checker.check_and_categorize_books(excel_data)

                # è®°å½•æŸ¥é‡ç»Ÿè®¡
                self.stats['existing_valid_count'] = len(categories['existing_valid'])
                self.stats['existing_stale_count'] = len(categories['existing_stale'])
                self.stats['new_count'] = len(categories['new'])

                logger.info(
                    f"æŸ¥é‡å®Œæˆ: "
                    f"æœ‰æ•ˆ={self.stats['existing_valid_count']}, "
                    f"è¿‡æœŸ={self.stats['existing_stale_count']}, "
                    f"æ–°æ•°æ®={self.stats['new_count']}"
                )

                # å¤„ç†å·²æœ‰æœ‰æ•ˆæ•°æ®ï¼ˆä»æ•°æ®åº“è·å–ï¼Œä¸çˆ¬å–ï¼‰
                if categories['existing_valid']:
                    await self._process_existing_valid_books(
                        categories['existing_valid'], df, excel_file_path, output_column
                    )

            # å¯åŠ¨å·¥ä½œå™¨
            if not await self.start_workers():
                raise Exception("å·¥ä½œå™¨å¯åŠ¨å¤±è´¥")

            # æ¸…ç©ºç»“æœç¼“å†²åŒº
            self.results_buffer = {}
            self._pending_indices.clear()
            self._final_only_indices.clear()
            self._db_result_cache.clear()
            self._last_flush_ts = time.monotonic()

            # ============================================================
            # ç¬¬ä¸€è½®å¤„ç†
            # ============================================================
            logger.info("=" * 80)
            logger.info("ç¬¬ä¸€è½®å¤„ç†ï¼šæ‰¹é‡è·å–ISBN")
            logger.info("=" * 80)

            # éœ€è¦å¤„ç†çš„æ•°æ®ï¼šè¿‡æœŸæ•°æ® + æ–°æ•°æ®
            data_to_process = []
            if categories:
                # ğŸ”§ ä¿®å¤ï¼šéœ€è¦å¤„ç†çš„æ•°æ®ï¼ˆå·²æœ‰è¿‡æœŸæ•°æ® + æ–°æ•°æ®ï¼‰
                # æ„å»ºéœ€è¦å¤„ç†çš„ç´¢å¼•åˆ—è¡¨
                all_books_to_process = categories['existing_stale'] + categories['new']

                # ä»å­—å…¸æ•°æ®ä¸­æå–barcodeï¼Œç„¶åæŸ¥æ‰¾å¯¹åº”çš„ç´¢å¼•
                for book_data in all_books_to_process:
                    barcode = book_data.get('barcode') or book_data.get('ä¹¦ç›®æ¡ç ')
                    if barcode:
                        # æŸ¥æ‰¾barcodeå¯¹åº”çš„Excelç´¢å¼•
                        index = self._find_row_index(df, barcode)
                        if index != -1 and index in pending_index_set:
                            data_to_process.append(index)

                logger.info(f"éœ€è¦é‡æ–°çˆ¬å–çš„è®°å½•æ•°: {len(data_to_process)} æ¡")
            else:
                # æ‰€æœ‰å¾…å¤„ç†çš„æ•°æ®éƒ½æ¥è‡ªå‰©ä½™ç´¢å¼•
                data_to_process = pending_indices.copy()

            skip_existing_isbn = categories is None
            failed_barcodes_first_round = await self._process_batch(
                df, excel_file_path, barcode_column, output_column, total_records,
                batch_name="ç¬¬ä¸€è½®", data_to_process_indices=data_to_process,
                skip_existing_with_value=skip_existing_isbn,
            )

            # ============================================================
            # ç¬¬äºŒè½®å¤„ç†ï¼šé‡è¯•å¤±è´¥çš„æ¡ç 
            # ============================================================
            if retry_failed and failed_barcodes_first_round:
                logger.info("=" * 80)
                logger.info(f"ç¬¬äºŒè½®å¤„ç†ï¼šé‡è¯•å¤±è´¥çš„æ¡ç ï¼ˆå…± {len(failed_barcodes_first_round)} æ¡ï¼‰")
                logger.info("=" * 80)

                # é‡æ–°å¤„ç†å¤±è´¥çš„æ¡ç 
                failed_barcodes_second_round = await self._process_failed_barcodes(
                    df, excel_file_path, failed_barcodes_first_round,
                    batch_name="ç¬¬äºŒè½®"
                )

                # è®°å½•æœ€ç»ˆç»Ÿè®¡
                retry_success_count = len(failed_barcodes_first_round) - len(failed_barcodes_second_round)
                logger.info(f"ç¬¬äºŒè½®é‡è¯•ç»“æœï¼š")
                logger.info(f"  é‡è¯•æˆåŠŸ: {retry_success_count} æ¡")
                logger.info(f"  ä»å¤±è´¥: {len(failed_barcodes_second_round)} æ¡")
            else:
                failed_barcodes_second_round = failed_barcodes_first_round
                if not retry_failed:
                    logger.info("å·²ç¦ç”¨å¤±è´¥æ¡ç é‡è¯•åŠŸèƒ½")

            # æœ€ç»ˆä¿å­˜ - ä¿è¯æ‰€æœ‰ç¼“å†²ç»“æœè½ç›˜
            await self._maybe_flush_results(df, excel_file_path, force=True)

            # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            stats = {
                'total_records': total_records,
                'success_count': self.stats['successful_isbn'],
                'failed_count': self.stats['failed_isbn'],
                'skipped_count': self.stats['skipped_count'],
                'success_rate': self.stats['successful_isbn'] / (total_records - self.stats['skipped_count']) * 100 if total_records > self.stats['skipped_count'] else 0,
                'processing_time': time.time() - start_time,
                'retry_enabled': retry_failed,
                'failed_after_retry': len(failed_barcodes_second_round)
            }

            logger.info("=" * 80)
            logger.info("å¼‚æ­¥æ‰¹é‡å¤„ç†å®Œæˆ:")
            logger.info("=" * 80)
            logger.info(f"  æ€»è®°å½•æ•°: {stats['total_records']}")
            logger.info(f"  æˆåŠŸè·å–: {stats['success_count']}")
            logger.info(f"  è·å–å¤±è´¥: {stats['failed_count']}")
            logger.info(f"  è·³è¿‡è®°å½•: {stats['skipped_count']}")
            logger.info(f"  æˆåŠŸç‡: {stats['success_rate']:.2f}%")
            logger.info(f"  å¤„ç†æ—¶é—´: {stats['processing_time']:.2f}ç§’")
            if retry_failed:
                logger.info(f"  é‡è¯•åä»å¤±è´¥: {stats['failed_after_retry']} æ¡")
            logger.info("=" * 80)

            return excel_file_path, stats

        except Exception as e:
            logger.error(f"å¼‚æ­¥æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
            raise
        finally:
            if 'df' in locals():
                try:
                    await self._maybe_flush_results(df, excel_file_path, force=True)
                except Exception as flush_error:
                    logger.error(f"æ¸…ç†é˜¶æ®µä¿å­˜ç»“æœå¤±è´¥: {flush_error}")
            await self.stop_workers()

    async def _process_batch(self, df: pd.DataFrame, excel_file_path: str,
                           barcode_column: str, output_column: str, total_records: int,
                           batch_name: str = "å¤„ç†", data_to_process_indices: List[int] = None,
                           skip_existing_with_value: bool = True) -> List[Tuple[int, str]]:
        """æ‰¹é‡å¤„ç†æ¡ç ï¼ˆç¬¬ä¸€è½®æˆ–ç¬¬äºŒè½®ï¼‰"""
        failed_barcodes: List[Tuple[int, str]] = []

        semaphores = asyncio.Semaphore(self.max_concurrent)

        async def process_with_semaphore(index: int, barcode: str):
            async with semaphores:
                return await self._process_single_barcode_safe(index, barcode)

        tasks: List[Tuple[int, asyncio.Task]] = []
        for index, row in df.iterrows():
            if data_to_process_indices is not None and index not in data_to_process_indices:
                continue
            barcode = str(row[barcode_column]).strip()
            if not barcode or barcode == 'nan':
                self._stage_result(index, "è·³è¿‡ï¼šç©ºæ¡ç ")
                self.stats['skipped_count'] += 1
                continue
            if skip_existing_with_value:
                current_value = df.at[index, output_column]
                if not pd.isna(current_value) and str(current_value).strip():
                    self._stage_result(index, str(current_value).strip())
                    self.stats['total_processed'] += 1
                    continue
            task = process_with_semaphore(index, barcode)
            tasks.append((index, task))

        logger.info(f"{batch_name}ï¼šå¼€å§‹å¹¶å‘å¤„ç†{len(tasks)} æ¡è®°å½•..")
        if not tasks:
            return failed_barcodes

        async def _run_task_chunk(task_group: List[Tuple[int, asyncio.Task]]) -> int:
            results = await asyncio.gather(*[task for _, task in task_group], return_exceptions=True)
            for (index, _), result in zip(task_group, results):
                if isinstance(result, Exception):
                    logger.error(f"å¤„ç†ç¬¬ {index + 1} æ¡è®°å½•æ—¶å‘ç”Ÿå¼‚å¸¸: {result}")
                    result = "è·å–å¤±è´¥"
                self._stage_result(index, result)
                if result == "è·å–å¤±è´¥":
                    barcode = str(df.at[index, barcode_column]).strip()
                    failed_barcodes.append((index, barcode))
            await self._maybe_flush_results(df, excel_file_path)
            return len(task_group)

        processed_count = 0
        chunk_size = self.batch_size if self.batch_size and self.batch_size > 0 else len(tasks)
        chunk_size = max(1, chunk_size)
        for start_idx in range(0, len(tasks), chunk_size):
            chunk = tasks[start_idx:start_idx + chunk_size]
            processed_count += await _run_task_chunk(chunk)

        await self._maybe_flush_results(df, excel_file_path, force=True)
        logger.info(f"{batch_name}å¤„ç†å®Œæˆï¼Œå¤±è´¥ {len(failed_barcodes)} æ¡")
        return failed_barcodes

    async def _process_failed_barcodes(self, df: pd.DataFrame, excel_file_path: str,
                                     failed_barcodes: List[Tuple[int, str]],
                                     batch_name: str = "é‡è¯•") -> List[Tuple[int, str]]:
        """
        é‡æ–°å¤„ç†å¤±è´¥çš„æ¡ç 

        Args:
            df: DataFrameå¯¹è±¡
            excel_file_path: Excelæ–‡ä»¶è·¯å¾„
            failed_barcodes: å¤±è´¥çš„æ¡ç åˆ—è¡¨ [(index, barcode), ...]
            batch_name: æ‰¹æ¬¡åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰

        Returns:
            List[Tuple[int, str]]: ä»ç„¶å¤±è´¥çš„æ¡ç åˆ—è¡¨
        """
        still_failed = []

        # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
        semaphores = asyncio.Semaphore(self.max_concurrent)

        # é‡æ–°å¤„ç†æ¯ä¸ªå¤±è´¥çš„æ¡ç 
        for index, barcode in failed_barcodes:
            async with semaphores:
                logger.info(f"{batch_name}ï¼šé‡æ–°å¤„ç†æ¡ç  {barcode} (è¡Œ {index + 1})")

                # å†æ¬¡è·å–ISBN
                isbn = await self._process_single_barcode_safe(index, barcode, is_retry=True)

                # æ›´æ–°ç»“æœç¼“å†²åŒº
                self._stage_result(index, isbn)

                # å¦‚æœä»ç„¶å¤±è´¥ï¼Œè®°å½•
                if isbn == "è·å–å¤±è´¥":
                    still_failed.append((index, barcode))

                # ç­‰å¾…ä¸€å°æ®µæ—¶é—´
                await self._processor_delay(0.5)

        await self._maybe_flush_results(df, excel_file_path, force=True)

        logger.info(f"{batch_name}é‡è¯•å®Œæˆï¼Œä»æœ‰ {len(still_failed)} æ¡å¤±è´¥")

        return still_failed

    async def _process_single_barcode_safe(self, index: int, barcode: str, is_retry: bool = False) -> str:
        """
        å®‰å…¨å¤„ç†å•ä¸ªæ¡ç ï¼ˆä½¿ç”¨ç»“æœç¼“å†²åŒºé¿å…å¹¶å‘å†²çªï¼‰

        Args:
            index: è®°å½•ç´¢å¼•
            barcode: æ¡ç å€¼
            is_retry: æ˜¯å¦ä¸ºé‡è¯•æ“ä½œï¼ˆå½±å“æ—¥å¿—è¾“å‡ºï¼‰

        Returns:
            str: å¤„ç†ç»“æœï¼ˆISBNå·æˆ–"è·å–å¤±è´¥"ï¼‰
        """
        try:
            # é€‰æ‹©å·¥ä½œå™¨ï¼ˆè½®è¯¢ï¼‰
            worker = await self._get_next_worker()

            # æ ¹æ®æ˜¯å¦ä¸ºé‡è¯•ç”Ÿæˆä¸åŒçš„æ—¥å¿—å‰ç¼€
            log_prefix = "é‡è¯•" if is_retry else "å¤„ç†"
            logger.info(f"å·¥ä½œå™¨ {worker.worker_id}: {log_prefix}ç¬¬ {index + 1} æ¡è®°å½•: {barcode}")

            # è·å–ISBN
            isbn = await worker.get_isbn(barcode)

            if isbn:
                self.stats['successful_isbn'] += 1
                logger.info(f"âœ“ æˆåŠŸè·å–ISBN: {isbn}")
                result = isbn
            else:
                self.stats['failed_isbn'] += 1
                logger.warning(f"âœ— è·å–ISBNå¤±è´¥: {barcode}")
                result = "è·å–å¤±è´¥"

            self.stats['total_processed'] += 1

            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´é¿å…è¿‡å¿«è¯·æ±‚
            await self._processor_delay(0.5)

            return result

        except Exception as e:
            error_msg = f"é”™è¯¯: {str(e)[:50]}"
            logger.error(f"å¤„ç†è®°å½• {index + 1} æ—¶å‡ºé”™: {str(e)}")
            self.stats['failed_isbn'] += 1
            self.stats['total_processed'] += 1
            return error_msg

    async def _process_single_barcode(self, index: int, barcode: str, df: pd.DataFrame,
                                     excel_file_path: str, output_column: str):
        """å¤„ç†å•ä¸ªæ¡ç ï¼ˆä¿ç•™ä»¥å…¼å®¹æ—§ç‰ˆæœ¬ï¼‰"""
        try:
            # é€‰æ‹©å·¥ä½œå™¨ï¼ˆè½®è¯¢ï¼‰
            worker = await self._get_next_worker()

            logger.info(f"å·¥ä½œå™¨ {worker.worker_id}: å¤„ç†ç¬¬ {index + 1} æ¡è®°å½•: {barcode}")

            # è·å–ISBN
            isbn = await worker.get_isbn(barcode)

            if isbn:
                df.at[index, output_column] = isbn
                self.stats['successful_isbn'] += 1
                logger.info(f"âœ“ æˆåŠŸè·å–ISBN: {isbn}")
            else:
                df.at[index, output_column] = "è·å–å¤±è´¥"
                self.stats['failed_isbn'] += 1
                logger.warning(f"âœ— è·å–ISBNå¤±è´¥: {barcode}")

            self.stats['total_processed'] += 1

            # ç«‹å³ä¿å­˜åˆ°Excel
            await self._save_to_excel(df, excel_file_path, index + 1)

            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´é¿å…è¿‡å¿«è¯·æ±‚
            await self._processor_delay(0.5)

        except Exception as e:
            error_msg = f"å¤„ç†è®°å½• {index + 1} æ—¶å‡ºé”™: {str(e)}"
            logger.error(error_msg)
            df.at[index, output_column] = f"é”™è¯¯: {str(e)[:50]}"
            self.stats['failed_isbn'] += 1
            await self._save_to_excel(df, excel_file_path, index + 1)

    async def _save_results_to_excel(self, df: pd.DataFrame, excel_file_path: str, processed_count: int, is_final: bool = False):
        """Write buffered results to Excel with atomic rename semantics."""
        if not self.output_column:
            logger.warning("è¾“å‡ºåˆ—æœªè®¾ç½®ï¼Œè·³è¿‡ä¿å­˜è¯·æ±‚")
            return

        async with self.write_lock:
            try:
                if is_final:
                    target_indices = list(self.results_buffer.keys())
                else:
                    target_indices = list(self._pending_indices)

                if not target_indices:
                    return

                for index in target_indices:
                    value = self.results_buffer.get(index)
                    if value is None:
                        continue
                    df.at[index, self.output_column] = value

                temp_file = excel_file_path.replace('.xlsx', f'_tmp_{int(time.time())}.xlsx')
                df.to_excel(temp_file, index=False, engine='openpyxl')

                if os.path.exists(excel_file_path):
                    os.remove(excel_file_path)
                os.rename(temp_file, excel_file_path)

                if is_final:
                    logger.info(f"[ä¿å­˜] å·²å†™å…¥ {len(target_indices)} æ¡è®°å½•åˆ°Excelï¼ˆæœ€ç»ˆä¿å­˜ï¼‰")
                else:
                    logger.debug(f"[ä¿å­˜] å¢é‡å†™å…¥ {len(target_indices)} æ¡è®°å½•åˆ°Excel")

                if is_final:
                    self.results_buffer.clear()
                    self._pending_indices.clear()
                    self._final_only_indices.clear()
                else:
                    for index in target_indices:
                        self._pending_indices.discard(index)
                        self.results_buffer.pop(index, None)

            except Exception as e:
                logger.error(f"ä¿å­˜Excelå¤±è´¥: {e}")
                if 'temp_file' in locals() and os.path.exists(temp_file):
                    os.remove(temp_file)
    async def _save_batch_results(self, df: pd.DataFrame, excel_file_path: str, processed_count: int, is_final: bool = False):
        """æ‰¹é‡ä¿å­˜æ‰€æœ‰ç»“æœåˆ°Excelï¼ˆå¹¶å‘ä¿®å¤ç‰ˆï¼‰"""
        async with self.write_lock:
            try:
                # åŸå­æ€§ä¿å­˜ï¼šå…ˆå†™ä¸´æ—¶æ–‡ä»¶ï¼Œå†é‡å‘½å
                temp_file = excel_file_path.replace('.xlsx', f'_tmp_{int(time.time())}.xlsx')
                df.to_excel(temp_file, index=False, engine='openpyxl')

                # é‡å‘½åä¸ºæœ€ç»ˆæ–‡ä»¶
                if os.path.exists(excel_file_path):
                    os.remove(excel_file_path)
                os.rename(temp_file, excel_file_path)

                if is_final:
                    logger.info(f"[OK] å·²ä¿å­˜å…¨éƒ¨ {processed_count} æ¡è®°å½•åˆ°Excel")
                else:
                    logger.debug(f"å·²ä¿å­˜ {processed_count} æ¡è®°å½•")

            except Exception as e:
                logger.error(f"ä¿å­˜Excelå¤±è´¥: {e}")
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                temp_file = excel_file_path.replace('.xlsx', f'_tmp_{int(time.time())}.xlsx')
                if os.path.exists(temp_file):
                    os.remove(temp_file)

    async def _save_single_result(self, df: pd.DataFrame, excel_file_path: str, index: int, result: str):
        """ä¿å­˜å•æ¡è®°å½•åˆ°Excelï¼ˆå¤„ç†ä¸€æ¡ç«‹å³ä¿å­˜ï¼‰"""
        async with self.write_lock:
            try:
                column = self.output_column or 'ISBNå·'
                if column not in df.columns:
                    df[column] = ""

                df.at[index, column] = result

                # åŸå­æ€§ä¿å­˜ï¼šå…ˆå†™ä¸´æ—¶æ–‡ä»¶ï¼Œå†é‡å‘½å
                temp_file = excel_file_path.replace('.xlsx', f'_tmp_{int(time.time())}.xlsx')
                df.to_excel(temp_file, index=False, engine='openpyxl')

                # é‡å‘½åä¸ºæœ€ç»ˆæ–‡ä»¶
                if os.path.exists(excel_file_path):
                    os.remove(excel_file_path)
                os.rename(temp_file, excel_file_path)

                logger.info(f"âœ“ å·²ä¿å­˜ç¬¬ {index + 1} æ¡è®°å½•: {result}")

            except Exception as e:
                logger.error(f"ä¿å­˜å•æ¡è®°å½•å¤±è´¥: {e}")
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if 'temp_file' in locals() and os.path.exists(temp_file):
                    os.remove(temp_file)

    async def _save_to_excel(self, df: pd.DataFrame, excel_file_path: str, processed_count: int):
        """ä¿å­˜åˆ°Excelæ–‡ä»¶ï¼ˆä¿ç•™ä»¥å…¼å®¹æ—§ç‰ˆæœ¬ï¼‰"""
        async with self.write_lock:
            try:
                # åŸå­æ€§ä¿å­˜ï¼šå…ˆå†™ä¸´æ—¶æ–‡ä»¶ï¼Œå†é‡å‘½å
                temp_file = excel_file_path + '.tmp'
                df.to_excel(temp_file, index=False)

                # é‡å‘½åä¸ºæœ€ç»ˆæ–‡ä»¶
                if os.path.exists(excel_file_path):
                    os.remove(excel_file_path)
                os.rename(temp_file, excel_file_path)

                logger.debug(f"å·²å®‰å…¨ä¿å­˜ç¬¬ {processed_count} æ¡è®°å½•åˆ°Excel")

            except Exception as e:
                logger.warning(f"ä¿å­˜Excelå¤±è´¥: {e}")
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                temp_file = excel_file_path + '.tmp'
                if os.path.exists(temp_file):
                    os.remove(temp_file)

    def _can_partial_flush(self, value: Optional[str]) -> bool:
        """Return True when a value can be flushed beforeæœ€ç»ˆä¿å­˜."""
        if value is None:
            return False
        text = str(value).strip()
        if not text:
            return False
        return not text.startswith(("è·³è¿‡", "è·å–å¤±è´¥", "é”™è¯¯"))

    def _stage_result(self, index: int, value: str):
        """Record a result and decide whether it can be flushed incrementally."""
        self.results_buffer[index] = value
        if self._can_partial_flush(value):
            self._final_only_indices.discard(index)
            self._pending_indices.add(index)
        else:
            self._pending_indices.discard(index)
            self._final_only_indices.add(index)

    async def _maybe_flush_results(self, df: pd.DataFrame, excel_file_path: str, *, force: bool = False):
        """Flush buffered results when thresholds are met."""
        if not self.results_buffer:
            return

        if force:
            await self._save_results_to_excel(df, excel_file_path, len(self.results_buffer), is_final=True)
            self._last_flush_ts = time.monotonic()
            return

        should_flush = False
        if self.save_interval > 0 and len(self._pending_indices) >= self.save_interval:
            should_flush = True
        elif self.save_interval <= 0 and self._pending_indices:
            # åœ¨ä»…æœ€ç»ˆä¿å­˜æ¨¡å¼ä¸‹ä¾èµ–æ—¶é—´è§¦å‘
            should_flush = False

        if not should_flush and self.save_interval_seconds > 0 and self._pending_indices:
            if (time.monotonic() - self._last_flush_ts) >= self.save_interval_seconds:
                should_flush = True

        if should_flush and self._pending_indices:
            await self._save_results_to_excel(df, excel_file_path, len(self._pending_indices), is_final=False)
            self._last_flush_ts = time.monotonic()

    # ============================================================================
    # æ•°æ®åº“ç›¸å…³æ–¹æ³•
    # ============================================================================

    async def _process_existing_valid_books(self, valid_data: List[Dict], df: pd.DataFrame,
                                           excel_file_path: str, output_column: str):
        """å¤„ç†å·²æœ‰æœ‰æ•ˆæ•°æ®ï¼ˆä»æ•°æ®åº“è·å–ï¼‰"""
        logger.info(f"ä»æ•°æ®åº“åŠ è½½å·²æœ‰æœ‰æ•ˆæ•°æ®: {len(valid_data)} æ¡")
        if not valid_data:
            return

        deduped: Dict[str, Dict] = {}
        for book_data in valid_data:
            barcode = book_data.get('barcode') or book_data.get('ä¹¦ç›®æ¡ç ')
            if not barcode:
                logger.warning(f"è·³è¿‡ç¼ºå°‘barcodeçš„è®°å½•: {book_data}")
                continue
            barcode = str(barcode).strip()
            updated_at = (book_data.get('updated_at') or '').strip()
            existing = deduped.get(barcode)
            if existing:
                prev = (existing.get('updated_at') or '').strip()
                if updated_at and prev and updated_at <= prev:
                    continue
            deduped[barcode] = book_data

        try:
            for barcode, book_data in deduped.items():
                excel_index = self._find_row_index(df, barcode)
                if excel_index == -1:
                    logger.warning(f"åœ¨Excelä¸­æœªæ‰¾åˆ°æ¡ç  {barcode}ï¼Œè·³è¿‡")
                    continue

                isbn_value = (book_data.get('isbn') or book_data.get('ISBN') or '').strip()
                if not isbn_value:
                    continue

                updated_at = (book_data.get('updated_at') or '').strip()
                if output_column in df.columns:
                    existing_cell = df.at[excel_index, output_column]
                    existing_value = '' if pd.isna(existing_cell) else str(existing_cell).strip()
                else:
                    existing_value = ''
                cached_marker = self._db_result_cache.get(barcode)
                if existing_value == isbn_value and cached_marker == updated_at:
                    logger.debug(f"è·³è¿‡é‡å¤å›å¡«: {barcode}")
                    continue

                df.at[excel_index, output_column] = isbn_value
                self._stage_result(excel_index, isbn_value)
                if updated_at:
                    self._db_result_cache[barcode] = updated_at

                self.stats['successful_isbn'] += 1
                self.stats['total_processed'] += 1

            await self._maybe_flush_results(df, excel_file_path, force=True)
            logger.info(f"å·²æœ‰æœ‰æ•ˆæ•°æ®å¤„ç†å®Œæˆ: {len(deduped)} æ¡")

        except Exception as e:
            logger.error(f"å¤„ç†å·²æœ‰æœ‰æ•ˆæ•°æ®å¤±è´¥: {e}")
            raise

    def _is_valid_isbn_value(self, value: Optional[str]) -> bool:
        """Return True if the given Excel cell contains a legal ISBN value."""
        if value is None:
            return False
        if isinstance(value, float) and pd.isna(value):
            return False
        text = str(value).strip()
        if not text or text.lower() == 'nan':
            return False
        if not ISBN_ALLOWED_PATTERN.fullmatch(text):
            return False
        return any(ch.isdigit() for ch in text)

    def _collect_rows_with_valid_isbn(self, df: pd.DataFrame, output_column: str) -> Set[int]:
        """Collect row indices whose ISBN column already contains a valid value."""
        if output_column not in df.columns:
            return set()
        valid_rows: Set[int] = set()
        for idx, cell in df[output_column].items():
            if self._is_valid_isbn_value(cell):
                valid_rows.add(idx)
        return valid_rows

    def _initialize_row_index_cache(self, df: pd.DataFrame, barcode_column: str) -> None:
        """é¢„æ„å»ºæ¡ç åˆ°è¡Œç´¢å¼•çš„æ˜ å°„ï¼Œé¿å…é‡å¤æ‰«æã€‚"""
        if barcode_column not in df.columns:
            self._row_index_cache = {}
            self._row_cache_column = None
            return

        cache: Dict[str, int] = {}
        duplicate_count = 0
        for idx, value in df[barcode_column].items():
            barcode = str(value).strip()
            if not barcode:
                continue
            if barcode in cache:
                duplicate_count += 1
                continue
            cache[barcode] = idx

        self._row_index_cache = cache
        self._row_cache_column = barcode_column
        if duplicate_count:
            logger.warning(f"æ¡ç åˆ— {barcode_column} ä¸­å‘ç° {duplicate_count} ä¸ªé‡å¤æ¡ç ï¼Œä»…ä½¿ç”¨ç¬¬ä¸€æ¬¡å‡ºç°çš„è®°å½•")

    def _find_row_index(self, df: pd.DataFrame, barcode: str) -> int:
        """æ ¹æ®barcodeæŸ¥æ‰¾Excelè¡Œç´¢å¼•"""
        barcode_str = str(barcode).strip()
        if not barcode_str:
            return -1

        if self._row_index_cache and self._row_cache_column and self._row_cache_column in df.columns:
            cached = self._row_index_cache.get(barcode_str)
            if cached is not None:
                return cached

        barcode_column = None
        if self.barcode_column and self.barcode_column in df.columns:
            barcode_column = self.barcode_column
        elif 'barcode' in df.columns:
            barcode_column = 'barcode'
        elif 'ä¹¦ç›®æ¡ç ' in df.columns:
            barcode_column = 'ä¹¦ç›®æ¡ç '
        else:
            logger.error("Excelä¸­æœªæ‰¾åˆ°barcodeåˆ—")
            return -1

        for idx, row in df.iterrows():
            row_barcode = str(row[barcode_column]).strip()
            if row_barcode == barcode_str:
                if self._row_cache_column == barcode_column:
                    self._row_index_cache[barcode_str] = idx
                return idx

        return -1

    async def _batch_save_to_database(self, books_data: List[Dict], borrow_records_list: List[Dict],
                                     statistics_list: List[Dict]):
        """
        æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“

        Args:
            books_data: booksè¡¨æ•°æ®åˆ—è¡¨
            borrow_records_list: borrow_recordsè¡¨æ•°æ®åˆ—è¡¨
            statistics_list: borrow_statisticsè¡¨æ•°æ®åˆ—è¡¨
        """
        if not self.db_manager:
            logger.warning("æ•°æ®åº“ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡ä¿å­˜")
            return

        try:
            logger.info(f"å¼€å§‹æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“: {len(books_data)} æ¡booksæ•°æ®")

            batch_size = self.db_config.get('write_strategy', {}).get('batch_size', 100)
            self.db_manager.batch_save_data(books_data, borrow_records_list, statistics_list, batch_size)

            logger.info("æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“å®Œæˆ")

        except Exception as e:
            logger.error(f"æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            raise

    async def _process_all_books_without_db(self, df: pd.DataFrame, excel_file_path: str,
                                           barcode_column: str, output_column: str, total_records: int):
        """
        é™çº§å¤„ç†ï¼šæ²¡æœ‰æ•°æ®åº“åŠŸèƒ½æ—¶çš„åŸæœ‰é€»è¾‘

        Args:
            df: DataFrameå¯¹è±¡
            excel_file_path: Excelæ–‡ä»¶è·¯å¾„
            barcode_column: æ¡ç åˆ—å
            output_column: è¾“å‡ºåˆ—å
            total_records: æ€»è®°å½•æ•°
        """
        logger.info("é™çº§åˆ°åŸæœ‰å¤„ç†é€»è¾‘ï¼ˆæ— æ•°æ®åº“ï¼‰")

        # è°ƒç”¨åŸæœ‰çš„_process_batchæ–¹æ³•
        await self._process_batch(
            df, excel_file_path, barcode_column, output_column, total_records,
            batch_name="å¤„ç†"
        )

# ============================================================================
# ä¾¿æ·æ¥å£å‡½æ•°
# ============================================================================

def process_isbn_async(excel_file_path: str,
                      max_concurrent: int = 2,
                      save_interval: int = 25,
                      barcode_column: str = "ä¹¦ç›®æ¡ç ",
                      output_column: str = "ISBNå·",
                      username: str = None,
                      password: str = None,
                      retry_failed: bool = True,
                      enable_database: bool = False,
                      db_config: Dict = None,
                      processing_config: Optional[ProcessingConfig] = None) -> Tuple[str, Dict]:
    """
    å¼‚æ­¥å¤„ç†ISBNçš„ä¾¿æ·æ¥å£å‡½æ•°

    Args:
        excel_file_path: Excelæ–‡ä»¶è·¯å¾„
        max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤2ï¼Œå…è®¸å¹¶å‘å¤„ç†ï¼‰
        save_interval: ä¿å­˜é—´éš”ï¼Œ1è¡¨ç¤ºå¤„ç†ä¸€æ¡ä¿å­˜ä¸€æ¬¡ï¼ˆé»˜è®¤ï¼‰
        barcode_column: ä¹¦ç›®æ¡ç åˆ—å
        output_column: è¾“å‡ºISBNåˆ—å
        username: FOLIOç³»ç»Ÿç”¨æˆ·åï¼ˆå¯é€‰ï¼‰
        password: FOLIOç³»ç»Ÿå¯†ç ï¼ˆå¯é€‰ï¼‰
        retry_failed: æ˜¯å¦åœ¨ç¬¬ä¸€è½®å¤„ç†å®Œåé‡è¯•å¤±è´¥çš„æ¡ç ï¼ˆé»˜è®¤Trueï¼‰
        enable_database: æ˜¯å¦å¯ç”¨æ•°æ®åº“åŠŸèƒ½ï¼ˆé»˜è®¤Falseï¼‰
        db_config: æ•°æ®åº“é…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼‰

    Returns:
        Tuple[str, Dict]: (æ–‡ä»¶è·¯å¾„, ç»Ÿè®¡ä¿¡æ¯)
    """
    async def _process():
        # åˆ›å»ºå¤„ç†å™¨
        processor = ISBNAsyncProcessor(
            max_concurrent=max_concurrent,
            save_interval=save_interval,
            enable_database=enable_database,
            db_config=db_config,
            processing_config=processing_config,
        )

        # å¦‚æœæä¾›äº†ç”¨æˆ·åå’Œå¯†ç ï¼Œæ›´æ–°é…ç½®
        if username and password:
            processor.username = username
            processor.password = password

        # æ‰¹é‡å¤„ç†
        output_file, stats = await processor.process_excel_file(
            excel_file_path=excel_file_path,
            barcode_column=barcode_column,
            output_column=output_column,
            retry_failed=retry_failed
        )

        return output_file, stats

    return asyncio.run(_process())


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test_async_processor():
        """æµ‹è¯•å¼‚æ­¥å¤„ç†å™¨"""
        test_excel = "runtime/outputs/æœˆå½’è¿˜æ•°æ®ç­›é€‰ç»“æœ_20251101_183318.xlsx"

        if not os.path.exists(test_excel):
            logger.error(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_excel}")
            return

        try:
            # æ˜¾ç¤ºæ€§èƒ½ä¼°ç®—
            import pandas as pd
            df_test = pd.read_excel(test_excel)
            data_size = len(df_test)
            estimated_time = data_size * 1.5 / 3600  # ä¼°ç®—1.5ç§’/æ¡
            logger.info(f"æ•°æ®é‡: {data_size} æ¡ï¼Œé¢„è®¡å¤„ç†æ—¶é—´: {estimated_time:.1f} å°æ—¶")

            # æ‰§è¡Œå¤„ç†
            output_file, stats = await process_isbn_async(
                excel_file_path=test_excel,
                max_concurrent=3,
                barcode_column="ä¹¦ç›®æ¡ç ",
                output_column="ISBNå·"
            )

            logger.info(f"å¤„ç†å®Œæˆï¼Œç»“æœ: {output_file}")
            logger.info(f"å¤„ç†ç»Ÿè®¡: {stats}")

        except Exception as e:
            logger.error(f"æµ‹è¯•å¤±è´¥: {str(e)}")

    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_async_processor())
