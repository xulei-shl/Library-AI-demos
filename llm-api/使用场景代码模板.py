"""
å¸¸è§ä½¿ç”¨åœºæ™¯ä»£ç æ¨¡æ¿é›†åˆ

ä½¿ç”¨æ–¹æ³•ï¼š
1. é€‰æ‹©ç¬¦åˆæ‚¨éœ€æ±‚çš„åœºæ™¯
2. å¤åˆ¶å¯¹åº”ä»£ç æ¨¡æ¿
3. ä¿®æ”¹é…ç½®å‚æ•°
4. è¿è¡Œæµ‹è¯•
"""

import asyncio
import json
from typing import List, Dict, Any
from pathlib import Path

# ============================================================================
# åœºæ™¯1: æ–‡æœ¬å¤„ç† - æ‰¹é‡æ ¡å¯¹
# ============================================================================

async def batch_text_correction(texts: List[str]):
    """æ‰¹é‡æ–‡æœ¬æ ¡å¯¹åœºæ™¯

    é€‚ç”¨åœºæ™¯ï¼š
    - æ‰¹é‡æ ¡å¯¹ç”¨æˆ·è¾“å…¥
    - å†…å®¹è´¨é‡æ£€æŸ¥
    - æ•°æ®æ¸…æ´—
    """
    import sys
    sys.path.append("llm_api")

    from core.llm_client import UnifiedLLMClient

    client = UnifiedLLMClient("llm_api/config/settings.yaml")

    try:
        # å‡†å¤‡è¯·æ±‚
        requests = [
            {"task_name": "correction", "user_prompt": f"è¯·æ ¡å¯¹ä»¥ä¸‹æ–‡æœ¬ï¼š{text}"}
            for text in texts
        ]

        # æ‰¹é‡è°ƒç”¨
        print(f"ğŸ“ å¼€å§‹æ‰¹é‡æ ¡å¯¹ {len(texts)} ä¸ªæ–‡æœ¬...")
        results = await client.batch_call(requests, max_concurrent=5)

        # å¤„ç†ç»“æœ
        corrected_texts = []
        for i, (original, corrected) in enumerate(zip(texts, results)):
            print(f"\næ–‡æœ¬ {i+1}:")
            print(f"  åŸæ–‡: {original}")
            print(f"  æ ¡å¯¹å: {corrected}")
            corrected_texts.append(corrected)

        return corrected_texts

    finally:
        await client.close()


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    sample_texts = [
        "è¿™æ˜¯ä¸€ä¸ªåŒ…å«é”™åˆ«å­—çš„æ–‡æœ¬ã€‚",
        "å¦ä¸€ä¸ªéœ€è¦æ ¡å¯¹çš„æ–‡æœ¬ç¤ºä¾‹ã€‚",
        "ç¬¬ä¸‰æ®µéœ€è¦å¤„ç†çš„æ–‡æœ¬ã€‚"
    ]
    asyncio.run(batch_text_correction(sample_texts))


# ============================================================================
# åœºæ™¯2: å›¾ç‰‡åˆ†æ - æ‰¹é‡å¤„ç†
# ============================================================================

async def batch_image_analysis(image_descriptions: List[str]):
    """æ‰¹é‡å›¾ç‰‡åˆ†æåœºæ™¯

    é€‚ç”¨åœºæ™¯ï¼š
    - æ‰¹é‡å›¾ç‰‡å†…å®¹å®¡æ ¸
    - å›¾ç‰‡æ ‡ç­¾ç”Ÿæˆ
    - è§†è§‰æ•°æ®åˆ†æ
    """
    import sys
    sys.path.append("llm_api")

    from core.llm_client import UnifiedLLMClient

    client = UnifiedLLMClient("llm_api/config/settings.yaml")

    try:
        requests = [
            {"task_name": "fact_description", "user_prompt": img}
            for img in image_descriptions
        ]

        print(f"ğŸ–¼ï¸ å¼€å§‹æ‰¹é‡åˆ†æ {len(image_descriptions)} å¼ å›¾ç‰‡...")
        results = await client.batch_call(requests, max_concurrent=3)

        # è§£æJSONç»“æœï¼ˆå¦‚æœè¿”å›JSONæ ¼å¼ï¼‰
        parsed_results = []
        for i, result in enumerate(results):
            print(f"\nå›¾ç‰‡ {i+1} åˆ†æç»“æœ:")
            try:
                # å°è¯•è§£æJSON
                data = json.loads(result)
                print(f"  æè¿°: {data.get('description', '')}")
                print(f"  æ–‡å­—å†…å®¹: {data.get('text_content', '')}")
                parsed_results.append(data)
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯JSONï¼Œç›´æ¥æ‰“å°
                print(f"  {result}")
                parsed_results.append({"raw_result": result})

        return parsed_results

    finally:
        await client.close()


# ============================================================================
# åœºæ™¯3: æµå¼å¯¹è¯ - å®æ—¶å“åº”
# ============================================================================

async def streaming_chat():
    """æµå¼å¯¹è¯åœºæ™¯

    é€‚ç”¨åœºæ™¯ï¼š
    - èŠå¤©æœºå™¨äºº
    - å®æ—¶é—®ç­”ç³»ç»Ÿ
    - åœ¨çº¿åŠ©æ‰‹
    """
    import sys
    sys.path.append("llm_api")

    from core.llm_client import UnifiedLLMClient

    client = UnifiedLLMClient("llm_api/config/settings.yaml")

    try:
        question = "è¯·è¯¦ç»†è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "

        print(f"ğŸ¤” é—®é¢˜: {question}")
        print("ğŸ’¬ å›ç­”: ", end="", flush=True)

        # æµå¼è¾“å‡º
        async for chunk in client.stream_call(
            task_name="correction",  # æˆ–è‡ªå®šä¹‰å¯¹è¯ä»»åŠ¡
            user_prompt=question
        ):
            print(chunk, end="", flush=True)

        print()  # æ¢è¡Œ

    finally:
        await client.close()


# ============================================================================
# åœºæ™¯4: å¸¦ç¼“å­˜çš„è°ƒç”¨ - é¿å…é‡å¤è¯·æ±‚
# ============================================================================

class CachedLLMClient:
    """å¸¦ç¼“å­˜çš„LLMå®¢æˆ·ç«¯

    é¿å…é‡å¤è¯·æ±‚ï¼Œæå‡æ€§èƒ½
    """

    def __init__(self, config_path: str):
        import sys
        sys.path.append("llm_api")

        from core.llm_client import UnifiedLLMClient

        self.client = UnifiedLLMClient(config_path)
        self.cache = {}  # ç®€å•çš„å†…å­˜ç¼“å­˜

    async def call_with_cache(self, task_name: str, user_prompt: str, **kwargs):
        """å¸¦ç¼“å­˜çš„è°ƒç”¨"""
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"{task_name}:{user_prompt}:{json.dumps(kwargs, sort_keys=True)}"

        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.cache:
            print("ğŸ“¦ ä½¿ç”¨ç¼“å­˜ç»“æœ")
            return self.cache[cache_key]

        # è°ƒç”¨API
        result = await self.client.call(task_name, user_prompt, **kwargs)

        # å­˜å…¥ç¼“å­˜
        self.cache[cache_key] = result

        return result

    async def close(self):
        await self.client.close()


async def cached_example():
    """ä½¿ç”¨ç¼“å­˜çš„ç¤ºä¾‹"""
    client = CachedLLMClient("llm_api/config/settings.yaml")

    try:
        # ç¬¬ä¸€æ¬¡è°ƒç”¨
        print("ç¬¬ä¸€æ¬¡è°ƒç”¨:")
        result1 = await client.call_with_cache(
            "correction",
            "æµ‹è¯•ç¼“å­˜åŠŸèƒ½"
        )
        print(result1)

        # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆå‘½ä¸­ç¼“å­˜ï¼‰
        print("\nç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆåº”è¯¥å‘½ä¸­ç¼“å­˜ï¼‰:")
        result2 = await client.call_with_cache(
            "correction",
            "æµ‹è¯•ç¼“å­˜åŠŸèƒ½"
        )
        print(result2)

    finally:
        await client.close()


# ============================================================================
# åœºæ™¯5: å¸¦é‡è¯•çš„è°ƒç”¨ - æé«˜ç¨³å®šæ€§
# ============================================================================

async def robust_call(task_name: str, user_prompt: str, max_attempts: int = 5):
    """å¸¦é‡è¯•çš„ç¨³å®šè°ƒç”¨

    é€‚ç”¨åœºæ™¯ï¼š
    - ç”Ÿäº§ç¯å¢ƒ
    - ç½‘ç»œä¸ç¨³å®šç¯å¢ƒ
    - é«˜å¯ç”¨æ€§è¦æ±‚
    """
    import sys
    sys.path.append("llm_api")

    from core.llm_client import UnifiedLLMClient
    from core.exceptions import LLMCallError, RateLimitError, NetworkError

    client = UnifiedLLMClient("llm_api/config/settings.yaml")

    try:
        for attempt in range(max_attempts):
            try:
                print(f"å°è¯• {attempt + 1}/{max_attempts}...")
                result = await client.call(task_name, user_prompt)
                print("âœ… è°ƒç”¨æˆåŠŸ")
                return result

            except RateLimitError as e:
                print(f"âš ï¸ é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…åé‡è¯•... ({e})")
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿

            except NetworkError as e:
                print(f"ğŸŒ ç½‘ç»œé”™è¯¯ï¼Œé‡è¯•ä¸­... ({e})")
                await asyncio.sleep(1)

            except LLMCallError as e:
                print(f"âŒ è°ƒç”¨å¤±è´¥: {e}")
                if attempt == max_attempts - 1:
                    raise  # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸

    finally:
        await client.close()


# ============================================================================
# åœºæ™¯6: å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç† - è‡ªåŠ¨èµ„æºæ¸…ç†
# ============================================================================

class LLMContextManager:
    """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨

    ä½¿ç”¨æ–¹å¼ï¼š
    async with LLMContextManager("config.yaml") as llm:
        result = await llm.call(...)
    """

    def __init__(self, config_path: str):
        self.config_path = config_path
        self.client = None

    async def __aenter__(self):
        import sys
        sys.path.append("llm_api")

        from core.llm_client import UnifiedLLMClient

        self.client = UnifiedLLMClient(self.config_path)
        return self.client

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.client:
            await self.client.close()


# ä½¿ç”¨ç¤ºä¾‹
async def context_manager_example():
    """ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„ç¤ºä¾‹"""
    async with LLMContextManager("llm_api/config/settings.yaml") as client:
        result = await client.call(
            task_name="correction",
            user_prompt="ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„ç¤ºä¾‹"
        )
        print(result)


# ============================================================================
# åœºæ™¯7: ç›‘æ§å’Œæ—¥å¿— - ç”Ÿäº§ç¯å¢ƒç›‘æ§
# ============================================================================

import logging
from utils.logger import get_logger


async def monitored_llm_call(task_name: str, user_prompt: str):
    """å¸¦ç›‘æ§çš„LLMè°ƒç”¨

    é€‚ç”¨åœºæ™¯ï¼š
    - ç”Ÿäº§ç¯å¢ƒ
    - éœ€è¦è¯¦ç»†æ—¥å¿—çš„åœºæ™¯
    - é—®é¢˜è¿½è¸ªå’Œè°ƒè¯•
    """
    import sys
    sys.path.append("llm_api")

    from core.llm_client import UnifiedLLMClient
    from utils.logger import log_function_call

    logger = get_logger(__name__)

    @log_function_call
    async def _call():
        client = UnifiedLLMClient("llm_api/config/settings.yaml")
        try:
            result = await client.call(task_name, user_prompt)
            logger.info(f"LLMè°ƒç”¨æˆåŠŸï¼Œä»»åŠ¡: {task_name}")
            return result
        finally:
            await client.close()

    return await _call()


# ============================================================================
# åœºæ™¯8: å¤šä»»åŠ¡å¹¶å‘å¤„ç† - å¤æ‚å·¥ä½œæµ
# ============================================================================

async def process_multiple_tasks(requests: List[Dict[str, Any]]):
    """å¤šä»»åŠ¡å¹¶å‘å¤„ç†

    é€‚ç”¨åœºæ™¯ï¼š
    - å¤æ‚ä¸šåŠ¡æµç¨‹
    - éœ€è¦åŒæ—¶å¤„ç†å¤šç§ç±»å‹çš„ä»»åŠ¡
    - å·¥ä½œæµç¼–æ’
    """
    import sys
    sys.path.append("llm_api")

    from core.llm_client import UnifiedLLMClient

    client = UnifiedLLMClient("llm_api/config/settings.yaml")

    try:
        # æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„
        text_requests = [r for r in requests if r.get("type") == "text"]
        image_requests = [r for r in requests if r.get("type") == "image"]

        # å¹¶å‘å¤„ç†
        tasks = []

        if text_requests:
            tasks.append(
                client.batch_call([
                    {"task_name": "correction", "user_prompt": r["prompt"]}
                    for r in text_requests
                ], max_concurrent=5)
            )

        if image_requests:
            tasks.append(
                client.batch_call([
                    {"task_name": "fact_description", "user_prompt": r["prompt"]}
                    for r in image_requests
                ], max_concurrent=3)
            )

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        return results

    finally:
        await client.close()


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é€‰æ‹©è¦è¿è¡Œçš„ç¤ºä¾‹
    print("è¯·é€‰æ‹©è¦è¿è¡Œçš„ç¤ºä¾‹ï¼š")
    print("1. æ‰¹é‡æ–‡æœ¬æ ¡å¯¹")
    print("2. æ‰¹é‡å›¾ç‰‡åˆ†æ")
    print("3. æµå¼å¯¹è¯")
    print("4. ç¼“å­˜è°ƒç”¨")
    print("5. ç¨³å¥è°ƒç”¨")
    print("6. ä¸Šä¸‹æ–‡ç®¡ç†å™¨")
    print("7. ç›‘æ§è°ƒç”¨")
    print("8. å¤šä»»åŠ¡å¹¶å‘")

    choice = input("è¯·è¾“å…¥é€‰é¡¹ (1-8): ").strip()

    if choice == "1":
        sample_texts = ["æµ‹è¯•æ–‡æœ¬1", "æµ‹è¯•æ–‡æœ¬2", "æµ‹è¯•æ–‡æœ¬3"]
        asyncio.run(batch_text_correction(sample_texts))
    elif choice == "2":
        sample_images = ["å›¾ç‰‡1æè¿°", "å›¾ç‰‡2æè¿°", "å›¾ç‰‡3æè¿°"]
        asyncio.run(batch_image_analysis(sample_images))
    elif choice == "3":
        asyncio.run(streaming_chat())
    elif choice == "4":
        asyncio.run(cached_example())
    elif choice == "5":
        asyncio.run(robust_call("correction", "æµ‹è¯•ç¨³å¥è°ƒç”¨"))
    elif choice == "6":
        asyncio.run(context_manager_example())
    elif choice == "7":
        asyncio.run(monitored_llm_call("correction", "æµ‹è¯•ç›‘æ§è°ƒç”¨"))
    elif choice == "8":
        requests = [
            {"type": "text", "prompt": "æ–‡æœ¬1"},
            {"type": "text", "prompt": "æ–‡æœ¬2"},
            {"type": "image", "prompt": "å›¾ç‰‡1"},
        ]
        asyncio.run(process_multiple_tasks(requests))
    else:
        print("æ— æ•ˆé€‰é¡¹")
