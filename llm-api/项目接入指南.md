# 统一LLM调用系统 - 项目接入指南

## 📋 目录

- [快速开始](#快速开始)
- [集成步骤](#集成步骤)
- [配置说明](#配置说明)
- [使用方法](#使用方法)
- [常用场景](#常用场景)
- [最佳实践](#最佳实践)
- [故障排除](#故障排除)

---

## 🚀 快速开始

### 1分钟集成

```python
import asyncio
from core.llm_client import UnifiedLLMClient

async def main():
    # 创建客户端（使用默认配置 config/settings.yaml）
    client = UnifiedLLMClient()

    # 直接调用
    result = await client.call(
        task_name="fact_description",
        user_prompt="请描述这张图片的内容"
    )

    print(result)
    await client.close()

asyncio.run(main())
```

---

## 📦 集成步骤

### 步骤 1: 复制模块到您的项目

将整个 `llm-api` 模块复制到您的项目中：

```bash
# 推荐目录结构
your_project/
├── llm_api/          # 复制整个 llm-api 目录
│   ├── config/
│   ├── core/
│   ├── utils/
│   ├── examples/
│   └── prompts/
├── your_app.py       # 您的应用代码
└── .env             # 环境变量
```

### 步骤 2: 安装依赖

```bash
pip install openai PyYAML

# 可选依赖
pip install langfuse  # 监控功能
```

### 步骤 3: 配置环境变量

在项目根目录创建 `.env` 文件：

```bash
# 至少配置一个文本提供商
DEEPSEEK_API_KEY=your_api_key_here

# 可选：配置多个提供商作为备用
GROQ_API_KEY=your_groq_key
MODELSCOPE_API_KEY=your_modelscope_key
ONEAPI_API_KEY=your_oneapi_key

# 可选：监控服务
LANGFUSE_SECRET_KEY=your_langfuse_secret
LANGFUSE_PUBLIC_KEY=your_langfuse_public
LANGFUSE_HOST=your_langfuse_host
```

### 步骤 4: 修改配置文件

编辑 `config/settings.yaml`，替换为您的API配置：

```yaml
api_providers:
  text:
    primary:
      name: "您的文本提供商"
      api_key: env:DEEPSEEK_API_KEY
      base_url: "https://api.deepseek.com/v1"
      model: "deepseek-chat"
      timeout_seconds: 120
    # 可以添加备用提供商
    secondary:
      name: "备用提供商"
      api_key: env:GROQ_API_KEY
      base_url: "https://api.groq.com/openai/v1"
      model: "mixtral-8x7b-32768"
      timeout_seconds: 120
```

### 步骤 5: 在代码中使用

```python
from llm_api.core.llm_client import UnifiedLLMClient

# 方式1: 直接使用
client = UnifiedLLMClient("llm_api/config/settings.yaml")
result = await client.call(task_name="fact_description", user_prompt="...")
```

---

## ⚙️ 配置说明

### 完整配置示例

参考 `config/settings.yaml:1-100`，配置包含以下部分：

#### 1. API提供商配置

```yaml
api_providers:
  text:           # 文本提供商
    primary:       # 主提供商（优先使用）
      name: "提供商名称"
      api_key: env:YOUR_API_KEY  # 从环境变量读取
      base_url: "https://api.example.com/v1"
      model: "your-model"
      timeout_seconds: 120
    secondary:     # 备用提供商（主失败时自动切换）

  vision:          # 视觉提供商（可选）
    primary:
      # 视觉模型配置
```

#### 2. 任务配置

```yaml
tasks:
  my_task:         # 任务名称（自定义）
    provider_type: "text"  # text 或 vision
    temperature: 0.7       # 创造性控制（0-1）
    top_p: 0.9            # 采样控制
    max_tokens: 2048      # 最大输出长度

    prompt:
      type: "md"          # 提示词格式
      source: "prompts/my_task.md"  # .md文件路径

    retry:                # 重试策略
      max_retries: 3
      base_delay: 1
      max_delay: 60
      enable_provider_switch: true

    langfuse:             # 监控配置（可选）
      enabled: false

    json_repair:          # JSON处理（可选）
      enabled: true
      strict_mode: false
```

#### 3. 提示词格式

**方式1: .md 文件**（推荐）

创建 `prompts/my_task.md`：
```markdown
# 任务名称

你是一个专业的助手...

## 要求
1. 要求1
2. 要求2

## 输出格式
请返回JSON格式：
```json
{
  "result": "..."
}
```
```

配置：
```yaml
prompt:
  type: "md"
  source: "prompts/my_task.md"
```

**方式2: Python字典**

```yaml
prompt:
  type: "dict"
  content:
    role: "system"
    content: |
      你是一个专业的助手...
```

**方式3: Langfuse平台**

```yaml
prompt:
  type: "langfuse"
  langfuse_name: "my_prompt_name"
```

### 配置优先级

1. 方法调用时的 kwargs 参数（最高优先级）
2. 任务配置（tasks节）
3. 全局默认配置（defaults节）
4. 硬编码默认值（最低优先级）

---

## 💻 使用方法

### 基础调用

```python
import asyncio
from core.llm_client import UnifiedLLMClient

async def example():
    client = UnifiedLLMClient()

    # 1. 简单调用
    result = await client.call(
        task_name="correction",
        user_prompt="请校对以下文本：这是一个测试。"
    )
    print(result)

    # 2. 带参数覆盖
    result = await client.call(
        task_name="correction",
        user_prompt="请校对...",
        temperature=0.3,  # 覆盖配置的temperature
        max_tokens=1000
    )

    await client.close()

asyncio.run(example())
```

### 流式调用

```python
async def streaming_example():
    client = UnifiedLLMClient()

    # 流式输出（实时显示）
    print("响应: ", end="", flush=True)
    async for chunk in client.stream_call(
        task_name="correction",
        user_prompt="写一首诗"
    ):
        print(chunk, end="", flush=True)
    print()  # 换行

    await client.close()

asyncio.run(streaming_example())
```

### 批量调用

```python
async def batch_example():
    client = UnifiedLLMClient()

    requests = [
        {"task_name": "correction", "user_prompt": "文本1"},
        {"task_name": "correction", "user_prompt": "文本2"},
        {"task_name": "correction", "user_prompt": "文本3"},
    ]

    # 并发处理，限制最大并发数
    results = await client.batch_call(requests, max_concurrent=3)

    for i, result in enumerate(results):
        print(f"请求 {i+1}: {result}")

    await client.close()

asyncio.run(batch_example())
```

### 自定义任务

#### 1. 添加任务配置

在 `config/settings.yaml` 中添加：

```yaml
tasks:
  my_custom_task:
    provider_type: "text"
    temperature: 0.5
    prompt:
      type: "md"
      source: "prompts/my_custom_task.md"

    retry:
      max_retries: 3
      enable_provider_switch: true
```

#### 2. 创建提示词文件

创建 `prompts/my_custom_task.md`：
```markdown
# 自定义任务

请按照以下要求处理用户输入...

## 规则
1. 规则1
2. 规则2

## 输出
请返回简洁的回答。
```

#### 3. 使用自定义任务

```python
result = await client.call(
    task_name="my_custom_task",
    user_prompt="你的用户输入"
)
```

### 图片分析任务

```python
result = await client.call(
    task_name="fact_description",
    user_prompt="请详细描述这张图片的内容"
)
```

`fact_description` 任务支持返回JSON格式的结构化数据：

```json
{
  "description": "图像的详细事实描述",
  "text_content": "图像中的文字内容",
  "key_objects": [
    {
      "name": "对象名称",
      "position": "位置描述",
      "特征": "主要特征描述"
    }
  ],
  "scene": "整体场景描述"
}
```

### 辅助方法

```python
client = UnifiedLLMClient()

# 获取所有任务列表
tasks = client.list_tasks()
print(f"可用任务: {tasks}")

# 获取任务配置
config = client.get_task_config("correction")
print(f"任务配置: {config}")

# 重新加载配置（热更新）
client.reload_config()

# 关闭客户端
await client.close()
```

---

## 🎯 常用场景

### 场景1: 文本校对

```python
import asyncio
from core.llm_client import UnifiedLLMClient

async def text_correction():
    client = UnifiedLLMClient()

    texts = [
        "这里有错别字，需要校对。",
        "另一个需要校对的文本。",
        "还有一段文本。"
    ]

    for text in texts:
        result = await client.call(
            task_name="correction",
            user_prompt=f"请校对以下文本：{text}"
        )
        print(f"原文: {text}")
        print(f"校对后: {result}\n")

    await client.close()

asyncio.run(text_correction())
```

### 场景2: 批量图片分析

```python
async def batch_image_analysis():
    client = UnifiedLLMClient()

    images = ["图片1描述", "图片2描述", "图片3描述"]

    requests = [
        {"task_name": "fact_description", "user_prompt": img}
        for img in images
    ]

    results = await client.batch_call(requests, max_concurrent=5)

    for i, result in enumerate(results):
        print(f"图片 {i+1} 分析结果:")
        print(result)
        print("-" * 50)

    await client.close()

asyncio.run(batch_image_analysis())
```

### 场景3: 带监控的调用

```python
# 1. 配置 Langfuse
# 在 config/settings.yaml 中启用
langfuse:
  enabled: true

# 2. 使用监控功能
result = await client.call(
    task_name="correction",
    user_prompt="测试文本",
    # 可选：添加额外元数据
    metadata={"user_id": 123, "source": "web"}
)
```

### 场景4: JSON响应处理

```python
# 1. 在任务配置中启用 JSON 修复
tasks:
  my_task:
    json_repair:
      enabled: true
      strict_mode: false  # false: 修复失败返回原文，true: 返回None

# 2. 调用并处理 JSON
result = await client.call(
    task_name="my_task",
    user_prompt="请返回JSON格式的数据"
)

# 自动处理：
# - 移除 ```json ``` 标记
# - 修复常见语法错误
# - 提取JSON片段
print(result)  # 直接使用修复后的JSON字符串
```

---

## ✅ 最佳实践

### 1. 错误处理

```python
from core.exceptions import (
    LLMCallError,
    ConfigurationError,
    ProviderError,
    RateLimitError
)

try:
    result = await client.call(
        task_name="correction",
        user_prompt="测试"
    )
except LLMCallError as e:
    # 所有重试都失败
    print(f"调用失败: {e}")
    print(f"错误历史: {e.error_history}")
except ConfigurationError as e:
    # 配置文件错误
    print(f"配置错误: {e}")
except RateLimitError as e:
    # 速率限制
    print(f"速率限制，请稍后重试: {e}")
except Exception as e:
    # 其他错误
    print(f"未知错误: {e}")
```

### 2. 资源管理

```python
# 推荐：使用 async with 或手动关闭
client = UnifiedLLMClient()

try:
    result = await client.call(...)
    # 处理结果
finally:
    await client.close()  # 务必关闭

# 或者使用上下文管理器（如果已实现）
# async with UnifiedLLMClient() as client:
#     result = await client.call(...)
```

### 3. 配置管理

```python
# 1. 环境变量优先原则
# .env 文件
API_KEY=your_key

# config/settings.yaml
api_key: env:API_KEY  # 使用环境变量

# 2. 多环境配置
# 开发环境
dev_config = "config/settings.dev.yaml"
client_dev = UnifiedLLMClient(dev_config)

# 生产环境
prod_config = "config/settings.prod.yaml"
client_prod = UnifiedLLMClient(prod_config)
```

### 4. 性能优化

```python
# 1. 批量调用减少开销
requests = [...]  # 准备多个请求
results = await client.batch_call(requests, max_concurrent=10)

# 2. 合理设置并发数
# 根据API提供商限制调整 max_concurrent
results = await client.batch_call(requests, max_concurrent=5)

# 3. 流式调用提升用户体验
async for chunk in client.stream_call(...):
    print(chunk, end="", flush=True)

# 4. 复用客户端实例
# 错误示例：每次调用都创建新客户端
# for text in texts:
#     client = UnifiedLLMClient()  # ❌ 不要这样做
#     await client.call(...)

# 正确示例：复用客户端
client = UnifiedLLMClient()  # ✅ 创建一次
for text in texts:
    await client.call(...)
await client.close()
```

### 5. 日志记录

```python
from utils.logger import get_logger, log_function_call

logger = get_logger(__name__)

@log_function_call
async def my_llm_function(user_input):
    # 自动记录：函数调用、参数、返回值、耗时、异常
    client = UnifiedLLMClient()
    try:
        result = await client.call(
            task_name="correction",
            user_prompt=user_input
        )
        logger.info(f"处理成功，输入长度: {len(user_input)}")
        return result
    finally:
        await client.close()

# 手动记录
logger.info("这是一条中文日志")
logger.warning("警告信息")
logger.error("错误信息")
```

### 6. 速率限制

```yaml
# 在配置中设置
rate_limit:
  global:
    requests_per_minute: 1000  # 全局限流
    concurrent_requests: 10    # 全局并发限制

# 在任务中设置
tasks:
  my_task:
    rate_limit:
      requests_per_minute: 60  # 任务限流
      burst_size: 10           # 突发量
```

---

## 🔧 故障排除

### 常见问题

#### 1. 配置文件错误

**错误信息：**
```
FileNotFoundError: 配置文件不存在: config/settings.yaml
```

**解决方案：**
```python
# 确保使用正确路径
client = UnifiedLLMClient("your_project/llm_api/config/settings.yaml")
```

#### 2. API密钥缺失

**错误信息：**
```
警告: missing_env_key key=DEEPSEEK_API_KEY
```

**解决方案：**
```bash
# 1. 检查 .env 文件是否存在
# 2. 检查环境变量名是否正确
echo $DEEPSEEK_API_KEY

# 3. 在 .env 中配置
DEEPSEEK_API_KEY=sk-your-actual-key
```

#### 3. 提示词文件不存在

**错误信息：**
```
PromptError: 文件不存在: prompts/my_task.md
```

**解决方案：**
```python
# 1. 检查文件路径是否正确
# 2. 确保文件存在于指定位置
# 3. 使用绝对路径测试

prompt:
  type: "md"
  source: "/full/path/to/prompts/my_task.md"
```

#### 4. Langfuse未安装

**错误信息：**
```
Langfuse未安装
```

**解决方案：**
```bash
# 安装 Langfuse
pip install langfuse

# 或在配置中禁用
tasks:
  my_task:
    langfuse:
      enabled: false
```

#### 5. 网络连接错误

**错误信息：**
```
NetworkError: 连接超时
```

**解决方案：**
```yaml
# 1. 增加超时时间
api_providers:
  text:
    primary:
      timeout_seconds: 180  # 增加到180秒

# 2. 检查网络和代理设置
# 3. 检查 base_url 是否正确
```

#### 6. 模型不支持

**错误信息：**
```
ModelError: 模型不存在或无权限访问
```

**解决方案：**
```yaml
# 1. 确认模型名称正确
api_providers:
  text:
    primary:
      model: "your-actual-model-name"

# 2. 检查API密钥权限
# 3. 确认模型名称拼写
```

#### 7. 速率限制

**错误信息：**
```
RateLimitError: 请求过于频繁
```

**解决方案：**
```python
# 1. 使用批量调用
results = await client.batch_call(requests, max_concurrent=1)

# 2. 增加延迟
await asyncio.sleep(1)

# 3. 配置重试
retry:
  max_retries: 5
  base_delay: 2
```

### 调试技巧

#### 1. 启用详细日志

```yaml
# config/settings.yaml
logging:
  levels:
    file: "DEBUG"  # 文件日志使用DEBUG级别
```

#### 2. 查看错误历史

```python
try:
    result = await client.call(...)
except LLMCallError as e:
    print("=== 错误历史 ===")
    for error in e.error_history:
        print(f"Provider: {error['provider']}")
        print(f"错误类型: {error['error_type']}")
        print(f"错误信息: {error['error']}")
```

#### 3. 测试连接

```python
async def test_connection():
    client = UnifiedLLMClient()

    # 测试简单调用
    try:
        result = await client.call(
            task_name="correction",
            user_prompt="hi"
        )
        print("连接成功!")
        print(result)
    except Exception as e:
        print(f"连接失败: {e}")
    finally:
        await client.close()

asyncio.run(test_connection())
```

#### 4. 列出可用任务

```python
client = UnifiedLLMClient()

# 查看所有任务
tasks = client.list_tasks()
print(f"可用任务: {tasks}")

# 查看任务配置
for task in tasks:
    config = client.get_task_config(task)
    print(f"\n{task}:")
    print(f"  提供商类型: {config['provider_type']}")
    print(f"  提示词类型: {config['prompt']['type']}")
```

---

## 📚 更多资源

- **完整文档**: [README.md](./README.md)
- **示例代码**: [examples/](./examples/)
- **配置参考**: [config/settings.yaml](./config/settings.yaml)
- **提示词模板**: [prompts/](./prompts/)

---

## ❓ 常见问题 FAQ

**Q: 支持哪些LLM提供商？**
A: 支持所有 OpenAI 兼容的API，包括 DeepSeek、ChatGLM、Qwen、Mixtral 等。

**Q: 如何添加新的LLM提供商？**
A: 在 `config/settings.yaml` 的 `api_providers` 节添加配置。

**Q: 如何自定义提示词？**
A: 创建 `.md` 文件并在任务配置的 `prompt.source` 中指定。

**Q: 如何启用监控？**
A: 安装 `langfuse` 并在配置中设置 `langfuse.enabled: true`。

**Q: 如何处理大量请求？**
A: 使用 `batch_call()` 方法并合理设置 `max_concurrent` 参数。

**Q: 如何实现多环境配置？**
A: 为不同环境创建不同的配置文件，在初始化时指定路径。

---

**祝您使用愉快！** 🎉

如有问题，请提交 Issue 或查看详细文档。
