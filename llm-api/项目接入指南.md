# ç»Ÿä¸€LLMè°ƒç”¨ç³»ç»Ÿ - é¡¹ç›®æ¥å…¥æŒ‡å—

## ğŸ“‹ ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [é›†æˆæ­¥éª¤](#é›†æˆæ­¥éª¤)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
- [å¸¸ç”¨åœºæ™¯](#å¸¸ç”¨åœºæ™¯)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1åˆ†é’Ÿé›†æˆ

```python
import asyncio
from core.llm_client import UnifiedLLMClient

async def main():
    # åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨é»˜è®¤é…ç½® config/settings.yamlï¼‰
    client = UnifiedLLMClient()

    # ç›´æ¥è°ƒç”¨
    result = await client.call(
        task_name="fact_description",
        user_prompt="è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹"
    )

    print(result)
    await client.close()

asyncio.run(main())
```

---

## ğŸ“¦ é›†æˆæ­¥éª¤

### æ­¥éª¤ 1: å¤åˆ¶æ¨¡å—åˆ°æ‚¨çš„é¡¹ç›®

å°†æ•´ä¸ª `llm-api` æ¨¡å—å¤åˆ¶åˆ°æ‚¨çš„é¡¹ç›®ä¸­ï¼š

```bash
# æ¨èç›®å½•ç»“æ„
your_project/
â”œâ”€â”€ llm_api/          # å¤åˆ¶æ•´ä¸ª llm-api ç›®å½•
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ examples/
â”‚   â””â”€â”€ prompts/
â”œâ”€â”€ your_app.py       # æ‚¨çš„åº”ç”¨ä»£ç 
â””â”€â”€ .env             # ç¯å¢ƒå˜é‡
```

### æ­¥éª¤ 2: å®‰è£…ä¾èµ–

```bash
pip install openai PyYAML

# å¯é€‰ä¾èµ–
pip install langfuse  # ç›‘æ§åŠŸèƒ½
```

### æ­¥éª¤ 3: é…ç½®ç¯å¢ƒå˜é‡

åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` æ–‡ä»¶ï¼š

```bash
# è‡³å°‘é…ç½®ä¸€ä¸ªæ–‡æœ¬æä¾›å•†
DEEPSEEK_API_KEY=your_api_key_here

# å¯é€‰ï¼šé…ç½®å¤šä¸ªæä¾›å•†ä½œä¸ºå¤‡ç”¨
GROQ_API_KEY=your_groq_key
MODELSCOPE_API_KEY=your_modelscope_key
ONEAPI_API_KEY=your_oneapi_key

# å¯é€‰ï¼šç›‘æ§æœåŠ¡
LANGFUSE_SECRET_KEY=your_langfuse_secret
LANGFUSE_PUBLIC_KEY=your_langfuse_public
LANGFUSE_HOST=your_langfuse_host
```

### æ­¥éª¤ 4: ä¿®æ”¹é…ç½®æ–‡ä»¶

ç¼–è¾‘ `config/settings.yaml`ï¼Œæ›¿æ¢ä¸ºæ‚¨çš„APIé…ç½®ï¼š

```yaml
api_providers:
  text:
    primary:
      name: "æ‚¨çš„æ–‡æœ¬æä¾›å•†"
      api_key: env:DEEPSEEK_API_KEY
      base_url: "https://api.deepseek.com/v1"
      model: "deepseek-chat"
      timeout_seconds: 120
    # å¯ä»¥æ·»åŠ å¤‡ç”¨æä¾›å•†
    secondary:
      name: "å¤‡ç”¨æä¾›å•†"
      api_key: env:GROQ_API_KEY
      base_url: "https://api.groq.com/openai/v1"
      model: "mixtral-8x7b-32768"
      timeout_seconds: 120
```

### æ­¥éª¤ 5: åœ¨ä»£ç ä¸­ä½¿ç”¨

```python
from llm_api.core.llm_client import UnifiedLLMClient

# æ–¹å¼1: ç›´æ¥ä½¿ç”¨
client = UnifiedLLMClient("llm_api/config/settings.yaml")
result = await client.call(task_name="fact_description", user_prompt="...")
```

---

## âš™ï¸ é…ç½®è¯´æ˜

### å®Œæ•´é…ç½®ç¤ºä¾‹

å‚è€ƒ `config/settings.yaml:1-100`ï¼Œé…ç½®åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š

#### 1. APIæä¾›å•†é…ç½®

```yaml
api_providers:
  text:           # æ–‡æœ¬æä¾›å•†
    primary:       # ä¸»æä¾›å•†ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
      name: "æä¾›å•†åç§°"
      api_key: env:YOUR_API_KEY  # ä»ç¯å¢ƒå˜é‡è¯»å–
      base_url: "https://api.example.com/v1"
      model: "your-model"
      timeout_seconds: 120
    secondary:     # å¤‡ç”¨æä¾›å•†ï¼ˆä¸»å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢ï¼‰

  vision:          # è§†è§‰æä¾›å•†ï¼ˆå¯é€‰ï¼‰
    primary:
      # è§†è§‰æ¨¡å‹é…ç½®
```

#### 2. ä»»åŠ¡é…ç½®

```yaml
tasks:
  my_task:         # ä»»åŠ¡åç§°ï¼ˆè‡ªå®šä¹‰ï¼‰
    provider_type: "text"  # text æˆ– vision
    temperature: 0.7       # åˆ›é€ æ€§æ§åˆ¶ï¼ˆ0-1ï¼‰
    top_p: 0.9            # é‡‡æ ·æ§åˆ¶
    max_tokens: 2048      # æœ€å¤§è¾“å‡ºé•¿åº¦

    prompt:
      type: "md"          # æç¤ºè¯æ ¼å¼
      source: "prompts/my_task.md"  # .mdæ–‡ä»¶è·¯å¾„

    retry:                # é‡è¯•ç­–ç•¥
      max_retries: 3
      base_delay: 1
      max_delay: 60
      enable_provider_switch: true

    langfuse:             # ç›‘æ§é…ç½®ï¼ˆå¯é€‰ï¼‰
      enabled: false

    json_repair:          # JSONå¤„ç†ï¼ˆå¯é€‰ï¼‰
      enabled: true
      strict_mode: false
```

#### 3. æç¤ºè¯æ ¼å¼

**æ–¹å¼1: .md æ–‡ä»¶**ï¼ˆæ¨èï¼‰

åˆ›å»º `prompts/my_task.md`ï¼š
```markdown
# ä»»åŠ¡åç§°

ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹...

## è¦æ±‚
1. è¦æ±‚1
2. è¦æ±‚2

## è¾“å‡ºæ ¼å¼
è¯·è¿”å›JSONæ ¼å¼ï¼š
```json
{
  "result": "..."
}
```
```

é…ç½®ï¼š
```yaml
prompt:
  type: "md"
  source: "prompts/my_task.md"
```

**æ–¹å¼2: Pythonå­—å…¸**

```yaml
prompt:
  type: "dict"
  content:
    role: "system"
    content: |
      ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹...
```

**æ–¹å¼3: Langfuseå¹³å°**

```yaml
prompt:
  type: "langfuse"
  langfuse_name: "my_prompt_name"
```

### é…ç½®ä¼˜å…ˆçº§

1. æ–¹æ³•è°ƒç”¨æ—¶çš„ kwargs å‚æ•°ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
2. ä»»åŠ¡é…ç½®ï¼ˆtasksèŠ‚ï¼‰
3. å…¨å±€é»˜è®¤é…ç½®ï¼ˆdefaultsèŠ‚ï¼‰
4. ç¡¬ç¼–ç é»˜è®¤å€¼ï¼ˆæœ€ä½ä¼˜å…ˆçº§ï¼‰

---

## ğŸ’» ä½¿ç”¨æ–¹æ³•

### åŸºç¡€è°ƒç”¨

```python
import asyncio
from core.llm_client import UnifiedLLMClient

async def example():
    client = UnifiedLLMClient()

    # 1. ç®€å•è°ƒç”¨
    result = await client.call(
        task_name="correction",
        user_prompt="è¯·æ ¡å¯¹ä»¥ä¸‹æ–‡æœ¬ï¼šè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ã€‚"
    )
    print(result)

    # 2. å¸¦å‚æ•°è¦†ç›–
    result = await client.call(
        task_name="correction",
        user_prompt="è¯·æ ¡å¯¹...",
        temperature=0.3,  # è¦†ç›–é…ç½®çš„temperature
        max_tokens=1000
    )

    await client.close()

asyncio.run(example())
```

### æµå¼è°ƒç”¨

```python
async def streaming_example():
    client = UnifiedLLMClient()

    # æµå¼è¾“å‡ºï¼ˆå®æ—¶æ˜¾ç¤ºï¼‰
    print("å“åº”: ", end="", flush=True)
    async for chunk in client.stream_call(
        task_name="correction",
        user_prompt="å†™ä¸€é¦–è¯—"
    ):
        print(chunk, end="", flush=True)
    print()  # æ¢è¡Œ

    await client.close()

asyncio.run(streaming_example())
```

### æ‰¹é‡è°ƒç”¨

```python
async def batch_example():
    client = UnifiedLLMClient()

    requests = [
        {"task_name": "correction", "user_prompt": "æ–‡æœ¬1"},
        {"task_name": "correction", "user_prompt": "æ–‡æœ¬2"},
        {"task_name": "correction", "user_prompt": "æ–‡æœ¬3"},
    ]

    # å¹¶å‘å¤„ç†ï¼Œé™åˆ¶æœ€å¤§å¹¶å‘æ•°
    results = await client.batch_call(requests, max_concurrent=3)

    for i, result in enumerate(results):
        print(f"è¯·æ±‚ {i+1}: {result}")

    await client.close()

asyncio.run(batch_example())
```

### è‡ªå®šä¹‰ä»»åŠ¡

#### 1. æ·»åŠ ä»»åŠ¡é…ç½®

åœ¨ `config/settings.yaml` ä¸­æ·»åŠ ï¼š

```yaml
tasks:
  my_custom_task:
    provider_type: "text"
    temperature: 0.5
    prompt:
      type: "md"
      source: "prompts/my_custom_task.md"

    retry:
      max_retries: 3
      enable_provider_switch: true
```

#### 2. åˆ›å»ºæç¤ºè¯æ–‡ä»¶

åˆ›å»º `prompts/my_custom_task.md`ï¼š
```markdown
# è‡ªå®šä¹‰ä»»åŠ¡

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚å¤„ç†ç”¨æˆ·è¾“å…¥...

## è§„åˆ™
1. è§„åˆ™1
2. è§„åˆ™2

## è¾“å‡º
è¯·è¿”å›ç®€æ´çš„å›ç­”ã€‚
```

#### 3. ä½¿ç”¨è‡ªå®šä¹‰ä»»åŠ¡

```python
result = await client.call(
    task_name="my_custom_task",
    user_prompt="ä½ çš„ç”¨æˆ·è¾“å…¥"
)
```

### å›¾ç‰‡åˆ†æä»»åŠ¡

```python
result = await client.call(
    task_name="fact_description",
    user_prompt="è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹"
)
```

`fact_description` ä»»åŠ¡æ”¯æŒè¿”å›JSONæ ¼å¼çš„ç»“æ„åŒ–æ•°æ®ï¼š

```json
{
  "description": "å›¾åƒçš„è¯¦ç»†äº‹å®æè¿°",
  "text_content": "å›¾åƒä¸­çš„æ–‡å­—å†…å®¹",
  "key_objects": [
    {
      "name": "å¯¹è±¡åç§°",
      "position": "ä½ç½®æè¿°",
      "ç‰¹å¾": "ä¸»è¦ç‰¹å¾æè¿°"
    }
  ],
  "scene": "æ•´ä½“åœºæ™¯æè¿°"
}
```

### è¾…åŠ©æ–¹æ³•

```python
client = UnifiedLLMClient()

# è·å–æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨
tasks = client.list_tasks()
print(f"å¯ç”¨ä»»åŠ¡: {tasks}")

# è·å–ä»»åŠ¡é…ç½®
config = client.get_task_config("correction")
print(f"ä»»åŠ¡é…ç½®: {config}")

# é‡æ–°åŠ è½½é…ç½®ï¼ˆçƒ­æ›´æ–°ï¼‰
client.reload_config()

# å…³é—­å®¢æˆ·ç«¯
await client.close()
```

---

## ğŸ¯ å¸¸ç”¨åœºæ™¯

### åœºæ™¯1: æ–‡æœ¬æ ¡å¯¹

```python
import asyncio
from core.llm_client import UnifiedLLMClient

async def text_correction():
    client = UnifiedLLMClient()

    texts = [
        "è¿™é‡Œæœ‰é”™åˆ«å­—ï¼Œéœ€è¦æ ¡å¯¹ã€‚",
        "å¦ä¸€ä¸ªéœ€è¦æ ¡å¯¹çš„æ–‡æœ¬ã€‚",
        "è¿˜æœ‰ä¸€æ®µæ–‡æœ¬ã€‚"
    ]

    for text in texts:
        result = await client.call(
            task_name="correction",
            user_prompt=f"è¯·æ ¡å¯¹ä»¥ä¸‹æ–‡æœ¬ï¼š{text}"
        )
        print(f"åŸæ–‡: {text}")
        print(f"æ ¡å¯¹å: {result}\n")

    await client.close()

asyncio.run(text_correction())
```

### åœºæ™¯2: æ‰¹é‡å›¾ç‰‡åˆ†æ

```python
async def batch_image_analysis():
    client = UnifiedLLMClient()

    images = ["å›¾ç‰‡1æè¿°", "å›¾ç‰‡2æè¿°", "å›¾ç‰‡3æè¿°"]

    requests = [
        {"task_name": "fact_description", "user_prompt": img}
        for img in images
    ]

    results = await client.batch_call(requests, max_concurrent=5)

    for i, result in enumerate(results):
        print(f"å›¾ç‰‡ {i+1} åˆ†æç»“æœ:")
        print(result)
        print("-" * 50)

    await client.close()

asyncio.run(batch_image_analysis())
```

### åœºæ™¯3: å¸¦ç›‘æ§çš„è°ƒç”¨

```python
# 1. é…ç½® Langfuse
# åœ¨ config/settings.yaml ä¸­å¯ç”¨
langfuse:
  enabled: true

# 2. ä½¿ç”¨ç›‘æ§åŠŸèƒ½
result = await client.call(
    task_name="correction",
    user_prompt="æµ‹è¯•æ–‡æœ¬",
    # å¯é€‰ï¼šæ·»åŠ é¢å¤–å…ƒæ•°æ®
    metadata={"user_id": 123, "source": "web"}
)
```

### åœºæ™¯4: JSONå“åº”å¤„ç†

```python
# 1. åœ¨ä»»åŠ¡é…ç½®ä¸­å¯ç”¨ JSON ä¿®å¤
tasks:
  my_task:
    json_repair:
      enabled: true
      strict_mode: false  # false: ä¿®å¤å¤±è´¥è¿”å›åŸæ–‡ï¼Œtrue: è¿”å›None

# 2. è°ƒç”¨å¹¶å¤„ç† JSON
result = await client.call(
    task_name="my_task",
    user_prompt="è¯·è¿”å›JSONæ ¼å¼çš„æ•°æ®"
)

# è‡ªåŠ¨å¤„ç†ï¼š
# - ç§»é™¤ ```json ``` æ ‡è®°
# - ä¿®å¤å¸¸è§è¯­æ³•é”™è¯¯
# - æå–JSONç‰‡æ®µ
print(result)  # ç›´æ¥ä½¿ç”¨ä¿®å¤åçš„JSONå­—ç¬¦ä¸²
```

---

## âœ… æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†

```python
from core.exceptions import (
    LLMCallError,
    ConfigurationError,
    ProviderError,
    RateLimitError
)

try:
    result = await client.call(
        task_name="correction",
        user_prompt="æµ‹è¯•"
    )
except LLMCallError as e:
    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    print(f"è°ƒç”¨å¤±è´¥: {e}")
    print(f"é”™è¯¯å†å²: {e.error_history}")
except ConfigurationError as e:
    # é…ç½®æ–‡ä»¶é”™è¯¯
    print(f"é…ç½®é”™è¯¯: {e}")
except RateLimitError as e:
    # é€Ÿç‡é™åˆ¶
    print(f"é€Ÿç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•: {e}")
except Exception as e:
    # å…¶ä»–é”™è¯¯
    print(f"æœªçŸ¥é”™è¯¯: {e}")
```

### 2. èµ„æºç®¡ç†

```python
# æ¨èï¼šä½¿ç”¨ async with æˆ–æ‰‹åŠ¨å…³é—­
client = UnifiedLLMClient()

try:
    result = await client.call(...)
    # å¤„ç†ç»“æœ
finally:
    await client.close()  # åŠ¡å¿…å…³é—­

# æˆ–è€…ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆå¦‚æœå·²å®ç°ï¼‰
# async with UnifiedLLMClient() as client:
#     result = await client.call(...)
```

### 3. é…ç½®ç®¡ç†

```python
# 1. ç¯å¢ƒå˜é‡ä¼˜å…ˆåŸåˆ™
# .env æ–‡ä»¶
API_KEY=your_key

# config/settings.yaml
api_key: env:API_KEY  # ä½¿ç”¨ç¯å¢ƒå˜é‡

# 2. å¤šç¯å¢ƒé…ç½®
# å¼€å‘ç¯å¢ƒ
dev_config = "config/settings.dev.yaml"
client_dev = UnifiedLLMClient(dev_config)

# ç”Ÿäº§ç¯å¢ƒ
prod_config = "config/settings.prod.yaml"
client_prod = UnifiedLLMClient(prod_config)
```

### 4. æ€§èƒ½ä¼˜åŒ–

```python
# 1. æ‰¹é‡è°ƒç”¨å‡å°‘å¼€é”€
requests = [...]  # å‡†å¤‡å¤šä¸ªè¯·æ±‚
results = await client.batch_call(requests, max_concurrent=10)

# 2. åˆç†è®¾ç½®å¹¶å‘æ•°
# æ ¹æ®APIæä¾›å•†é™åˆ¶è°ƒæ•´ max_concurrent
results = await client.batch_call(requests, max_concurrent=5)

# 3. æµå¼è°ƒç”¨æå‡ç”¨æˆ·ä½“éªŒ
async for chunk in client.stream_call(...):
    print(chunk, end="", flush=True)

# 4. å¤ç”¨å®¢æˆ·ç«¯å®ä¾‹
# é”™è¯¯ç¤ºä¾‹ï¼šæ¯æ¬¡è°ƒç”¨éƒ½åˆ›å»ºæ–°å®¢æˆ·ç«¯
# for text in texts:
#     client = UnifiedLLMClient()  # âŒ ä¸è¦è¿™æ ·åš
#     await client.call(...)

# æ­£ç¡®ç¤ºä¾‹ï¼šå¤ç”¨å®¢æˆ·ç«¯
client = UnifiedLLMClient()  # âœ… åˆ›å»ºä¸€æ¬¡
for text in texts:
    await client.call(...)
await client.close()
```

### 5. æ—¥å¿—è®°å½•

```python
from utils.logger import get_logger, log_function_call

logger = get_logger(__name__)

@log_function_call
async def my_llm_function(user_input):
    # è‡ªåŠ¨è®°å½•ï¼šå‡½æ•°è°ƒç”¨ã€å‚æ•°ã€è¿”å›å€¼ã€è€—æ—¶ã€å¼‚å¸¸
    client = UnifiedLLMClient()
    try:
        result = await client.call(
            task_name="correction",
            user_prompt=user_input
        )
        logger.info(f"å¤„ç†æˆåŠŸï¼Œè¾“å…¥é•¿åº¦: {len(user_input)}")
        return result
    finally:
        await client.close()

# æ‰‹åŠ¨è®°å½•
logger.info("è¿™æ˜¯ä¸€æ¡ä¸­æ–‡æ—¥å¿—")
logger.warning("è­¦å‘Šä¿¡æ¯")
logger.error("é”™è¯¯ä¿¡æ¯")
```

### 6. é€Ÿç‡é™åˆ¶

```yaml
# åœ¨é…ç½®ä¸­è®¾ç½®
rate_limit:
  global:
    requests_per_minute: 1000  # å…¨å±€é™æµ
    concurrent_requests: 10    # å…¨å±€å¹¶å‘é™åˆ¶

# åœ¨ä»»åŠ¡ä¸­è®¾ç½®
tasks:
  my_task:
    rate_limit:
      requests_per_minute: 60  # ä»»åŠ¡é™æµ
      burst_size: 10           # çªå‘é‡
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. é…ç½®æ–‡ä»¶é”™è¯¯

**é”™è¯¯ä¿¡æ¯ï¼š**
```
FileNotFoundError: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: config/settings.yaml
```

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# ç¡®ä¿ä½¿ç”¨æ­£ç¡®è·¯å¾„
client = UnifiedLLMClient("your_project/llm_api/config/settings.yaml")
```

#### 2. APIå¯†é’¥ç¼ºå¤±

**é”™è¯¯ä¿¡æ¯ï¼š**
```
è­¦å‘Š: missing_env_key key=DEEPSEEK_API_KEY
```

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# 1. æ£€æŸ¥ .env æ–‡ä»¶æ˜¯å¦å­˜åœ¨
# 2. æ£€æŸ¥ç¯å¢ƒå˜é‡åæ˜¯å¦æ­£ç¡®
echo $DEEPSEEK_API_KEY

# 3. åœ¨ .env ä¸­é…ç½®
DEEPSEEK_API_KEY=sk-your-actual-key
```

#### 3. æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨

**é”™è¯¯ä¿¡æ¯ï¼š**
```
PromptError: æ–‡ä»¶ä¸å­˜åœ¨: prompts/my_task.md
```

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# 1. æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®
# 2. ç¡®ä¿æ–‡ä»¶å­˜åœ¨äºæŒ‡å®šä½ç½®
# 3. ä½¿ç”¨ç»å¯¹è·¯å¾„æµ‹è¯•

prompt:
  type: "md"
  source: "/full/path/to/prompts/my_task.md"
```

#### 4. Langfuseæœªå®‰è£…

**é”™è¯¯ä¿¡æ¯ï¼š**
```
Langfuseæœªå®‰è£…
```

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# å®‰è£… Langfuse
pip install langfuse

# æˆ–åœ¨é…ç½®ä¸­ç¦ç”¨
tasks:
  my_task:
    langfuse:
      enabled: false
```

#### 5. ç½‘ç»œè¿æ¥é”™è¯¯

**é”™è¯¯ä¿¡æ¯ï¼š**
```
NetworkError: è¿æ¥è¶…æ—¶
```

**è§£å†³æ–¹æ¡ˆï¼š**
```yaml
# 1. å¢åŠ è¶…æ—¶æ—¶é—´
api_providers:
  text:
    primary:
      timeout_seconds: 180  # å¢åŠ åˆ°180ç§’

# 2. æ£€æŸ¥ç½‘ç»œå’Œä»£ç†è®¾ç½®
# 3. æ£€æŸ¥ base_url æ˜¯å¦æ­£ç¡®
```

#### 6. æ¨¡å‹ä¸æ”¯æŒ

**é”™è¯¯ä¿¡æ¯ï¼š**
```
ModelError: æ¨¡å‹ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®
```

**è§£å†³æ–¹æ¡ˆï¼š**
```yaml
# 1. ç¡®è®¤æ¨¡å‹åç§°æ­£ç¡®
api_providers:
  text:
    primary:
      model: "your-actual-model-name"

# 2. æ£€æŸ¥APIå¯†é’¥æƒé™
# 3. ç¡®è®¤æ¨¡å‹åç§°æ‹¼å†™
```

#### 7. é€Ÿç‡é™åˆ¶

**é”™è¯¯ä¿¡æ¯ï¼š**
```
RateLimitError: è¯·æ±‚è¿‡äºé¢‘ç¹
```

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# 1. ä½¿ç”¨æ‰¹é‡è°ƒç”¨
results = await client.batch_call(requests, max_concurrent=1)

# 2. å¢åŠ å»¶è¿Ÿ
await asyncio.sleep(1)

# 3. é…ç½®é‡è¯•
retry:
  max_retries: 5
  base_delay: 2
```

### è°ƒè¯•æŠ€å·§

#### 1. å¯ç”¨è¯¦ç»†æ—¥å¿—

```yaml
# config/settings.yaml
logging:
  levels:
    file: "DEBUG"  # æ–‡ä»¶æ—¥å¿—ä½¿ç”¨DEBUGçº§åˆ«
```

#### 2. æŸ¥çœ‹é”™è¯¯å†å²

```python
try:
    result = await client.call(...)
except LLMCallError as e:
    print("=== é”™è¯¯å†å² ===")
    for error in e.error_history:
        print(f"Provider: {error['provider']}")
        print(f"é”™è¯¯ç±»å‹: {error['error_type']}")
        print(f"é”™è¯¯ä¿¡æ¯: {error['error']}")
```

#### 3. æµ‹è¯•è¿æ¥

```python
async def test_connection():
    client = UnifiedLLMClient()

    # æµ‹è¯•ç®€å•è°ƒç”¨
    try:
        result = await client.call(
            task_name="correction",
            user_prompt="hi"
        )
        print("è¿æ¥æˆåŠŸ!")
        print(result)
    except Exception as e:
        print(f"è¿æ¥å¤±è´¥: {e}")
    finally:
        await client.close()

asyncio.run(test_connection())
```

#### 4. åˆ—å‡ºå¯ç”¨ä»»åŠ¡

```python
client = UnifiedLLMClient()

# æŸ¥çœ‹æ‰€æœ‰ä»»åŠ¡
tasks = client.list_tasks()
print(f"å¯ç”¨ä»»åŠ¡: {tasks}")

# æŸ¥çœ‹ä»»åŠ¡é…ç½®
for task in tasks:
    config = client.get_task_config(task)
    print(f"\n{task}:")
    print(f"  æä¾›å•†ç±»å‹: {config['provider_type']}")
    print(f"  æç¤ºè¯ç±»å‹: {config['prompt']['type']}")
```

---

## ğŸ“š æ›´å¤šèµ„æº

- **å®Œæ•´æ–‡æ¡£**: [README.md](./README.md)
- **ç¤ºä¾‹ä»£ç **: [examples/](./examples/)
- **é…ç½®å‚è€ƒ**: [config/settings.yaml](./config/settings.yaml)
- **æç¤ºè¯æ¨¡æ¿**: [prompts/](./prompts/)

---

## â“ å¸¸è§é—®é¢˜ FAQ

**Q: æ”¯æŒå“ªäº›LLMæä¾›å•†ï¼Ÿ**
A: æ”¯æŒæ‰€æœ‰ OpenAI å…¼å®¹çš„APIï¼ŒåŒ…æ‹¬ DeepSeekã€ChatGLMã€Qwenã€Mixtral ç­‰ã€‚

**Q: å¦‚ä½•æ·»åŠ æ–°çš„LLMæä¾›å•†ï¼Ÿ**
A: åœ¨ `config/settings.yaml` çš„ `api_providers` èŠ‚æ·»åŠ é…ç½®ã€‚

**Q: å¦‚ä½•è‡ªå®šä¹‰æç¤ºè¯ï¼Ÿ**
A: åˆ›å»º `.md` æ–‡ä»¶å¹¶åœ¨ä»»åŠ¡é…ç½®çš„ `prompt.source` ä¸­æŒ‡å®šã€‚

**Q: å¦‚ä½•å¯ç”¨ç›‘æ§ï¼Ÿ**
A: å®‰è£… `langfuse` å¹¶åœ¨é…ç½®ä¸­è®¾ç½® `langfuse.enabled: true`ã€‚

**Q: å¦‚ä½•å¤„ç†å¤§é‡è¯·æ±‚ï¼Ÿ**
A: ä½¿ç”¨ `batch_call()` æ–¹æ³•å¹¶åˆç†è®¾ç½® `max_concurrent` å‚æ•°ã€‚

**Q: å¦‚ä½•å®ç°å¤šç¯å¢ƒé…ç½®ï¼Ÿ**
A: ä¸ºä¸åŒç¯å¢ƒåˆ›å»ºä¸åŒçš„é…ç½®æ–‡ä»¶ï¼Œåœ¨åˆå§‹åŒ–æ—¶æŒ‡å®šè·¯å¾„ã€‚

---

**ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼** ğŸ‰

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æäº¤ Issue æˆ–æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£ã€‚
